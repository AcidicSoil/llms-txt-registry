Metadata-Version: 2.4
Name: llms-txt-registry
Version: 0.1.0
Summary: Centralized registry and orchestrator for llms.txt artifacts
License: MIT
Requires-Python: >=3.10
Description-Content-Type: text/markdown
Requires-Dist: pydantic>=2.0
Requires-Dist: requests>=2.31.0
Requires-Dist: lmstudio-llmstxt-generator
Provides-Extra: dev
Requires-Dist: pytest>=8.0.0; extra == "dev"
Requires-Dist: pytest-mock; extra == "dev"

# llms-txt Registry

A centralized registry of generated `llms.txt` documentation artifacts for various libraries and tools.

## Usage

This registry is designed to be consumed via **MCP** (Model Context Protocol) using `gitmcp` as the gateway.

### Gemini CLI Configuration

Add this to your `~/.gemini/settings.json` (or project config):

```json
{
  "mcpServers": {
    "llms-registry": {
      "command": "npx",
      "args": [
        "-y",
        "mcp-remote",
        "gitmcp.io/<org>/<repo>"
      ]
    }
  }
}
```

**Important**: Replace `<org>/<repo>` with the actual GitHub path to this repository (e.g., `my-org/llms-registry`).

### Codex Configuration

For Codex or other clients supporting `mcp-remote`:

```toml
[mcpServers.llms-registry]
command = "npx"
args = ["-y", "mcp-remote", "gitmcp.io/<org>/<repo>"]
```

## How It Works (The AI Flow)

1.  **Connection**: Your local client connects to the `gitmcp.io` gateway, which acts as a bridge to this GitHub repository.
2.  **Discovery**: The gateway exposes the files in `docs/` as **MCP Resources**. The AI sees a list of available documentation sets (e.g., `tanstack-router`, `supabase`).
3.  **Retrieval**: When you ask a question (e.g., *"How do I configure Supabase auth?"*), the AI automatically identifies the relevant resource (`docs/supabase/supabase-llms.txt`) and reads it via MCP.
4.  **Context**: The content of that file is injected into the conversation context, allowing the AI to answer with up-to-date, curated knowledge.

## Structure

Artifacts are stored in the `docs/` directory, organized by source ID:

- `docs/<source-id>/<source-id>-llms.txt`
- `docs/<source-id>/metadata.json`

Example:
- `docs/tanstack-router/tanstack-router-llms.txt`

A machine-readable index is available at `docs/index.json`.

## Contributor Guide (Local-First Workflow)

We use a local-first workflow to generate artifacts using your own LM Studio instance. This avoids sending code to remote APIs during CI.

### Prerequisites

- [uv](https://github.com/astral-sh/uv) (Recommended Python package manager)
- [LM Studio](https://lmstudio.ai/) (with a model loaded and Server started)

### Setup

1. Install the project and its dependencies:
   ```bash
   uv sync
   ```

2. Install Git hooks (Critical):
   ```bash
   ./scripts/setup_hooks.sh
   ```
   This installs a `pre-push` hook that ensures `docs/` are in sync with `sources.json`.

### Adding a Source

1. Edit `sources.json` to add a new entry:
   ```json
   {
     "id": "my-lib",
     "url": "https://github.com/owner/lib",
     "type": "repo",
     "profile": "default"
   }
   ```

2. Run the refresh command locally:
   ```bash
   # Make sure LM Studio server is running at http://localhost:1234
   uv run llms-registry --only my-lib
   ```

3. Verify the output in `docs/my-lib/`.

4. Commit and push.

### Refreshing All Sources

To update all artifacts:
```bash
uv run llms-registry
```

### Running Tests

```bash
uv run pytest
```
