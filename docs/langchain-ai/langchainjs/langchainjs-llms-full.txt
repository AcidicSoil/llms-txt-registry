# llms-full (private-aware)
> Built from GitHub files and website pages. Large files may be truncated.

--- examples/src/langchain-classic/callbacks/lunary_quickstart.ts ---
import { LunaryHandler } from "@langchain/community/callbacks/handlers/lunary";
import { ChatOpenAI } from "@langchain/openai";

const model = new ChatOpenAI({
  callbacks: [new LunaryHandler()],
});


--- examples/src/langchain-classic/chat/overview.ts ---
import { ConversationChain, LLMChain } from "@langchain/classic/chains";
import { ChatOpenAI } from "@langchain/openai";
import { BufferMemory } from "@langchain/classic/memory";
import {
  ChatPromptTemplate,
  HumanMessagePromptTemplate,
  MessagesPlaceholder,
  SystemMessagePromptTemplate,
} from "@langchain/core/prompts";
import { HumanMessage, SystemMessage } from "@langchain/core/messages";

export const run = async () => {
  const chat = new ChatOpenAI({ model: "gpt-4o-mini", temperature: 0 });

  // Sending one message to the chat model, receiving one message back

  let response = await chat.invoke([
    new HumanMessage(
      "Translate this sentence from English to French. I love programming."
    ),
  ]);

  console.log(response);

  // Sending an input made up of two messages to the chat model

  response = await chat.invoke([
    new SystemMessage(
      "You are a helpful assistant that translates English to French."
    ),
    new HumanMessage("Translate: I love programming."),
  ]);

  console.log(response);

  // Sending two separate prompts in parallel, receiving two responses back

  const responseA = await chat.invoke([
    new SystemMessage(
      "You are a helpful assistant that translates English to French."
    ),
    new HumanMessage(
      "Translate this sentence from English to French. I love programming."
    ),
    new SystemMessage(
      "You are a helpful assistant that translates English to French."
    ),
    new HumanMessage(
      "Translate this sentence from English to French. I love artificial intelligence."
    ),
  ]);

  console.log(responseA);

  // Using ChatPromptTemplate to encapsulate the reusable parts of the prompt

  const translatePrompt = ChatPromptTemplate.fromMessages([
    SystemMessagePromptTemplate.fromTemplate(
      "You are a helpful assistant that translates {input_language} to {output_language}."
    ),
    HumanMessagePromptTemplate.fromTemplate("{text}"),
  ]);

  const responseB = await chat.invoke(
    await translatePrompt.formatPromptValue({
      input_language: "English",
      output_language: "French",
      text: "I love programming.",
    })
  );

  console.log(responseB);

  // This pattern of asking for the completion of a formatted prompt is quite
  // common, so we introduce the next piece of the puzzle: LLMChain

  const translateChain = new LLMChain({
    prompt: translatePrompt,
    llm: chat,
  });

  const responseC = await translateChain.invoke({
    input_language: "English",
    output_language: "French",
    text: "I love programming.",
  });

  console.log(responseC);

  // Next up, stateful chains that remember the conversation history

  const chatPrompt = ChatPromptTemplate.fromMessages([
    SystemMessagePromptTemplate.fromTemplate(
      "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know."
    ),
    new MessagesPlaceholder("history"),
    HumanMessagePromptTemplate.fromTemplate("{input}"),
  ]);

  const chain = new ConversationChain({
    memory: new BufferMemory({ returnMessages: true }),
    prompt: chatPrompt,
    llm: chat,
  });

  const responseE = await chain.invoke({
    input: "hi from London, how are you doing today",
  });

  console.log(responseE);

  const responseF = await chain.invoke({
    input: "Do you know where I am?",
  });

  console.log(responseF);
};


--- examples/src/langchain-classic/get_started/quickstart.ts ---
import { ChatOpenAI } from "@langchain/openai";

const chatModel = new ChatOpenAI({
  model: "gpt-4o-mini",
});

console.log(await chatModel.invoke("what is LangSmith?"));

/*
  AIMessage {
    content: 'Langsmith can help with testing by generating test cases, automating the testing process, and analyzing test results.',
    name: undefined,
    additional_kwargs: { function_call: undefined, tool_calls: undefined }
  }
*/

import { ChatPromptTemplate } from "@langchain/core/prompts";

const prompt = ChatPromptTemplate.fromMessages([
  ["system", "You are a world class technical documentation writer."],
  ["user", "{input}"],
]);

const chain = prompt.pipe(chatModel);

console.log(
  await chain.invoke({
    input: "what is LangSmith?",
  })
);

import { StringOutputParser } from "@langchain/core/output_parsers";

const outputParser = new StringOutputParser();

const llmChain = prompt.pipe(chatModel).pipe(outputParser);

console.log(
  await llmChain.invoke({
    input: "what is LangSmith?",
  })
);


--- examples/src/langchain-classic/get_started/quickstart2.ts ---
import { CheerioWebBaseLoader } from "@langchain/community/document_loaders/web/cheerio";
import { ChatOpenAI, OpenAIEmbeddings } from "@langchain/openai";

const chatModel = new ChatOpenAI({
  model: "gpt-4o-mini",
});

const embeddings = new OpenAIEmbeddings({});

const loader = new CheerioWebBaseLoader(
  "https://docs.smith.langchain.com/user_guide"
);

const docs = await loader.load();

console.log(docs.length);
console.log(docs[0].pageContent.length);

import { RecursiveCharacterTextSplitter } from "@langchain/textsplitters";

const splitter = new RecursiveCharacterTextSplitter();

const splitDocs = await splitter.splitDocuments(docs);

console.log(splitDocs.length);
console.log(splitDocs[0].pageContent.length);

import { MemoryVectorStore } from "@langchain/classic/vectorstores/memory";

const vectorstore = await MemoryVectorStore.fromDocuments(
  splitDocs,
  embeddings
);

import { createStuffDocumentsChain } from "@langchain/classic/chains/combine_documents";
import { ChatPromptTemplate } from "@langchain/core/prompts";

const prompt =
  ChatPromptTemplate.fromTemplate(`Answer the following question based only on the provided context:

<context>
{context}
</context>

Question: {input}`);

const documentChain = await createStuffDocumentsChain({
  llm: chatModel,
  prompt,
});

import { Document } from "@langchain/core/documents";

console.log(
  await documentChain.invoke({
    input: "what is LangSmith?",
    context: [
      new Document({
        pageContent:
          "LangSmith is a platform for building production-grade LLM applications.",
      }),
    ],
  })
);

import { createRetrievalChain } from "@langchain/classic/chains/retrieval";

const retriever = vectorstore.asRetriever();

const retrievalChain = await createRetrievalChain({
  combineDocsChain: documentChain,
  retriever,
});

console.log(
  await retrievalChain.invoke({
    input: "what is LangSmith?",
  })
);

import { createHistoryAwareRetriever } from "@langchain/classic/chains/history_aware_retriever";
import { MessagesPlaceholder } from "@langchain/core/prompts";

const historyAwarePrompt = ChatPromptTemplate.fromMessages([
  new MessagesPlaceholder("chat_history"),
  ["user", "{input}"],
  [
    "user",
    "Given the above conversation, generate a search query to look up in order to get information relevant to the conversation",
  ],
]);

const historyAwareRetrieverChain = await createHistoryAwareRetriever({
  llm: chatModel,
  retriever,
  rephrasePrompt: historyAwarePrompt,
});

import { HumanMessage, AIMessage } from "@langchain/core/messages";

const chatHistory = [
  new HumanMessage("Can LangSmith help test my LLM applications?"),
  new AIMessage("Yes!"),
];

console.log(
  await historyAwareRetrieverChain.invoke({
    chat_history: chatHistory,
    input: "Tell me how!",
  })
);

const historyAwareRetrievalPrompt = ChatPromptTemplate.fromMessages([
  [
    "system",
    "Answer the user's questions based on the below context:\n\n{context}",
  ],
  new MessagesPlaceholder("chat_history"),
  ["user", "{input}"],
]);

const historyAwareCombineDocsChain = await createStuffDocumentsChain({
  llm: chatModel,
  prompt: historyAwareRetrievalPrompt,
});

const conversationalRetrievalChain = await createRetrievalChain({
  retriever: historyAwareRetrieverChain,
  combineDocsChain: historyAwareCombineDocsChain,
});

const result2 = await conversationalRetrievalChain.invoke({
  chat_history: [
    new HumanMessage("Can LangSmith help test my LLM applications?"),
    new AIMessage("Yes!"),
  ],
  input: "tell me how",
});

console.log(result2.answer);


--- examples/src/langchain-classic/get_started/quickstart3.ts ---
import { CheerioWebBaseLoader } from "@langchain/community/document_loaders/web/cheerio";
import { ChatOpenAI, OpenAIEmbeddings } from "@langchain/openai";

const chatModel = new ChatOpenAI({
  model: "gpt-4o-mini",
});

const embeddings = new OpenAIEmbeddings({});

const loader = new CheerioWebBaseLoader(
  "https://docs.smith.langchain.com/user_guide"
);

const docs = await loader.load();

console.log(docs.length);
console.log(docs[0].pageContent.length);

import { RecursiveCharacterTextSplitter } from "@langchain/textsplitters";

const splitter = new RecursiveCharacterTextSplitter();

const splitDocs = await splitter.splitDocuments(docs);

console.log(splitDocs.length);
console.log(splitDocs[0].pageContent.length);

import { MemoryVectorStore } from "@langchain/classic/vectorstores/memory";

const vectorstore = await MemoryVectorStore.fromDocuments(
  splitDocs,
  embeddings
);

import { createStuffDocumentsChain } from "@langchain/classic/chains/combine_documents";
import { ChatPromptTemplate } from "@langchain/core/prompts";

const prompt =
  ChatPromptTemplate.fromTemplate(`Answer the following question based only on the provided context:

<context>
{context}
</context>

Question: {input}`);

const documentChain = await createStuffDocumentsChain({
  llm: chatModel,
  prompt,
});

import { Document } from "@langchain/core/documents";

console.log(
  await documentChain.invoke({
    input: "what is LangSmith?",
    context: [
      new Document({
        pageContent:
          "LangSmith is a platform for building production-grade LLM applications.",
      }),
    ],
  })
);

const retriever = vectorstore.asRetriever();

import { createRetrieverTool } from "@langchain/classic/tools/retriever";

const retrieverTool = await createRetrieverTool(retriever, {
  name: "langsmith_search",
  description:
    "Search for information about LangSmith. For any questions about LangSmith, you must use this tool!",
});

import { TavilySearch } from "@langchain/tavily";

const searchTool = new TavilySearch();

const tools = [retrieverTool, searchTool];

import { pull } from "langchain/hub";
import {
  createOpenAIFunctionsAgent,
  AgentExecutor,
} from "@langchain/classic/agents";
import { HumanMessage, AIMessage } from "@langchain/core/messages";

// Get the prompt to use - you can modify this!
// If you want to see the prompt in full, you can at:
// https://smith.langchain.com/hub/hwchase17/openai-functions-agent
const agentPrompt = await pull<ChatPromptTemplate>(
  "hwchase17/openai-functions-agent"
);

const agentModel = new ChatOpenAI({
  model: "gpt-3.5-turbo-1106",
  temperature: 0,
});

const agent = await createOpenAIFunctionsAgent({
  llm: agentModel,
  tools,
  prompt: agentPrompt,
});

const agentExecutor = new AgentExecutor({
  agent,
  tools,
  verbose: true,
});

const agentResult = await agentExecutor.invoke({
  input: "how can LangSmith help with testing?",
});

console.log(agentResult);

const agentResult2 = await agentExecutor.invoke({
  input: "what is the weather in SF?",
});

console.log(agentResult2);

const agentResult3 = await agentExecutor.invoke({
  chat_history: [
    new HumanMessage("Can LangSmith help test my LLM applications?"),
    new AIMessage("Yes!"),
  ],
  input: "Tell me how",
});

console.log(agentResult3);


--- examples/src/langchain-classic/prompts/quickstart/basic_fstring.ts ---
import { PromptTemplate } from "@langchain/core/prompts";

// If a template is passed in, the input variables are inferred automatically from the template.
const prompt = PromptTemplate.fromTemplate(
  `You are a naming consultant for new companies.
What is a good name for a company that makes {product}?`
);

const formattedPrompt = await prompt.format({
  product: "colorful socks",
});

/*
You are a naming consultant for new companies.
What is a good name for a company that makes colorful socks?
*/


--- examples/src/langchain-classic/prompts/quickstart/basic_mustache.ts ---
import { PromptTemplate } from "@langchain/core/prompts";

// If a template is passed in, the input variables are inferred automatically from the template.
const prompt = PromptTemplate.fromTemplate(
  `You are a naming consultant for new companies.
What is a good name for a company that makes {{product}}?`,
  {
    templateFormat: "mustache",
  }
);

const formattedPrompt = await prompt.format({
  product: "colorful socks",
});

/*
You are a naming consultant for new companies.
What is a good name for a company that makes colorful socks?
*/


--- examples/src/langchain-classic/prompts/quickstart/hard_coded_fstring.ts ---
import { PromptTemplate } from "@langchain/core/prompts";

// An example prompt with no input variables
const noInputPrompt = new PromptTemplate({
  inputVariables: [],
  template: "Tell me a joke.",
});
const formattedNoInputPrompt = await noInputPrompt.format({});

console.log(formattedNoInputPrompt);
// "Tell me a joke."

// An example prompt with one input variable
const oneInputPrompt = new PromptTemplate({
  inputVariables: ["adjective"],
  template: "Tell me a {adjective} joke.",
});
const formattedOneInputPrompt = await oneInputPrompt.format({
  adjective: "funny",
});

console.log(formattedOneInputPrompt);
// "Tell me a funny joke."

// An example prompt with multiple input variables
const multipleInputPrompt = new PromptTemplate({
  inputVariables: ["adjective", "content"],
  template: "Tell me a {adjective} joke about {content}.",
});
const formattedMultipleInputPrompt = await multipleInputPrompt.format({
  adjective: "funny",
  content: "chickens",
});

console.log(formattedMultipleInputPrompt);
// "Tell me a funny joke about chickens."


--- examples/src/langchain-classic/prompts/quickstart/hard_coded_mustache.ts ---
import { PromptTemplate } from "@langchain/core/prompts";

// An example prompt with no input variables
const noInputPrompt = new PromptTemplate({
  inputVariables: [],
  template: "Tell me a joke.",
});
const formattedNoInputPrompt = await noInputPrompt.format({});

console.log(formattedNoInputPrompt);
// "Tell me a joke."

// An example prompt with one input variable
const oneInputPrompt = new PromptTemplate({
  inputVariables: ["adjective"],
  template: "Tell me a {{adjective}} joke.",
  templateFormat: "mustache",
});
const formattedOneInputPrompt = await oneInputPrompt.format({
  adjective: "funny",
});

console.log(formattedOneInputPrompt);
// "Tell me a funny joke."

// An example prompt with multiple input variables
const multipleInputPrompt = new PromptTemplate({
  inputVariables: ["adjective", "content"],
  template: "Tell me a {{adjective}} joke about {{content}}.",
  templateFormat: "mustache",
});
const formattedMultipleInputPrompt = await multipleInputPrompt.format({
  adjective: "funny",
  content: "chickens",
});

console.log(formattedMultipleInputPrompt);
// "Tell me a funny joke about chickens."


--- examples/src/langchain-classic/prompts/quickstart/input_vars_fstring.ts ---
import { PromptTemplate } from "@langchain/core/prompts";

const template = "Tell me a {adjective} joke about {content}.";

const promptTemplate = PromptTemplate.fromTemplate(template);
console.log(promptTemplate.inputVariables);
// ['adjective', 'content']
const formattedPromptTemplate = await promptTemplate.format({
  adjective: "funny",
  content: "chickens",
});
console.log(formattedPromptTemplate);
// "Tell me a funny joke about chickens."


--- examples/src/README.md ---
# langchain-examples

This folder contains examples of how to use LangChain.

## Run an example

What you'll usually want to do.

First, build langchain. From the repository root, run:

```sh
pnpm install
pnpm build
```

Most examples require API keys. Run `cp .env.example .env`, then edit `.env` with your API keys.

Then from the `examples/` directory, run:

`pnpm run start <path to example>`

eg.

`pnpm run start ./src/prompts/few_shot.ts`

## Run an example with the transpiled JS

You shouldn't need to do this, but if you want to run an example with the transpiled JS, you can do so with:

`pnpm run start:dist <path to example>`

eg.

`pnpm run start:dist ./dist/prompts/few_shot.js`


--- libs/langchain-mcp-adapters/examples/README.md ---
# LangChainJS-MCP-Adapters Examples

This directory contains examples demonstrating how to use the `@langchain/mcp-adapters` library with various MCP servers

## Running the Examples

```bash
# type check examples
pnpm lint:examples

# Run specific example
cd examples && npx -y tsx firecrawl_custom_config_example.ts
```

## Example Descriptions

### Filesystem LangGraph Example (`filesystem_langgraph_example.ts`)

Demonstrates using the Filesystem MCP server with LangGraph to create a structured workflow for complex file operations. The example creates a graph-based agent that can perform various file operations like creating multiple files, reading files, creating directory structures, and organizing files.

### Firecrawl - Custom Configuration (`firecrawl_custom_config_example.ts`)

Shows how to initialize the Firecrawl MCP server with a custom configuration. The example sets up a connection to Firecrawl using SSE transport, loads tools from the server, and creates a React agent to perform web scraping tasks and find news about artificial intelligence.

### Firecrawl - Multiple Servers (`firecrawl_multiple_servers_example.ts`)

Demonstrates how to use multiple MCP servers simultaneously by configuring both Firecrawl for web scraping and a Math server for calculations. The example creates a React agent that can use tools from both servers to answer queries involving both math calculations and web content retrieval.

### LangGraph - Complex Config (`langgraph_complex_config_example.ts`)

Illustrates using different configuration files to set up connections to MCP servers, with a focus on the Math server. This example shows how to parse JSON configuration files, connect to a Math server directly, and create a LangGraph workflow that can perform mathematical operations using MCP tools.

### LangGraph - Simple Config (`langgraph_example.ts`)

Shows a straightforward integration of LangGraph with MCP tools, creating a flexible agent workflow. The example demonstrates how to set up a graph-based structure with separate nodes for LLM reasoning and tool execution, with conditional routing between nodes based on whether tool calls are needed.

### Launching a Containerized MCP Server (`mcp_over_docker_example.ts`)

Shows how to run an MCP server inside a Docker container. This example configures a connection to a containerized Filesystem MCP server with appropriate volume mounting, demonstrating how to use Docker to isolate and run MCP servers while still allowing file operations.

## Requirements

Ensure you have the correct environment variables set in your `.env` file:

```
OPENAI_API_KEY=your_openai_api_key
FIRECRAWL_API_KEY=your_firecrawl_api_key
OPENAI_MODEL_NAME=gpt-4o  # or your preferred model
```


--- cookbook/README.md ---
# LangChain.js cookbook

Example code for building applications with LangChain.js, with an emphasis on more applied and end-to-end examples than contained in the [main documentation](https://js.langchain.com).

## Setup

These cookbooks are in Jupyter notebook form and use the [Deno runtime](https://deno.com) and the experimental [Deno Jupyter Kernel](https://deno.com/blog/v1.37) (requires >= Deno v1.37).

Full installation instructions are available here: https://docs.deno.com/runtime/manual/tools/jupyter

Note that you will also need to install the Python `jupyter` package, and that the syntax for imports and environment variables are slightly different from Node and the web. In particular, we use `Deno.env.get()` to retrieve environment variables, and e.g. `import { PromptTemplate } from "https://esm.sh/langchain/prompts";` to import from a URL to match Deno conventions.

Notebook | Description
:- | :-
[rewrite.ipynb](https://github.com/langchain-ai/langchainjs/tree/master/cookbook/rewrite.ipynb) | Handle real-world questions that contain extraneous, distracting information in your RAG chains by first rewriting them before performing retrieval.
[rag_fusion.ipynb](https://github.com/langchain-ai/langchainjs/tree/master/cookbook/rag_fusion.ipynb) | Turn user queries into more search friendly queries, then query a vector store and use reciprocal rank fusion to rank the results.
[basic_critique_revise.ipynb](https://github.com/langchain-ai/langchainjs/tree/master/cookbook/basic_critique_revise.ipynb) | Basic example of correcting an LLM's output using a pattern called critique-revise, where we highlight what part of the output is wrong and re-query the LLM for a correction.
[step_back.ipynb](https://github.com/langchain-ai/langchainjs/tree/master/cookbook/step_back.ipynb) | Example of a step back prompting technique, where we ask the LLM to take a step back and rephrase the original query for a more search friendly question.

## Links discovered
- [main documentation](https://js.langchain.com)
- [Deno runtime](https://deno.com)
- [Deno Jupyter Kernel](https://deno.com/blog/v1.37)
- [rewrite.ipynb](https://github.com/langchain-ai/langchainjs/tree/master/cookbook/rewrite.ipynb)
- [rag_fusion.ipynb](https://github.com/langchain-ai/langchainjs/tree/master/cookbook/rag_fusion.ipynb)
- [basic_critique_revise.ipynb](https://github.com/langchain-ai/langchainjs/tree/master/cookbook/basic_critique_revise.ipynb)
- [step_back.ipynb](https://github.com/langchain-ai/langchainjs/tree/master/cookbook/step_back.ipynb)

--- examples/eslint.config.ts ---
import { langchainConfig, type ConfigArray } from "@langchain/eslint";

const config: ConfigArray = [
  ...langchainConfig,
  {
    files: ["src/**/*.ts"],
    rules: {
      "no-process-env": "off",
      "@typescript-eslint/no-explicit-any": "off",
      "@typescript-eslint/no-unused-vars": "off",
      "@typescript-eslint/no-floating-promises": "off",
      "no-instanceof/no-instanceof": "off",
      "@typescript-eslint/no-misused-promises": "off",
      "import/no-extraneous-dependencies": "off",
    },
  },
];

export default config;


--- examples/state_of_the_union.txt ---
Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  

Last year COVID-19 kept us apart. This year we are finally together again. 

Tonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. 

With a duty to one another to the American people to the Constitution. 

And with an unwavering resolve that freedom will always triumph over tyranny. 

Six days ago, Russia’s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. 

He thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. 

He met the Ukrainian people. 

From President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world. 

Groups of citizens blocking tanks with their bodies. Everyone from students to retirees teachers turned soldiers defending their homeland. 

In this struggle as President Zelenskyy said in his speech to the European Parliament “Light will win over darkness.” The Ukrainian Ambassador to the United States is here tonight. 

Let each of us here tonight in this Chamber send an unmistakable signal to Ukraine and to the world. 

Please rise if you are able and show that, Yes, we the United States of America stand with the Ukrainian people. 

Throughout our history we’ve learned this lesson when dictators do not pay a price for their aggression they cause more chaos.   

They keep moving.   

And the costs and the threats to America and the world keep rising.   

That’s why the NATO Alliance was created to secure peace and stability in Europe after World War 2. 

The United States is a member along with 29 other nations. 

It matters. American diplomacy matters. American resolve matters. 

Putin’s latest attack on Ukraine was premeditated and unprovoked. 

He rejected repeated efforts at diplomacy. 

He thought the West and NATO wouldn’t respond. And he thought he could divide us at home. Putin was wrong. We were ready.  Here is what we did.   

We prepared extensively and carefully. 

We spent months building a coalition of other freedom-loving nations from Europe and the Americas to Asia and Africa to confront Putin. 

I spent countless hours unifying our European allies. We shared with the world in advance what we knew Putin was planning and precisely how he would try to falsely justify his aggression.  

We countered Russia’s lies with truth.   

And now that he has acted the free world is holding him accountable. 

Along with twenty-seven members of the European Union including France, Germany, Italy, as well as countries like the United Kingdom, Canada, Japan, Korea, Australia, New Zealand, and many others, even Switzerland. 

We are inflicting pain on Russia and supporting the people of Ukraine. Putin is now isolated from the world more than ever. 

Together with our allies –we are right now enforcing powerful economic sanctions. 

We are cutting off Russia’s largest banks from the international financial system.  

Preventing Russia’s central bank from defending the Russian Ruble making Putin’s $630 Billion “war fund” worthless.   

We are choking off Russia’s access to technology that will sap its economic strength and weaken its military for years to come.  

Tonight I say to the Russian oligarchs and corrupt leaders who have bilked billions of dollars off this violent regime no more. 

The U.S. Department of Justice is assembling a dedicated task force to go after the crimes of Russian oligarchs.  

We are joining with our European allies to find and seize your yachts your luxury apartments your private jets. We are coming for your ill-begotten gains. 

And tonight I am announcing that we will join our allies in closing off American air space to all Russian flights – further isolating Russia – and adding an additional squeeze –on their economy. The Ruble has lost 30% of its value. 

The Russian stock market has lost 40% of its value and trading remains suspended. Russia’s economy is reeling and Putin alone is to blame. 

Together with our allies we are providing support to the Ukrainians in their fight for freedom. Military assistance. Economic assistance. Humanitarian assistance. 

We are giving more than $1 Billion in direct assistance to Ukraine. 

And we will continue to aid the Ukrainian people as they defend their country and to help ease their suffering.  

Let me be clear, our forces are not engaged and will not engage in conflict with Russian forces in Ukraine.  

Our forces are not going to Europe to fight in Ukraine, but to defend our NATO Allies – in the event that Putin decides to keep moving west.  

For that purpose we’ve mobilized American ground forces, air squadrons, and ship deployments to protect NATO countries including Poland, Romania, Latvia, Lithuania, and Estonia. 

As I have made crystal clear the United States and our Allies will defend every inch of territory of NATO countries with the full force of our collective power.  

And we remain clear-eyed. The Ukrainians are fighting back with pure courage. But the next few days weeks, months, will be hard on them.  

Putin has unleashed violence and chaos.  But while he may make gains on the battlefield – he will pay a continuing high price over the long run. 

And a proud Ukrainian people, who have known 30 years  of independence, have repeatedly shown that they will not tolerate anyone who tries to take their country backwards.  

To all Americans, I will be honest with you, as I’ve always promised. A Russian dictator, invading a foreign country, has costs around the world. 

And I’m taking robust action to make sure the pain of our sanctions  is targeted at Russia’s economy. And I will use every tool at our disposal to protect American businesses and consumers. 

Tonight, I can announce that the United States has worked with 30 other countries to release 60 Million barrels of oil from reserves around the world.  

America will lead that effort, releasing 30 Million barrels from our own Strategic Petroleum Reserve. And we stand ready to do more if necessary, unified with our allies.  

These steps will help blunt gas prices here at home. And I know the news about what’s happening can seem alarming. 

But I want you to know that we are going to be okay. 

When the history of this era is written Putin’s war on Ukraine will have left Russia weaker and the rest of the world stronger. 

While it shouldn’t have taken something so terrible for people around the world to see what’s at stake now everyone sees it clearly. 

We see the unity among leaders of nations and a more unified Europe a more unified West. And we see unity among the people who are gathering in cities in large crowds around the world even in Russia to demonstrate their support for Ukraine.  

In the battle between democracy and autocracy, democracies are rising to the moment, and the world is clearly choosing the side of peace and security. 

This is a real test. It’s going to take time. So let us continue to draw inspiration from the iron will of the Ukrainian people. 

To our fellow Ukrainian Americans who forge a deep bond that connects our two nations we stand with you. 

Putin may circle Kyiv with tanks, but he will never gain the hearts and souls of the Ukrainian people. 

He will never extinguish their love of freedom. He will never weaken the resolve of the free world. 

We meet tonight in an America that has lived through two of the hardest years this nation has ever faced. 

The pandemic has been punishing. 

And so many families are living paycheck to paycheck, struggling to keep up with the rising cost of food, gas, housing, and so much more. 

I understand. 

I remember when my Dad had to leave our home in Scranton, Pennsylvania to find work. I grew up in a family where if the price of food went up, you felt it. 

That’s why one of the first things I did as President was fight to pass the American Rescue Plan.  

Because people were hurting. We needed to act, and we did. 

Few pieces of legislation have done more in a critical moment in our history to lift us out of crisis. 

It fueled our efforts to vaccinate the nation and combat COVID-19. It delivered immediate economic relief for tens of millions of Americans.  

Helped put food on their table, keep a roof over their heads, and cut the cost of health insurance. 

And as my Dad used to say, it gave people a little breathing room. 

And unlike the $2 Trillion tax cut passed in the previous administration that benefitted the top 1% of Americans, the American Rescue Plan helped working people—and left no one behind. 

And it worked. It created jobs. Lots of jobs. 

In fact—our economy created over 6.5 Million new jobs just last year, more jobs created in one year  
than ever before in the history of America. 

Our economy grew at a rate of 5.7% last year, the strongest growth in nearly 40 years, the first step in bringing fundamental change to an economy that hasn’t worked for the working people of this nation for too long.  

For the past 40 years we were told that if we gave tax breaks to those at the very top, the benefits would trickle down to everyone else. 

But that trickle-down theory led to weaker economic growth, lower wages, bigger deficits, and the widest gap between those at the top and everyone else in nearly a century. 

Vice President Harris and I ran for office with a new economic vision for America. 

Invest in America. Educate Americans. Grow the workforce. Build the economy from the bottom up  
and the middle out, not from the top down.  

Because we know that when the middle class grows, the poor have a ladder up and the wealthy do very well. 

America used to have the best roads, bridges, and airports on Earth. 

Now our infrastructure is ranked 13th in the world. 

We won’t be able to compete for the jobs of the 21st Century if we don’t fix that. 

That’s why it was so important to pass the Bipartisan Infrastructure Law—the most sweeping investment to rebuild America in history. 

This was a bipartisan effort, and I want to thank the members of both parties who worked to make it happen. 

We’re done talking about infrastructure weeks. 

We’re going to have an infrastructure decade. 

It is going to transform America and put us on a path to win the economic competition of the 21st Century that we face with the rest of the world—particularly with China.  

As I’ve told Xi Jinping, it is never a good bet to bet against the American people. 

We’ll create good jobs for millions of Americans, modernizing roads, airports, ports, and waterways all across America. 

And we’ll do it all to withstand the devastating effects of the climate crisis and promote environmental justice. 

We’ll build a national network of 500,000 electric vehicle charging stations, begin to replace poisonous lead pipes—so every child—and every American—has clean water to drink at home and at school, provide affordable high-speed internet for every American—urban, suburban, rural, and tribal communities. 

4,000 projects have already been announced. 

And tonight, I’m announcing that this year we will start fixing over 65,000 miles of highway and 1,500 bridges in disrepair. 

When we use taxpayer dollars to rebuild America – we are going to Buy American: buy American products to support American jobs. 

The federal government spends about $600 Billion a year to keep the country safe and secure. 

There’s been a law on the books for almost a century 
to make sure taxpayers’ dollars support American jobs and businesses. 

Every Administration says they’ll do it, but we are actually doing it. 

We will buy American to make sure everything from the deck of an aircraft carrier to the steel on highway guardrails are made in America. 

But to compete for the best jobs of the future, we also need to level the playing field with China and other competitors. 

That’s why it is so important to pass the Bipartisan Innovation Act sitting in Congress that will make record investments in emerging technologies and American manufacturing. 

Let me give you one example of why it’s so important to pass it. 

If you travel 20 miles east of Columbus, Ohio, you’ll find 1,000 empty acres of land. 

It won’t look like much, but if you stop and look closely, you’ll see a “Field of dreams,” the ground on which America’s future will be built. 

This is where Intel, the American company that helped build Silicon Valley, is going to build its $20 billion semiconductor “mega site”. 

Up to eight state-of-the-art factories in one place. 10,000 new good-paying jobs. 

Some of the most sophisticated manufacturing in the world to make computer chips the size of a fingertip that power the world and our everyday lives. 

Smartphones. The Internet. Technology we have yet to invent. 

But that’s just the beginning. 

Intel’s CEO, Pat Gelsinger, who is here tonight, told me they are ready to increase their investment from  
$20 billion to $100 billion. 

That would be one of the biggest investments in manufacturing in American history. 

And all they’re waiting for is for you to pass this bill. 

So let’s not wait any longer. Send it to my desk. I’ll sign it.  

And we will really take off. 

And Intel is not alone. 

There’s something happening in America. 

Just look around and you’ll see an amazing story. 

The rebirth of the pride that comes from stamping products “Made In America.” The revitalization of American manufacturing.   

Companies are choosing to build new factories here, when just a few years ago, they would have built them overseas. 

That’s what is happening. Ford is investing $11 billion to build electric vehicles, creating 11,000 jobs across the country. 

GM is making the largest investment in its history—$7 billion to build electric vehicles, creating 4,000 jobs in Michigan. 

All told, we created 369,000 new manufacturing jobs in America just last year. 

Powered by people I’ve met like JoJo Burgess, from generations of union steelworkers from Pittsburgh, who’s here with us tonight. 

As Ohio Senator Sherrod Brown says, “It’s time to bury the label “Rust Belt.” 

It’s time. 

But with all the bright spots in our economy, record job growth and higher wages, too many families are struggling to keep up with the bills.  

Inflation is robbing them of the gains they might otherwise feel. 

I get it. That’s why my top priority is getting prices under control. 

Look, our economy roared back faster than most predicted, but the pandemic meant that businesses had a hard time hiring enough workers to keep up production in their factories. 

The pandemic also disrupted global supply chains. 

When factories close, it takes longer to make goods and get them from the warehouse to the store, and prices go up. 

Look at cars. 

Last year, there weren’t enough semiconductors to make all the cars that people wanted to buy. 

And guess what, prices of automobiles went up. 

So—we have a choice. 

One way to fight inflation is to drive down wages and make Americans poorer.  

I have a better plan to fight inflation. 

Lower your costs, not your wages. 

Make more cars and semiconductors in America. 

More infrastructure and innovation in America. 

More goods moving faster and cheaper in America. 

More jobs where you can earn a good living in America. 

And instead of relying on foreign supply chains, let’s make it in America. 

Economists call it “increasing the productive capacity of our economy.” 

I call it building a better America. 

My plan to fight inflation will lower your costs and lower the deficit. 

17 Nobel laureates in economics say my plan will ease long-term inflationary pressures. Top business leaders and most Americans support my plan. And here’s the plan: 

First – cut the cost of prescription drugs. Just look at insulin. One in ten Americans has diabetes. In Virginia, I met a 13-year-old boy named Joshua Davis.  

He and his Dad both have Type 1 diabetes, which means they need insulin every day. Insulin costs about $10 a vial to make.  

But drug companies charge families like Joshua and his Dad up to 30 times more. I spoke with Joshua’s mom. 

Imagine what it’s like to look at your child who needs insulin and have no idea how you’re going to pay for it.  

What it does to your dignity, your ability to look your child in the eye, to be the parent you expect to be. 

Joshua is here with us tonight. Yesterday was his birthday. Happy birthday, buddy.  

For Joshua, and for the 200,000 other young people with Type 1 diabetes, let’s cap the cost of insulin at $35 a month so everyone can afford it.  

Drug companies will still do very well. And while we’re at it let Medicare negotiate lower prices for prescription drugs, like the VA already does. 

Look, the American Rescue Plan is helping millions of families on Affordable Care Act plans save $2,400 a year on their health care premiums. Let’s close the coverage gap and make those savings permanent. 

Second – cut energy costs for families an average of $500 a year by combatting climate change.  

Let’s provide investments and tax credits to weatherize your homes and businesses to be energy efficient and you get a tax credit; double America’s clean energy production in solar, wind, and so much more;  lower the price of electric vehicles, saving you another $80 a month because you’ll never have to pay at the gas pump again. 

Third – cut the cost of child care. Many families pay up to $14,000 a year for child care per child.  

Middle-class and working families shouldn’t have to pay more than 7% of their income for care of young children.  

My plan will cut the cost in half for most families and help parents, including millions of women, who left the workforce during the pandemic because they couldn’t afford child care, to be able to get back to work. 

My plan doesn’t stop there. It also includes home and long-term care. More affordable housing. And Pre-K for every 3- and 4-year-old.  

All of these will lower costs. 

And under my plan, nobody earning less than $400,000 a year will pay an additional penny in new taxes. Nobody.  

The one thing all Americans agree on is that the tax system is not fair. We have to fix it.  

I’m not looking to punish anyone. But let’s make sure corporations and the wealthiest Americans start paying their fair share. 

Just last year, 55 Fortune 500 corporations earned $40 billion in profits and paid zero dollars in federal income tax.  

That’s simply not fair. That’s why I’ve proposed a 15% minimum tax rate for corporations. 

We got more than 130 countries to agree on a global minimum tax rate so companies can’t get out of paying their taxes at home by shipping jobs and factories overseas. 

That’s why I’ve proposed closing loopholes so the very wealthy don’t pay a lower tax rate than a teacher or a firefighter.  

So that’s my plan. It will grow the economy and lower costs for families. 

So what are we waiting for? Let’s get this done. And while you’re at it, confirm my nominees to the Federal Reserve, which plays a critical role in fighting inflation.  

My plan will not only lower costs to give families a fair shot, it will lower the deficit. 

The previous Administration not only ballooned the deficit with tax cuts for the very wealthy and corporations, it undermined the watchdogs whose job was to keep pandemic relief funds from being wasted. 

But in my administration, the watchdogs have been welcomed back. 

We’re going after the criminals who stole billions in relief money meant for small businesses and millions of Americans.  

And tonight, I’m announcing that the Justice Department will name a chief prosecutor for pandemic fraud. 

By the end of this year, the deficit will be down to less than half what it was before I took office.  

The only president ever to cut the deficit by more than one trillion dollars in a single year. 

Lowering your costs also means demanding more competition. 

I’m a capitalist, but capitalism without competition isn’t capitalism. 

It’s exploitation—and it drives up prices. 

When corporations don’t have to compete, their profits go up, your prices go up, and small businesses and family farmers and ranchers go under. 

We see it happening with ocean carriers moving goods in and out of America. 

During the pandemic, these foreign-owned companies raised prices by as much as 1,000% and made record profits. 

Tonight, I’m announcing a crackdown on these companies overcharging American businesses and consumers. 

And as Wall Street firms take over more nursing homes, quality in those homes has gone down and costs have gone up.  

That ends on my watch. 

Medicare is going to set higher standards for nursing homes and make sure your loved ones get the care they deserve and expect. 

We’ll also cut costs and keep the economy going strong by giving workers a fair shot, provide more training and apprenticeships, hire them based on their skills not degrees. 

Let’s pass the Paycheck Fairness Act and paid leave.  

Raise the minimum wage to $15 an hour and extend the Child Tax Credit, so no one has to raise a family in poverty. 

Let’s increase Pell Grants and increase our historic support of HBCUs, and invest in what Jill—our First Lady who teaches full-time—calls America’s best-kept secret: community colleges. 

And let’s pass the PRO Act when a majority of workers want to form a union—they shouldn’t be stopped.  

When we invest in our workers, when we build the economy from the bottom up and the middle out together, we can do something we haven’t done in a long time: build a better America. 

For more than two years, COVID-19 has impacted every decision in our lives and the life of the nation. 

And I know you’re tired, frustrated, and exhausted. 

But I also know this. 

Because of the progress we’ve made, because of your resilience and the tools we have, tonight I can say  
we are moving forward safely, back to more normal routines.  

We’ve reached a new moment in the fight against COVID-19, with severe cases down to a level not seen since last July.  

Just a few days ago, the Centers for Disease Control and Prevention—the CDC—issued new mask guidelines. 

Under these new guidelines, most Americans in most of the country can now be mask free.   

And based on the projections, more of the country will reach that point across the next couple of weeks. 

Thanks to the progress we have made this past year, COVID-19 need no longer control our lives.  

I know some are talking about “living with COVID-19”. Tonight – I say that we will never just accept living with COVID-19. 

We will continue to combat the virus as we do other diseases. And because this is a virus that mutates and spreads, we will stay on guard. 

Here are four common sense steps as we move forward safely.  

First, stay protected with vaccines and treatments. We know how incredibly effective vaccines are. If you’re vaccinated and boosted you have the highest degree of protection. 

We will never give up on vaccinating more Americans. Now, I know parents with kids under 5 are eager to see a vaccine authorized for their children. 

The scientists are working hard to get that done and we’ll be ready with plenty of vaccines when they do. 

We’re also ready with anti-viral treatments. If you get COVID-19, the Pfizer pill reduces your chances of ending up in the hospital by 90%.  

We’ve ordered more of these pills than anyone in the world. And Pfizer is working overtime to get us 1 Million pills this month and more than double that next month.  

And we’re launching the “Test to Treat” initiative so people can get tested at a pharmacy, and if they’re positive, receive antiviral pills on the spot at no cost.  

If you’re immunocompromised or have some other vulnerability, we have treatments and free high-quality masks. 

We’re leaving no one behind or ignoring anyone’s needs as we move forward. 

And on testing, we have made hundreds of millions of tests available for you to order for free.   

Even if you already ordered free tests tonight, I am announcing that you can order more from covidtests.gov starting next week. 

Second – we must prepare for new variants. Over the past year, we’ve gotten much better at detecting new variants. 

If necessary, we’ll be able to deploy new vaccines within 100 days instead of many more months or years.  

And, if Congress provides the funds we need, we’ll have new stockpiles of tests, masks, and pills ready if needed. 

I cannot promise a new variant won’t come. But I can promise you we’ll do everything within our power to be ready if it does.  

Third – we can end the shutdown of schools and businesses. We have the tools we need. 

It’s time for Americans to get back to work and fill our great downtowns again.  People working from home can feel safe to begin to return to the office.   

We’re doing that here in the federal government. The vast majority of federal workers will once again work in person. 

Our schools are open. Let’s keep it that way. Our kids need to be in school. 

And with 75% of adult Americans fully vaccinated and hospitalizations down by 77%, most Americans can remove their masks, return to work, stay in the classroom, and move forward safely. 

We achieved this because we provided free vaccines, treatments, tests, and masks. 

Of course, continuing this costs money. 

I will soon send Congress a request. 

The vast majority of Americans have used these tools and may want to again, so I expect Congress to pass it quickly.   

Fourth, we will continue vaccinating the world.     

We’ve sent 475 Million vaccine doses to 112 countries, more than any other nation. 

And we won’t stop. 

We have lost so much to COVID-19. Time with one another. And worst of all, so much loss of life. 

Let’s use this moment to reset. Let’s stop looking at COVID-19 as a partisan dividing line and see it for what it is: A God-awful disease.  

Let’s stop seeing each other as enemies, and start seeing each other for who we really are: Fellow Americans.  

We can’t change how divided we’ve been. But we can change how we move forward—on COVID-19 and other issues we must face together. 

I recently visited the New York City Police Department days after the funerals of Officer Wilbert Mora and his partner, Officer Jason Rivera. 

They were responding to a 9-1-1 call when a man shot and killed them with a stolen gun. 

Officer Mora was 27 years old. 

Officer Rivera was 22. 

Both Dominican Americans who’d grown up on the same streets they later chose to patrol as police officers. 

I spoke with their families and told them that we are forever in debt for their sacrifice, and we will carry on their mission to restore the trust and safety every community deserves. 

I’ve worked on these issues a long time. 

I know what works: Investing in crime preventionand community police officers who’ll walk the beat, who’ll know the neighborhood, and who can restore trust and safety. 

So let’s not abandon our streets. Or choose between safety and equal justice. 

Let’s come together to protect our communities, restore trust, and hold law enforcement accountable. 

That’s why the Justice Department required body cameras, banned chokeholds, and restricted no-knock warrants for its officers. 

That’s why the American Rescue Plan provided $350 Billion that cities, states, and counties can use to hire more police and invest in proven strategies like community violence interruption—trusted messengers breaking the cycle of violence and trauma and giving young people hope.  

We should all agree: The answer is not to Defund the police. The answer is to FUND the police with the resources and training they need to protect our communities. 

I ask Democrats and Republicans alike: Pass my budget and keep our neighborhoods safe.  

And I will keep doing everything in my power to crack down on gun trafficking and ghost guns you can buy online and make at home—they have no serial numbers and can’t be traced. 

And I ask Congress to pass proven measures to reduce gun violence. Pass universal background checks. Why should anyone on a terrorist list be able to purchase a weapon? 

Ban assault weapons and high-capacity magazines. 

Repeal the liability shield that makes gun manufacturers the only industry in America that can’t be sued. 

These laws don’t infringe on the Second Amendment. They save lives. 

The most fundamental right in America is the right to vote – and to have it counted. And it’s under assault. 

In state after state, new laws have been passed, not only to suppress the vote, but to subvert entire elections. 

We cannot let this happen. 

Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. 

Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. 

One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. 

And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence. 

A former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she’s been nominated, she’s received a broad range of support—from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. 

And if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. 

We can do both. At our border, we’ve installed new technology like cutting-edge scanners to better detect drug smuggling.  

We’ve set up joint patrols with Mexico and Guatemala to catch more human traffickers.  

We’re putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster. 

We’re securing commitments and supporting partners in South and Central America to host more refugees and secure their own borders. 

We can do all this while keeping lit the torch of liberty that has led generations of immigrants to this land—my forefathers and so many of yours. 

Provide a pathway to citizenship for Dreamers, those on temporary status, farm workers, and essential workers. 

Revise our laws so businesses have the workers they need and families don’t wait decades to reunite. 

It’s not only the right thing to do—it’s the economically smart thing to do. 

That’s why immigration reform is supported by everyone from labor unions to religious leaders to the U.S. Chamber of Commerce. 

Let’s get it done once and for all. 

Advancing liberty and justice also requires protecting the rights of women. 

The constitutional right affirmed in Roe v. Wade—standing precedent for half a century—is under attack as never before. 

If we want to go forward—not backward—we must protect access to health care. Preserve a woman’s right to choose. And let’s continue to advance maternal health care in America. 

And for our LGBTQ+ Americans, let’s finally get the bipartisan Equality Act to my desk. The onslaught of state laws targeting transgender Americans and their families is wrong. 

As I said last year, especially to our younger transgender Americans, I will always have your back as your President, so you can be yourself and reach your God-given potential. 

While it often appears that we never agree, that isn’t true. I signed 80 bipartisan bills into law last year. From preventing government shutdowns to protecting Asian-Americans from still-too-common hate crimes to reforming military justice. 

And soon, we’ll strengthen the Violence Against Women Act that I first wrote three decades ago. It is important for us to show the nation that we can come together and do big things. 

So tonight I’m offering a Unity Agenda for the Nation. Four big things we can do together.  

First, beat the opioid epidemic. 

There is so much we can do. Increase funding for prevention, treatment, harm reduction, and recovery.  

Get rid of outdated rules that stop doctors from prescribing treatments. And stop the flow of illicit drugs by working with state and local law enforcement to go after traffickers. 

If you’re suffering from addiction, know you are not alone. I believe in recovery, and I celebrate the 23 million Americans in recovery. 

Second, let’s take on mental health. Especially among our children, whose lives and education have been turned upside down.  

The American Rescue Plan gave schools money to hire teachers and help students make up for lost learning.  

I urge every parent to make sure your school does just that. And we can all play a part—sign up to be a tutor or a mentor. 

Children were also struggling before the pandemic. Bullying, violence, trauma, and the harms of social media. 

As Frances Haugen, who is here with us tonight, has shown, we must hold social media platforms accountable for the national experiment they’re conducting on our children for profit. 

It’s time to strengthen privacy protections, ban targeted advertising to children, demand tech companies stop collecting personal data on our children. 

And let’s get all Americans the mental health services they need. More people they can turn to for help, and full parity between physical and mental health care. 

Third, support our veterans. 

Veterans are the best of us. 

I’ve always believed that we have a sacred obligation to equip all those we send to war and care for them and their families when they come home. 

My administration is providing assistance with job training and housing, and now helping lower-income veterans get VA care debt-free.  

Our troops in Iraq and Afghanistan faced many dangers. 

One was stationed at bases and breathing in toxic smoke from “burn pits” that incinerated wastes of war—medical and hazard material, jet fuel, and more. 

When they came home, many of the world’s fittest and best trained warriors were never the same. 

Headaches. Numbness. Dizziness. 

A cancer that would put them in a flag-draped coffin. 

I know. 

One of those soldiers was my son Major Beau Biden. 

We don’t know for sure if a burn pit was the cause of his brain cancer, or the diseases of so many of our troops. 

But I’m committed to finding out everything we can. 

Committed to military families like Danielle Robinson from Ohio. 

The widow of Sergeant First Class Heath Robinson.  

He was born a soldier. Army National Guard. Combat medic in Kosovo and Iraq. 

Stationed near Baghdad, just yards from burn pits the size of football fields. 

Heath’s widow Danielle is here with us tonight. They loved going to Ohio State football games. He loved building Legos with their daughter. 

But cancer from prolonged exposure to burn pits ravaged Heath’s lungs and body. 

Danielle says Heath was a fighter to the very end. 

He didn’t know how to stop fighting, and neither did she. 

Through her pain she found purpose to demand we do better. 

Tonight, Danielle—we are. 

The VA is pioneering new ways of linking toxic exposures to diseases, already helping more veterans get benefits. 

And tonight, I’m announcing we’re expanding eligibility to veterans suffering from nine respiratory cancers. 

I’m also calling on Congress: pass a law to make sure veterans devastated by toxic exposures in Iraq and Afghanistan finally get the benefits and comprehensive health care they deserve. 

And fourth, let’s end cancer as we know it. 

This is personal to me and Jill, to Kamala, and to so many of you. 

Cancer is the #2 cause of death in America–second only to heart disease. 

Last month, I announced our plan to supercharge  
the Cancer Moonshot that President Obama asked me to lead six years ago. 

Our goal is to cut the cancer death rate by at least 50% over the next 25 years, turn more cancers from death sentences into treatable diseases.  

More support for patients and families. 

To get there, I call on Congress to fund ARPA-H, the Advanced Research Projects Agency for Health. 

It’s based on DARPA—the Defense Department project that led to the Internet, GPS, and so much more.  

ARPA-H will have a singular purpose—to drive breakthroughs in cancer, Alzheimer’s, diabetes, and more. 

A unity agenda for the nation. 

We can do this. 

My fellow Americans—tonight , we have gathered in a sacred space—the citadel of our democracy. 

In this Capitol, generation after generation, Americans have debated great questions amid great strife, and have done great things. 

We have fought for freedom, expanded liberty, defeated totalitarianism and terror. 

And built the strongest, freest, and most prosperous nation the world has ever known. 

Now is the hour. 

Our moment of responsibility. 

Our test of resolve and conscience, of history itself. 

It is in this moment that our character is formed. Our purpose is found. Our future is forged. 

Well I know this nation.  

We will meet the test. 

To protect freedom and liberty, to expand fairness and opportunity. 

We will save democracy. 

As hard as these times have been, I am more optimistic about America today than I have been my whole life. 

Because I see the future that is within our grasp. 

Because I know there is simply nothing beyond our capacity. 

We are the only nation on Earth that has always turned every crisis we have faced into an opportunity. 

The only nation that can be defined by a single word: possibilities. 

So on this night, in our 245th year as a nation, I have come to report on the State of the Union. 

And my report is this: the State of the Union is strong—because you, the American people, are strong. 

We are stronger today than we were a year ago. 

And we will be stronger a year from now than we are today. 

Now is our moment to meet and overcome the challenges of our time. 

And we will, as one people. 

One America. 

The United States of America. 

May God bless you all. May God protect our troops.

--- examples/src/index.ts ---
import path from "path";
import url from "url";
import { awaitAllCallbacks } from "@langchain/core/callbacks/promises";

const [exampleName, ...args] = process.argv.slice(2);

if (!exampleName) {
  console.error("Please provide path to example to run");
  process.exit(1);
}

// Allow people to pass all possible variations of a path to an example
// ./src/foo.ts, ./dist/foo.js, src/foo.ts, dist/foo.js, foo.ts
let exampleRelativePath = exampleName;

if (exampleRelativePath.startsWith("./examples/")) {
  exampleRelativePath = exampleName.slice(11);
} else if (exampleRelativePath.startsWith("examples/")) {
  exampleRelativePath = exampleName.slice(9);
}

if (exampleRelativePath.startsWith("./src/")) {
  exampleRelativePath = exampleRelativePath.slice(6);
} else if (exampleRelativePath.startsWith("./dist/")) {
  exampleRelativePath = exampleRelativePath.slice(7);
} else if (exampleRelativePath.startsWith("src/")) {
  exampleRelativePath = exampleRelativePath.slice(4);
} else if (exampleRelativePath.startsWith("dist/")) {
  exampleRelativePath = exampleRelativePath.slice(5);
}

let runExample;
try {
  ({ run: runExample } = await import(
    path.join(
      path.dirname(url.fileURLToPath(import.meta.url)),
      exampleRelativePath
    )
  ));
} catch (e) {
  console.log(e);
  throw new Error(`Could not load example ${exampleName}: ${e}`);
}

if (runExample) {
  const maybePromise = runExample(args);

  if (maybePromise instanceof Promise) {
    maybePromise
      .catch((e) => {
        console.error(`Example failed with:`);
        console.error(e);
      })
      .finally(() => awaitAllCallbacks());
  }
}


--- examples/src/createAgent/accessExternalContext.ts ---
/**
 * Access External Context for Runtime Parameters
 *
 * Context contains runtime parameters like user ID, database connections, and static
 * configuration that affects how the agent behaves. This context can influence the
 * system prompt and tool behavior.
 *
 * Context vs State Distinction:
 * - Context: Static runtime parameters (user ID, DB connections, config)
 *   - Set once per session/request
 *   - Doesn't change during conversation
 *   - Used to look up user info, configure behavior
 *
 * - State: Dynamic conversation data (messages, memory, session variables)
 *   - Modified over time during interaction
 *   - Persists and evolves through the conversation
 *   - Managed by the agent framework
 *
 * Example Scenario:
 * A support agent that needs the current user's ID to look up their account details
 * and tailor responses. The user ID is static context, but the conversation messages
 * are dynamic state.
 */

import fs from "node:fs/promises";
import { createAgent, dynamicSystemPromptMiddleware, tool } from "langchain";
import { ChatOpenAI } from "@langchain/openai";
import { z } from "zod";

/**
 * Runtime context - static parameters set per session
 */
const runtimeContext: {
  userId?: string;
  username?: string;
  dbConnection?: string;
  userTier?: string;
} = {};

/**
 * Simulated database lookup using context
 */
function getUserDetails(userId: string) {
  // In a real app, this would query your database using the connection from context
  const userDatabase = {
    user123: { username: "john_doe", tier: "premium", joinDate: "2023-01-15" },
    user456: { username: "jane_smith", tier: "basic", joinDate: "2023-06-20" },
    user789: {
      username: "bob_wilson",
      tier: "enterprise",
      joinDate: "2022-03-10",
    },
  };

  return (
    userDatabase[userId as keyof typeof userDatabase] || {
      username: "unknown",
      tier: "basic",
      joinDate: "unknown",
    }
  );
}

/**
 * Tool that uses runtime context for personalized behavior
 */
const accountInfoTool = tool(
  async (input: { action: string }) => {
    const userId = runtimeContext.userId ?? "unknown";
    const userDetails = getUserDetails(userId);

    return `Account ${input.action} for ${userDetails.username} (${userDetails.tier} tier, member since ${userDetails.joinDate})`;
  },
  {
    name: "account_info",
    description: "Get account information for the current user",
    schema: z.object({
      action: z.string().describe("Action to perform: check, update, etc."),
    }),
  }
);

/**
 * Create agent that uses runtime context to customize behavior
 */
const supportAgent = createAgent({
  model: new ChatOpenAI({ model: "gpt-4" }),
  tools: [accountInfoTool],
  middleware: [
    dynamicSystemPromptMiddleware(async () => {
      // Access runtime context to customize system prompt
      const userId = runtimeContext.userId ?? "unknown";
      const userDetails = getUserDetails(userId);

      return `You are a customer support agent helping ${userDetails.username}.

User Context (from runtime parameters):
- User ID: ${userId}
- Account Tier: ${userDetails.tier}
- Member Since: ${userDetails.joinDate}

Tailor your responses based on their ${userDetails.tier} tier. Enterprise users get priority treatment, premium users get detailed explanations, basic users get simple guidance.`;
    }),
  ],
});

/**
 * Function to handle user requests with runtime context
 * @param userId - Static user identifier from session/auth
 * @param query - User's message (this becomes part of dynamic state)
 */
async function handleSupportRequest(userId: string, query: string) {
  // Set runtime context (static for this session)
  runtimeContext.userId = userId;

  return supportAgent.invoke({
    messages: [{ role: "user", content: query }],
  });
}

/**
 * Example: Different users get different treatment based on context
 */
console.log("=== Premium User Request ===");
const premiumResult = await handleSupportRequest(
  "user123", // Premium user ID
  "I'm having trouble with my account settings"
);
console.log(
  "Response:",
  premiumResult.messages[premiumResult.messages.length - 1].content
);

console.log("\n=== Basic User Request ===");
const basicResult = await handleSupportRequest(
  "user456", // Basic user ID
  "I'm having trouble with my account settings"
);
console.log(
  "Response:",
  basicResult.messages[basicResult.messages.length - 1].content
);

console.log("\n=== Enterprise User Request ===");
const enterpriseResult = await handleSupportRequest(
  "user789", // Enterprise user ID
  "I'm having trouble with my account settings"
);
console.log(
  "Response:",
  enterpriseResult.messages[enterpriseResult.messages.length - 1].content
);

/**
 * Get the current file's path and derive the output PNG path
 */
const currentFilePath = new URL(import.meta.url).pathname;
const outputPath = currentFilePath.replace(/\.ts$/, ".png");
console.log(`\nSaving visualization to: ${outputPath}`);
await fs.writeFile(outputPath, await supportAgent.drawMermaidPng());

/**
 * Example Output:
 * === Premium User Request ===
 * Response: I'm sorry to hear that you're experiencing trouble with your account settings, John.
 * As a premium user, you are entitled to a detailed explanation and solution to your problem.
 * Could you please elaborate more on the issue? Providing specific details such as the feature
 * you're having trouble with, or any error messages you're seeing, will assist me greatly in
 * pinpointing and resolving your issue.
 *
 * === Basic User Request ===
 * Response: Sure, I'd be happy to help. Could you please tell me more about the issues you're
 * facing with your account settings?
 *
 * === Enterprise User Request ===
 * Response: I'm sorry to hear you're having trouble with your account settings, Bob. As an enterprise
 * user, you are given priority assistance. Could you please provide more details about the issue?
 * It will allow me to assist you more effectively.
 */


--- examples/src/createAgent/accessExternalContextInTools.ts ---
/**
 * Access External Context in Tools for User-Specific Data
 *
 * Tools can access runtime context (like user ID) to lookup user-specific data
 * from databases or APIs. This enables secure, personalized tool behavior.
 *
 * Key Benefits:
 * - User-Specific Data Access: Tools lookup data belonging to the authenticated user
 * - Security Through Context: User ID from context ensures tools only access authorized data
 * - Database Integration: Context provides connection details and user identity for queries
 *
 * Common Pattern:
 * 1. User ID passed in context from authentication/session
 * 2. Tool accesses context to get user ID
 * 3. Tool queries database using user ID to get user-specific data
 * 4. Tool returns personalized results
 *
 * Example Scenario:
 * An e-commerce assistant where tools need to access a user's purchase history,
 * saved items, or account details. The user ID from context ensures each user
 * only sees their own data, providing both personalization and security.
 */

import fs from "node:fs/promises";
import { createAgent, tool } from "langchain";
import {
  getContextVariable,
  setContextVariable,
} from "@langchain/core/context";
import { z } from "zod";

/**
 * Simulated database with user-specific data
 */
const userPurchasesDB = {
  user123: [
    {
      id: "p1",
      item: "Wireless Headphones",
      date: "2024-01-15",
      price: 199.99,
    },
    { id: "p2", item: "Phone Case", date: "2024-02-03", price: 29.99 },
    { id: "p3", item: "Laptop Stand", date: "2024-02-20", price: 89.99 },
  ],
  user456: [
    { id: "p4", item: "Bluetooth Speaker", date: "2024-01-20", price: 79.99 },
    { id: "p5", item: "Charging Cable", date: "2024-02-15", price: 19.99 },
  ],
  user789: [
    { id: "p6", item: "Monitor", date: "2024-01-05", price: 349.99 },
    { id: "p7", item: "Keyboard", date: "2024-01-06", price: 129.99 },
    { id: "p8", item: "Mouse", date: "2024-01-06", price: 69.99 },
    { id: "p9", item: "Webcam", date: "2024-02-10", price: 159.99 },
  ],
};

/**
 * Tool that accesses user-specific data via context
 */
const getUserPurchasesTool = tool(
  async (input: { limit?: number }) => {
    /**
     * Access user ID from context - this ensures security and personalization
     */
    const userId = getContextVariable("userId");

    if (!userId) {
      return "Error: User not authenticated. Please log in to view purchases.";
    }

    console.log(`Looking up purchases for user: ${userId}`);

    // Query database using user ID from context
    const userPurchases =
      userPurchasesDB[userId as keyof typeof userPurchasesDB] || [];

    if (userPurchases.length === 0) {
      return "No purchases found for your account.";
    }

    // Apply limit if specified
    const purchases = input.limit
      ? userPurchases.slice(0, input.limit)
      : userPurchases;

    const purchaseList = purchases
      .map((p) => `• ${p.item} - $${p.price} (${p.date})`)
      .join("\n");

    return `Your recent purchases:\n${purchaseList}`;
  },
  {
    name: "get_user_purchases",
    description: "Get the authenticated user's purchase history",
    schema: z.object({
      limit: z
        .number()
        .optional()
        .describe("Maximum number of purchases to return"),
    }),
  }
);

/**
 * Tool for user-specific account information
 */
const getAccountInfoTool = tool(
  async () => {
    const userId = getContextVariable("userId");

    if (!userId) {
      return "Error: User not authenticated.";
    }

    console.log(`Getting account info for user: ${userId}`);

    /**
     * Simulate account lookup
     */
    const accountInfo = {
      user123: { name: "John Doe", memberSince: "2023-01-15", totalOrders: 3 },
      user456: {
        name: "Jane Smith",
        memberSince: "2023-06-20",
        totalOrders: 2,
      },
      user789: {
        name: "Bob Wilson",
        memberSince: "2022-03-10",
        totalOrders: 4,
      },
    };

    const userAccount = accountInfo[userId as keyof typeof accountInfo];

    if (!userAccount) {
      return "Account information not found.";
    }

    return `Account: ${userAccount.name}\nMember since: ${userAccount.memberSince}\nTotal orders: ${userAccount.totalOrders}`;
  },
  {
    name: "get_account_info",
    description: "Get the authenticated user's account information",
    schema: z.object({}),
  }
);

/**
 * Create e-commerce assistant agent
 */
const ecommerceAgent = createAgent({
  model: "openai:gpt-4o",
  tools: [getUserPurchasesTool, getAccountInfoTool],
  systemPrompt:
    "You are a helpful e-commerce assistant. You can help users check their purchase history and account information. Always use the available tools to provide accurate, personalized information.",
});

/**
 * Function to handle authenticated user requests
 * @param userId - The user ID
 * @param query - The user query
 */
async function handleUserRequest(userId: string, query: string) {
  // Set user ID in context - this would typically come from authentication/session
  setContextVariable("userId", userId);

  console.log(`\n--- Handling request for user: ${userId} ---`);
  console.log(`Query: ${query}`);

  const result = await ecommerceAgent.invoke({
    messages: query,
  });

  console.log("Response:", result.messages[result.messages.length - 1].content);
}

/**
 * Example: User checking their purchase history
 */
console.log("=== Purchase History Lookup ===");
await handleUserRequest("user123", "Can you show me my recent purchases?");

/**
 * Example: User checking account information
 */
console.log("\n=== Account Information Lookup ===");
await handleUserRequest("user456", "What's my account information?");

/**
 * Example: User with more purchase history
 */
console.log("\n=== Limited Purchase History ===");
await handleUserRequest("user789", "Show me my last 2 purchases");

/**
 * Example: Unauthenticated request (no user ID in context)
 */
console.log("\n=== Unauthenticated Request ===");
await handleUserRequest("", "Show me my purchases");

/**
 * Get the current file's path and derive the output PNG path
 */
const currentFilePath = new URL(import.meta.url).pathname;
const outputPath = currentFilePath.replace(/\.ts$/, ".png");
console.log(`\nSaving visualization to: ${outputPath}`);
await fs.writeFile(outputPath, await ecommerceAgent.drawMermaidPng());

/**
 * Example Output:
 * === Purchase History Lookup ===
 *
 * --- Handling request for user: user123 ---
 * Query: Can you show me my recent purchases?
 * Looking up purchases for user: user123
 * Response: Here are your recent purchases:
 *
 * 1. **Wireless Headphones** - $199.99 (Purchased on 2024-01-15)
 * 2. **Phone Case** - $29.99 (Purchased on 2024-02-03)
 * 3. **Laptop Stand** - $89.99 (Purchased on 2024-02-20)
 *
 * === Account Information Lookup ===
 *
 * --- Handling request for user: user456 ---
 * Query: What's my account information?
 * Getting account info for user: user456
 * Response: Here is your account information:
 *
 * - **Name:** Jane Smith
 * - **Member since:** June 20, 2023
 * - **Total orders:** 2
 *
 * === Limited Purchase History ===
 *
 * --- Handling request for user: user789 ---
 * Query: Show me my last 2 purchases
 * Looking up purchases for user: user789
 * Response: Here are your last two purchases:
 *
 * 1. **Keyboard** - $129.99 (Purchased on 2024-01-06)
 * 2. **Monitor** - $349.99 (Purchased on 2024-01-05)
 *
 * === Unauthenticated Request ===
 *
 * --- Handling request for user:  ---
 * Query: Show me my purchases
 * Response: It seems that you're not currently logged in. Please log in to your account to view
 * your purchase history. If you need further assistance, feel free to ask!
 */


--- examples/src/createAgent/accessLongTermMemory.ts ---
/**
 * Access Long-Term Memory for Procedural Instructions
 *
 * This pattern programmatically retrieves and includes procedural memory components
 * in the system prompt. Unlike semantic search, this is deterministic lookup of
 * stored instructions, preferences, or system behaviors that evolve over time.
 *
 * Procedural vs Semantic Memory:
 * - Procedural: Fixed instructions, preferences, learned behaviors (this example)
 *   - Retrieved programmatically, not by similarity
 *   - Always included in system prompt
 *   - Updated over time based on user interactions
 *
 * - Semantic: Content-based search, contextual retrieval (better suited for tools, see `accessLongTermMemoryInTools.ts`)
 *   - Agent decides what to search for
 *   - Retrieved based on similarity/relevance
 *   - Used for answering specific questions
 *
 * Example Scenario:
 * A coding assistant like Cursor that learns user preferences over time. It stores
 * procedural instructions like "always use TypeScript", "prefer functional patterns",
 * "avoid console.log in production" and programmatically includes these in every
 * system prompt to maintain consistent behavior.
 */

import fs from "node:fs/promises";
import { createAgent, dynamicSystemPromptMiddleware, tool } from "langchain";
import { InMemoryStore } from "@langchain/langgraph";
import { ChatOpenAI } from "@langchain/openai";
import { z } from "zod";

/**
 * Long-term memory store using LangChain primitives
 */
const store = new InMemoryStore();

/**
 * Interface for procedural memory data
 */
interface ProceduralMemory {
  userId: string;
  codingPreferences: string[];
  behaviorInstructions: string[];
  communicationStyle: string;
  lastUpdated: string;
}

/**
 * Initialize some sample procedural memory data
 */
await store.put(["procedural_memory"], "user123", {
  userId: "user123",
  codingPreferences: [
    "Always use TypeScript for new projects",
    "Prefer functional programming patterns",
    "Use const over let when possible",
    "Write comprehensive JSDoc comments",
  ],
  behaviorInstructions: [
    "Provide detailed explanations for complex concepts",
    "Always suggest best practices",
    "Include code examples in responses",
  ],
  communicationStyle: "technical and detailed",
  lastUpdated: "2024-02-20",
});

await store.put(["procedural_memory"], "user456", {
  userId: "user456",
  codingPreferences: [
    "Keep code simple and readable",
    "Avoid over-engineering solutions",
    "Prefer vanilla JavaScript over frameworks when possible",
  ],
  behaviorInstructions: [
    "Keep explanations concise",
    "Focus on practical solutions",
    "Avoid jargon when possible",
  ],
  communicationStyle: "simple and direct",
  lastUpdated: "2024-02-18",
});

/**
 * Tool to get user's procedural memory
 */
const getProceduralMemoryTool = tool(
  async (_: never, config): Promise<string> => {
    if (!store) {
      throw new Error("Store is required when compiling the graph");
    }

    const userId = config.configurable?.userId;
    if (!userId) {
      throw new Error("userId is required in the config");
    }

    const memoryData = await store.get(["procedural_memory"], userId);
    return memoryData
      ? JSON.stringify(memoryData.value, null, 2)
      : "No procedural memory found";
  },
  {
    name: "get_procedural_memory",
    description: "Get user's procedural memory and preferences",
    schema: z.object({}),
  }
);

/**
 * Tool to update user's procedural memory
 */
const updateProceduralMemoryTool = tool(
  async (input, config): Promise<string> => {
    if (!store) {
      throw new Error("Store is required when compiling the graph");
    }

    const userId = config.configurable?.userId;
    if (!userId) {
      throw new Error("userId is required in the config");
    }

    // Get existing memory
    const existingMemory = await store.get(["procedural_memory"], userId);
    const currentMemory = (existingMemory?.value as ProceduralMemory) || {
      userId,
      codingPreferences: [],
      behaviorInstructions: [],
      communicationStyle: "neutral",
      lastUpdated: "",
    };

    // Update with new values
    const updatedMemory = {
      ...currentMemory,
      ...input,
      lastUpdated: new Date().toISOString().split("T")[0],
    };

    await store.put(["procedural_memory"], userId, updatedMemory);
    return "Successfully updated procedural memory";
  },
  {
    name: "update_procedural_memory",
    description: "Update user's procedural memory and preferences",
    schema: z.object({
      codingPreferences: z
        .array(z.string())
        .optional()
        .describe("Coding preferences to add"),
      behaviorInstructions: z
        .array(z.string())
        .optional()
        .describe("Behavior instructions to update"),
      communicationStyle: z
        .string()
        .optional()
        .describe("Communication style preference"),
    }),
  }
);

/**
 * Create coding assistant with procedural memory from LangChain store
 */
const codingAssistant = createAgent({
  model: new ChatOpenAI({ model: "gpt-4" }),
  tools: [getProceduralMemoryTool, updateProceduralMemoryTool],
  store, // Pass the store to the agent
  middleware: [
    dynamicSystemPromptMiddleware(async (_, runtime) => {
      /**
       * PROGRAMMATIC LOOKUP: Always retrieve procedural memory for this user
       * This is deterministic - we're not searching, just loading stored preferences
       */
      const storeInstance = runtime.store;
      const userId = runtime.configurable?.userId as string;

      let memoryPrompt = "";
      if (storeInstance && userId) {
        const memoryData = await storeInstance.get(
          ["procedural_memory"],
          userId
        );
        if (memoryData?.value) {
          const memory = memoryData.value as ProceduralMemory;
          memoryPrompt = `
PROCEDURAL MEMORY (${memory.lastUpdated}):

Coding Preferences:
${memory.codingPreferences.map((pref) => `• ${pref}`).join("\n")}

Behavior Instructions:
${memory.behaviorInstructions.map((inst) => `• ${inst}`).join("\n")}

Communication Style: ${memory.communicationStyle}

---`;
        }
      }

      return `You are a coding assistant with access to the user's long-term preferences stored in LangChain memory.

${memoryPrompt}

Use the above procedural memory to maintain consistent behavior and preferences across all interactions. You can also use the tools to retrieve or update memory as needed.`;
    }),
  ],
});

/**
 * Example: Different users get different behaviors based on their procedural memory
 */
console.log("=== User with Technical Preferences ===");
const techResult = await codingAssistant.invoke(
  {
    messages: [
      {
        role: "user",
        content: "How should I implement a simple counter component?",
      },
    ],
  },
  { configurable: { userId: "user123" } }
);
console.log(
  "Response:",
  techResult.messages[techResult.messages.length - 1].content
);

console.log("\n=== User with Simple Preferences ===");
const simpleResult = await codingAssistant.invoke(
  {
    messages: [
      {
        role: "user",
        content: "How should I implement a simple counter component?",
      },
    ],
  },
  { configurable: { userId: "user456" } }
);
console.log(
  "Response:",
  simpleResult.messages[simpleResult.messages.length - 1].content
);

/**
 * Example: Update procedural memory using tools
 */
console.log("\n=== Updating Procedural Memory via Tools ===");
const updateResult = await codingAssistant.invoke(
  {
    messages: [
      {
        role: "user",
        content:
          "Please add 'Always include error handling in async functions' to my coding preferences",
      },
    ],
  },
  { configurable: { userId: "user123" } }
);
console.log(
  "Update Response:",
  updateResult.messages[updateResult.messages.length - 1].content
);

/**
 * Example: Verify memory was updated
 */
console.log("\n=== Verifying Updated Memory ===");
const verifyResult = await codingAssistant.invoke(
  {
    messages: [
      { role: "user", content: "What are my current coding preferences?" },
    ],
  },
  { configurable: { userId: "user123" } }
);
console.log(
  "Verification Response:",
  verifyResult.messages[verifyResult.messages.length - 1].content
);

/**
 * Example: Direct store access (for debugging/admin purposes)
 */
console.log("\n=== Direct Store Access ===");
const directMemory = await store.get(["procedural_memory"], "user123");
console.log(
  "Direct store access:",
  JSON.stringify(directMemory?.value, null, 2)
);

/**
 * Get the current file's path and derive the output PNG path
 */
const currentFilePath = new URL(import.meta.url).pathname;
const outputPath = currentFilePath.replace(/\.ts$/, ".png");
console.log(`\nSaving visualization to: ${outputPath}`);
await fs.writeFile(outputPath, await codingAssistant.drawMermaidPng());

/**
 * Example Output:
 *
 * === User with Technical Preferences ===
 * Response: For implementing a counter component, I'd recommend using TypeScript for type safety.
 * Here's a detailed implementation following your preferences:
 *
 * ```tsx
 * interface CounterProps {
 *   initialValue?: number;
 * }
 *
 * const Counter: React.FC<CounterProps> = ({ initialValue = 0 }) => {
 *   const [count, setCount] = useState<number>(initialValue);
 *   // ... detailed implementation with JSDoc comments
 * ```
 *
 * === User with Simple Preferences ===
 * Response: Here's a simple counter component:
 *
 * ```javascript
 * function Counter() {
 *   const [count, setCount] = useState(0);
 *   return (
 *     <div>
 *       <p>Count: {count}</p>
 *       <button onClick={() => setCount(count + 1)}>+</button>
 *     </div>
 *   );
 * }
 * ```
 *
 * This keeps it straightforward and readable.
 *
 * === Updating Procedural Memory via Tools ===
 * Update Response: I've successfully added 'Always include error handling in async functions'
 * to your coding preferences. Your procedural memory has been updated.
 *
 * === Direct Store Access ===
 * Direct store access: {
 *   "userId": "user123",
 *   "codingPreferences": [
 *     "Always use TypeScript for new projects",
 *     "Prefer functional programming patterns",
 *     "Use const over let when possible",
 *     "Write comprehensive JSDoc comments",
 *     "Always include error handling in async functions"
 *   ],
 *   "behaviorInstructions": [...],
 *   "communicationStyle": "technical and detailed",
 *   "lastUpdated": "2024-02-21"
 * }
 */


--- examples/src/createAgent/accessLongTermMemoryInTools.ts ---
/**
 * Access Long Term Memory in Tools
 *
 * This allows tools to query and utilize persistent memory stores during execution, allowing them to leverage
 * historical knowledge and learned patterns in their operations.
 *
 * Why this is important:
 * - Knowledge-Enhanced Actions:
 *   Tools can incorporate past learnings and historical data to make better decisions
 * - Persistent Context Awareness:
 *   Maintains continuity across sessions by accessing previously stored information
 * - Intelligent Information Retrieval:
 *   Enables sophisticated search and recommendation capabilities based on accumulated knowledge
 *
 * Example Scenario:
 * A personal assistant that remembers each user's preferences and recent
 * interactions. Tools fetch preferences (e.g., summary style) and update them
 * over time, enabling stable, user-specific behavior across sessions.
 */

import fs from "node:fs/promises";
import { createAgent, tool } from "langchain";
import { InMemoryStore } from "@langchain/langgraph";
import { ChatOpenAI } from "@langchain/openai";
import { z } from "zod";

const model = new ChatOpenAI({ model: "gpt-4o", temperature: 0 });

/**
 * Long-term memory store using LangChain primitives
 */
const store = new InMemoryStore();

/**
 * Initialize sample user data and knowledge in the store
 */
await store.put(["preferences"], "john_123", [
  "Prefers detailed technical explanations with code examples",
  "Likes step-by-step tutorials",
]);

await store.put(["context"], "john_123", [
  "Previously worked on React components and TypeScript projects",
  "Has experience with web development",
]);

await store.put(["preferences"], "sarah_456", [
  "Likes concise summaries and visual diagrams",
  "Prefers high-level overviews",
]);

await store.put(["context"], "sarah_456", [
  "Is a project manager focusing on team coordination",
  "Has background in project management",
]);

await store.put(["knowledge"], "web_dev", [
  "Previous discussion about implementing authentication in web applications",
  "Best practices for React component architecture",
  "Common patterns for state management",
]);

await store.put(["knowledge"], "database", [
  "Best practices for database optimization and query performance",
  "SQL indexing strategies",
  "Database design principles",
]);

await store.put(["interactions"], "john_123", []);
await store.put(["interactions"], "sarah_456", []);

/**
 * Knowledge retrieval tool that uses long-term memory
 */
const knowledgeRetrievalTool = tool(
  async (input: { query: string; userId?: string }) => {
    if (!store) {
      throw new Error("Store is required when compiling the graph");
    }

    const memories: string[] = [];

    /**
     * Get user-specific preferences and context if userId provided
     */
    if (input.userId) {
      const userPreferences = await store.get(["preferences"], input.userId);
      const userContext = await store.get(["context"], input.userId);

      if (userPreferences?.value) {
        memories.push(
          ...userPreferences.value.map(
            (pref: string) => `User preference: ${pref}`
          )
        );
      }
      if (userContext?.value) {
        memories.push(
          ...userContext.value.map((ctx: string) => `User context: ${ctx}`)
        );
      }
    }

    /**
     * Get relevant domain knowledge based on query keywords
     */
    const domains = ["web_dev", "database"];
    for (const domain of domains) {
      if (
        input.query.toLowerCase().includes(domain.replace("_", " ")) ||
        input.query.toLowerCase().includes("react") ||
        input.query.toLowerCase().includes("auth") ||
        input.query.toLowerCase().includes("database") ||
        input.query.toLowerCase().includes("optimization")
      ) {
        const domainKnowledge = await store.get(["knowledge"], domain);
        if (domainKnowledge?.value) {
          memories.push(
            ...domainKnowledge.value.map(
              (knowledge: string) => `Knowledge: ${knowledge}`
            )
          );
        }
      }
    }

    /**
     * Store this interaction for future reference
     */
    if (input.userId) {
      const interactions = await store.get(["interactions"], input.userId);
      const currentInteractions = interactions?.value || [];
      currentInteractions.push({
        query: input.query,
        timestamp: new Date().toISOString(),
      });
      await store.put(["interactions"], input.userId, currentInteractions);
    }

    const memoryContext = memories.slice(0, 5).join("\n- ");

    return `Found ${memories.length} relevant memories for "${input.query}":

- ${memoryContext}

This information helps me provide more personalized and contextually relevant responses based on your history and preferences.`;
  },
  {
    name: "knowledge_retrieval",
    description:
      "Retrieve relevant information from long-term memory based on user query and preferences",
    schema: z.object({
      query: z.string().describe("The query to search for in long-term memory"),
      userId: z
        .string()
        .optional()
        .describe("Optional user ID to personalize results"),
    }),
  }
);

/**
 * Preference learning tool that updates long-term memory
 */
const preferencelearningTool = tool(
  async (input: { observation: string; userId: string; category: string }) => {
    if (!store) {
      throw new Error("Store is required when compiling the graph");
    }

    /**
     * Get existing data for this category and user
     */
    const namespace =
      input.category === "preference" ? ["preferences"] : ["context"];
    const existingData = await store.get(namespace, input.userId);
    const currentItems = existingData?.value || [];

    /**
     * Add the new observation to the existing data
     */
    currentItems.push(input.observation);
    await store.put(namespace, input.userId, currentItems);

    /**
     * Check if we had existing preferences
     */
    let updateNote = "";
    if (existingData?.value && existingData.value.length > 0) {
      updateNote = `\n\nNote: This updates your existing ${
        input.category
      } information. Previous entries:\n${existingData.value
        .map((item: string) => `- ${item}`)
        .join("\n")}`;
    }

    return `Learned new ${input.category} for user ${input.userId}: "${input.observation}"${updateNote}

This information will be used to personalize future interactions.`;
  },
  {
    name: "preference_learning",
    description:
      "Learn and store user preferences or context in long-term memory",
    schema: z.object({
      observation: z
        .string()
        .describe("The preference or context to learn about the user"),
      userId: z.string().describe("The user ID this preference belongs to"),
      category: z
        .string()
        .describe("Category of information (preference, context, skill, etc.)"),
    }),
  }
);

/**
 * Create the agent with memory-aware tools
 */
const agent = createAgent({
  model,
  tools: [knowledgeRetrievalTool, preferencelearningTool],
  store, // Pass the store to the agent
  systemPrompt: `You are a personalized AI assistant with access to long-term memory about users and past interactions.

Use the knowledge_retrieval tool to:
- Find relevant information from past conversations
- Understand user preferences and context
- Provide personalized responses

Use the preference_learning tool to:
- Learn new things about users
- Update user preferences when they mention them
- Store important context for future interactions

Always try to personalize your responses based on retrieved memory when appropriate.`,
});

/**
 * Demonstrate long-term memory capabilities with configurable userId
 */
console.log("\n=== John's First Interaction ===");
const johnResult1 = await agent.invoke(
  {
    messages: [
      {
        role: "user",
        content:
          "Hi, I'm John. I need help with React components. Can you check what you know about my preferences?",
      },
    ],
  },
  { configurable: { userId: "john_123" } }
);

console.log(johnResult1.messages.at(-1)?.content);

console.log("\n=== Learning About John's New Preference ===");
const johnResult2 = await agent.invoke(
  {
    messages: [
      ...johnResult1.messages,
      {
        role: "user",
        content:
          "By the way, I prefer step-by-step tutorials over just code examples. Please remember this for next time.",
      },
    ],
  },
  { configurable: { userId: "john_123" } }
);

console.log(johnResult2.messages.at(-1)?.content);

console.log("\n=== Sarah's Different Context ===");
const sarahResult1 = await agent.invoke(
  {
    messages: [
      {
        role: "user",
        content:
          "Hello, I'm Sarah. I need information about database optimization. What do you know about my preferences?",
      },
    ],
  },
  { configurable: { userId: "sarah_456" } }
);

console.log(sarahResult1.messages.at(-1)?.content);

console.log("\n=== John Returns Later ===");
const johnResult3 = await agent.invoke(
  {
    messages: [
      {
        role: "user",
        content:
          "Hi again, it's John. Now I need help with authentication in React apps.",
      },
    ],
  },
  { configurable: { userId: "john_123" } }
);

console.log(johnResult3.messages.at(-1)?.content);

/**
 * Get the current file's path and derive the output PNG path
 */
const currentFilePath = new URL(import.meta.url).pathname;
const outputPath = currentFilePath.replace(/\.ts$/, ".png");
console.log(`\nSaving visualization to: ${outputPath}`);
await fs.writeFile(outputPath, await agent.drawMermaidPng());

/**
 * Example Output:
 * === John's First Interaction ===
 * Hi John! It looks like I don't have any stored preferences for you yet. Since you're interested
 * in React components, would you like to share any specific areas or topics within React that
 * you're focusing on? This way, I can tailor my assistance to better suit your needs.
 *
 * === Learning About John's New Preference ===
 * Got it, John! I've noted that you prefer step-by-step tutorials. Now, how can I assist you with
 * React components today?
 *
 * === Sarah's Different Context ===
 * Hello Sarah! It seems I don't have any stored preferences for you yet. Could you tell me a bit
 * about your specific interests or needs regarding database optimization? This will help me provide
 * more tailored information.
 *
 * === John Returns Later ===
 * Hi John! I remember you prefer step-by-step tutorials, so let's go through the process of implementing
 * authentication in a React app together.
 *
 * <...instructions on how to implement authentication in React apps...>
 */


--- libs/langchain-classic/README.md ---
# @langchain/classic

This package contains functionality from LangChain v0.x that has been moved out of the main `langchain` package as part of the v1.0 release. It exists to provide backward compatibility for existing applications while the core `langchain` package focuses on the essential building blocks for modern agent development.

## When to use this package

Use `@langchain/classic` if you:

- Have existing code that uses legacy chains (e.g., `LLMChain`, `ConversationalRetrievalQAChain`, `RetrievalQAChain`)
- Use the indexing API
- Depend on functionality from `@langchain/community` that was previously re-exported from `langchain`
- Are maintaining an existing application and not yet ready to migrate to the new `createAgent` API

## When NOT to use this package

**For new projects, use `langchain` v1.0 instead.** The new APIs provide:

- **`createAgent`**: A cleaner, more powerful way to build agents with middleware support
- **Better performance**: Optimized for modern agent workflows
- **Focused API surface**: Less complexity, easier to learn
- **Active development**: New features and improvements will focus on v1.0 APIs

See the [LangChain v1.0 release notes](https://docs.langchain.com/oss/javascript/releases/langchain-v1) for more information.

## Installation

```bash npm2yarn
npm install @langchain/classic
```

This package requires `@langchain/core` as a peer dependency:

```bash npm2yarn
npm install @langchain/core
```

## What's included

### Legacy Chains

All chain implementations from v0.x, including:

- `LLMChain` - Basic chain for calling an LLM with a prompt template
- `ConversationalRetrievalQAChain` - Chain for conversational question-answering over documents
- `RetrievalQAChain` - Chain for question-answering over documents without conversation memory
- `StuffDocumentsChain` - Chain for stuffing documents into a prompt
- `MapReduceDocumentsChain` - Chain for map-reduce operations over documents
- `RefineDocumentsChain` - Chain for iterative refinement over documents
- And many more...

### Indexing API

The `RecordManager` and related indexing functionality for managing document updates in vector stores.

### Community Integrations

Re-exports from `@langchain/community` that were previously available in the main `langchain` package.

### Other Deprecated Functionality

Various utilities and abstractions that have been replaced by better alternatives in v1.0.

## Migration

### From `langchain` v0.x to `@langchain/classic`

If you're upgrading to `langchain` v1.0 but want to keep using legacy functionality:

1. Install `@langchain/classic`:

   ```bash npm2yarn
   npm install @langchain/classic
   ```

2. Update your imports:

   ```typescript
   // Before (v0.x)
   import { LLMChain } from "langchain/chains";
   import { ConversationalRetrievalQAChain } from "langchain/chains";

   // After (v1.0)
   import { LLMChain } from "@langchain/classic/chains";
   import { ConversationalRetrievalQAChain } from "@langchain/classic/chains";
   ```

   Or if you imported from the root:

   ```typescript
   // Before (v0.x)
   import { LLMChain } from "langchain";

   // After (v1.0)
   import { LLMChain } from "@langchain/classic";
   ```

### From `@langchain/classic` to `langchain` v1.0

**For new development, we recommend using `createAgent` instead of legacy chains.**

Example migration from `LLMChain`:

```typescript
// Before (using LLMChain)
import { LLMChain } from "@langchain/classic/chains";
import { ChatOpenAI } from "@langchain/openai";
import { PromptTemplate } from "@langchain/core/prompts";

const model = new ChatOpenAI({ model: "gpt-4" });
const prompt = PromptTemplate.fromTemplate(
  "What is a good name for a company that makes {product}?"
);
const chain = new LLMChain({ llm: model, prompt });
const result = await chain.call({ product: "colorful socks" });

// After (using createAgent)
import { createAgent } from "langchain";

const agent = createAgent({
  model: "openai:gpt-4",
  systemPrompt: "You are a creative assistant that helps name companies.",
});

const result = await agent.invoke({
  messages: [
    {
      role: "user",
      content: "What is a good name for a company that makes colorful socks?",
    },
  ],
});
```

For more complex migrations, see the [migration guide](https://docs.langchain.com/oss/javascript/migrate/langchain-v1).

## Support and Maintenance

`@langchain/classic` will receive:

- **Bug fixes**: Critical bugs will be fixed
- **Security updates**: Security vulnerabilities will be patched
- **No new features**: New functionality will focus on `langchain` v1.0 APIs

This package is in **maintenance mode**. For new features and active development, use `langchain` v1.0.

## Examples

### Using a legacy chain

```typescript
import { LLMChain } from "@langchain/classic/chains";
import { ChatOpenAI } from "@langchain/openai";
import { PromptTemplate } from "@langchain/core/prompts";

const model = new ChatOpenAI({ model: "gpt-4" });

const prompt = PromptTemplate.fromTemplate(
  "Tell me a {adjective} joke about {content}."
);

const chain = new LLMChain({ llm: model, prompt });

const result = await chain.call({
  adjective: "funny",
  content: "chickens",
});

console.log(result.text);
```

### Using ConversationalRetrievalQAChain

```typescript
import { ConversationalRetrievalQAChain } from "@langchain/classic/chains";
import { ChatOpenAI } from "@langchain/openai";
import { OpenAIEmbeddings } from "@langchain/openai";
import { MemoryVectorStore } from "langchain/vectorstores/memory";

// Create vector store with documents
const vectorStore = await MemoryVectorStore.fromTexts(
  ["Document 1 text...", "Document 2 text..."],
  [{ id: 1 }, { id: 2 }],
  new OpenAIEmbeddings()
);

// Create chain
const model = new ChatOpenAI({ model: "gpt-4" });
const chain = ConversationalRetrievalQAChain.fromLLM(
  model,
  vectorStore.asRetriever()
);

// Use chain
const result = await chain.call({
  question: "What is in the documents?",
  chat_history: [],
});

console.log(result.text);
```

## Resources

- [LangChain v1.0 Release Notes](https://docs.langchain.com/oss/javascript/releases/langchain-v1)
- [Migration Guide](https://docs.langchain.com/oss/javascript/migrate/langchain-v1)
- [LangChain v1.0 Documentation](https://docs.langchain.com/oss/javascript/langchain/agents)
- [GitHub Repository](https://github.com/langchain-ai/langchainjs)

## Support

For bug reports and issues, please open an issue on [GitHub](https://github.com/langchain-ai/langchainjs/issues).

For questions and discussions, join our [Discord community](https://discord.gg/langchain).

## License

This package is licensed under the MIT License. See the [LICENSE](../../LICENSE) file for details.


## Links discovered
- [LangChain v1.0 release notes](https://docs.langchain.com/oss/javascript/releases/langchain-v1)
- [migration guide](https://docs.langchain.com/oss/javascript/migrate/langchain-v1)
- [LangChain v1.0 Release Notes](https://docs.langchain.com/oss/javascript/releases/langchain-v1)
- [Migration Guide](https://docs.langchain.com/oss/javascript/migrate/langchain-v1)
- [LangChain v1.0 Documentation](https://docs.langchain.com/oss/javascript/langchain/agents)
- [GitHub Repository](https://github.com/langchain-ai/langchainjs)
- [GitHub](https://github.com/langchain-ai/langchainjs/issues)
- [Discord community](https://discord.gg/langchain)
- [LICENSE](https://raw.githubusercontent.com/langchain-ai/langchainjs/main/libs/langchain-classic/../../LICENSE)

--- libs/langchain-classic/CHANGELOG.md ---
# @langchain/classic

## 1.0.4

### Patch Changes

- [#9379](https://github.com/langchain-ai/langchainjs/pull/9379) [`34c472d`](https://github.com/langchain-ai/langchainjs/commit/34c472d129c9c3d58042fad6479fd15e0763feaf) Thanks [@kenowessels](https://github.com/kenowessels)! - OpenAPIToJSONSchema required from nested schema

- Updated dependencies [[`415cb0b`](https://github.com/langchain-ai/langchainjs/commit/415cb0bfd26207583befdb02367bd12a46b33d51), [`a2ad61e`](https://github.com/langchain-ai/langchainjs/commit/a2ad61e787a06a55a615f63589a65ada05927792)]:
  - @langchain/openai@1.1.2

## 1.0.3

### Patch Changes

- Updated dependencies [[`04bd55c`](https://github.com/langchain-ai/langchainjs/commit/04bd55c63d8a0cb56f85da0b61a6bd6169b383f3), [`ac0d4fe`](https://github.com/langchain-ai/langchainjs/commit/ac0d4fe3807e05eb2185ae8a36da69498e6163d4), [`39dbe63`](https://github.com/langchain-ai/langchainjs/commit/39dbe63e3d8390bb90bb8b17f00755fa648c5651), [`dfbe45f`](https://github.com/langchain-ai/langchainjs/commit/dfbe45f3cfade7a1dbe15b2d702a8e9f8e5ac93a)]:
  - @langchain/openai@1.1.1
  - @langchain/textsplitters@1.0.0

## 1.0.2

### Patch Changes

- Updated dependencies [8319201]
- Updated dependencies [4906522]
  - @langchain/openai@1.1.0
  - @langchain/textsplitters@1.0.0

## 1.0.1

### Patch Changes

- dda9ea4: reinstate `OpenAIModerationChain`
  - @langchain/textsplitters@1.0.0
  - @langchain/openai@1.0.0

## 1.0.0

This release updates the package for compatibility with LangChain v1.0. See the v1.0 [release notes](https://docs.langchain.com/oss/javascript/releases/langchain-v1) for details on what's new.


## Links discovered
- [#9379](https://github.com/langchain-ai/langchainjs/pull/9379)
- [`34c472d`](https://github.com/langchain-ai/langchainjs/commit/34c472d129c9c3d58042fad6479fd15e0763feaf)
- [@kenowessels](https://github.com/kenowessels)
- [[`415cb0b`](https://github.com/langchain-ai/langchainjs/commit/415cb0bfd26207583befdb02367bd12a46b33d51)
- [`a2ad61e`](https://github.com/langchain-ai/langchainjs/commit/a2ad61e787a06a55a615f63589a65ada05927792)
- [[`04bd55c`](https://github.com/langchain-ai/langchainjs/commit/04bd55c63d8a0cb56f85da0b61a6bd6169b383f3)
- [`ac0d4fe`](https://github.com/langchain-ai/langchainjs/commit/ac0d4fe3807e05eb2185ae8a36da69498e6163d4)
- [`39dbe63`](https://github.com/langchain-ai/langchainjs/commit/39dbe63e3d8390bb90bb8b17f00755fa648c5651)
- [`dfbe45f`](https://github.com/langchain-ai/langchainjs/commit/dfbe45f3cfade7a1dbe15b2d702a8e9f8e5ac93a)
- [release notes](https://docs.langchain.com/oss/javascript/releases/langchain-v1)

--- libs/langchain-classic/eslint.config.ts ---
import { defineConfig } from "eslint/config";
import { langchainConfig } from "@langchain/eslint";

export default defineConfig(
  // @ts-ignore - generic typescript-eslint configs have type conflicts, FIXME
  ...langchainConfig,
  {
    files: ["examples/**/*"],
    rules: {
      "no-process-env": "off",
      "@typescript-eslint/no-explicit-any": "off",
      "@typescript-eslint/no-unused-vars": "off",
      "@typescript-eslint/no-floating-promises": "off",
      "import/no-extraneous-dependencies": "off",
      "no-instanceof/no-instanceof": "off",
      "@typescript-eslint/no-misused-promises": "off",
    },
  }
);


--- libs/langchain-classic/vitest.config.ts ---
import {
  configDefaults,
  defineConfig,
  type UserConfigExport,
} from "vitest/config";

export default defineConfig((env) => {
  const common: UserConfigExport = {
    test: {
      environment: "node",
      hideSkippedTests: true,
      globals: true,
      testTimeout: 30_000,
      maxWorkers: 0.5,
      exclude: ["**/*.int.test.ts", ...configDefaults.exclude],
      setupFiles: ["dotenv/config"],
    },
  };

  if (env.mode === "int") {
    return {
      test: {
        ...common.test,
        globals: false,
        testTimeout: 100_000,
        exclude: configDefaults.exclude,
        include: ["**/*.int.test.ts"],
        name: "int",
      },
    } satisfies UserConfigExport;
  }

  return {
    test: {
      ...common.test,
      include: configDefaults.include,
      typecheck: { enabled: true },
    },
  } satisfies UserConfigExport;
});


--- internal/model-profiles/src/api-schema.ts ---
/**
 * Schema definitions for model and provider types.
 *
 * Adapted from: https://github.com/sst/models.dev/blob/dev/packages/core/src/schema.ts
 *
 * Original source: SST models.dev
 * License: Apache-2.0 (https://github.com/sst/models.dev/blob/dev/LICENSE)
 *
 * This file contains Zod schema definitions for validating model and provider
 * configurations used in the langchain-model-profiles package.
 */

import { z } from "zod/v3";

export const Model = z
  .object({
    id: z.string(),
    name: z.string().min(1, "Model name cannot be empty"),
    attachment: z.boolean(),
    reasoning: z.boolean(),
    tool_call: z.boolean(),
    structured_output: z.boolean().optional(),
    temperature: z.boolean().optional(),
    knowledge: z
      .string()
      .regex(/^\d{4}-\d{2}(-\d{2})?$/, {
        message: "Must be in YYYY-MM or YYYY-MM-DD format",
      })
      .optional(),
    release_date: z.string().regex(/^\d{4}-\d{2}(-\d{2})?$/, {
      message: "Must be in YYYY-MM or YYYY-MM-DD format",
    }),
    last_updated: z.string().regex(/^\d{4}-\d{2}(-\d{2})?$/, {
      message: "Must be in YYYY-MM or YYYY-MM-DD format",
    }),
    modalities: z.object({
      input: z.array(z.enum(["text", "audio", "image", "video", "pdf"])),
      output: z.array(z.enum(["text", "audio", "image", "video", "pdf"])),
    }),
    open_weights: z.boolean(),
    cost: z
      .object({
        input: z.number().min(0, "Input price cannot be negative"),
        output: z.number().min(0, "Output price cannot be negative"),
        reasoning: z
          .number()
          .min(0, "Input price cannot be negative")
          .optional(),
        cache_read: z
          .number()
          .min(0, "Cache read price cannot be negative")
          .optional(),
        cache_write: z
          .number()
          .min(0, "Cache write price cannot be negative")
          .optional(),
        input_audio: z
          .number()
          .min(0, "Audio input price cannot be negative")
          .optional(),
        output_audio: z
          .number()
          .min(0, "Audio output price cannot be negative")
          .optional(),
      })
      .optional(),
    limit: z.object({
      context: z.number().min(0, "Context window must be positive"),
      output: z.number().min(0, "Output tokens must be positive"),
    }),
    status: z.enum(["alpha", "beta", "deprecated"]).optional(),
    provider: z
      .object({
        npm: z.string().optional(),
        api: z.string().optional(),
      })
      .optional(),
  })
  .strict()
  .refine(
    (data) => {
      return !(data.reasoning === false && data.cost?.reasoning !== undefined);
    },
    {
      message: "Cannot set cost.reasoning when reasoning is false",
      path: ["cost", "reasoning"],
    }
  );

export type Model = z.infer<typeof Model>;

export const Provider = z
  .object({
    id: z.string(),
    env: z.array(z.string()).min(1, "Provider env cannot be empty"),
    npm: z.string().min(1, "Provider npm module cannot be empty"),
    api: z.string().optional(),
    name: z.string().min(1, "Provider name cannot be empty"),
    doc: z
      .string()
      .min(
        1,
        "Please provide a link to the provider documentation where models are listed"
      ),
    models: z.record(Model),
  })
  .strict();

export type Provider = z.infer<typeof Provider>;

export const ProviderMap = z.record(z.string(), Provider);
export type ProviderMap = z.infer<typeof ProviderMap>;


--- libs/langchain-classic/src/document.ts ---
export { type DocumentInput, Document } from "@langchain/core/documents";


--- libs/langchain-classic/src/index.ts ---
console.warn(
  `[WARNING]: The root "langchain" entrypoint is empty. Please use a specific entrypoint instead.`
);


--- libs/langchain-classic/src/sql_db.ts ---
import type { DataSource as DataSourceT, DataSourceOptions } from "typeorm";
import { Serializable } from "@langchain/core/load/serializable";
import {
  generateTableInfoFromTables,
  getTableAndColumnsName,
  SerializedSqlDatabase,
  SqlDatabaseDataSourceParams,
  SqlDatabaseOptionsParams,
  SqlTable,
  verifyIgnoreTablesExistInDatabase,
  verifyIncludeTablesExistInDatabase,
  verifyListTablesExistInDatabase,
} from "./util/sql_utils.js";

export type { SqlDatabaseDataSourceParams, SqlDatabaseOptionsParams };

/**
 * Class that represents a SQL database in the LangChain framework.
 *
 * @security **Security Notice**
 * This class generates SQL queries for the given database.
 * The SQLDatabase class provides a getTableInfo method that can be used
 * to get column information as well as sample data from the table.
 * To mitigate risk of leaking sensitive data, limit permissions
 * to read and scope to the tables that are needed.
 * Optionally, use the includesTables or ignoreTables class parameters
 * to limit which tables can/cannot be accessed.
 *
 * @link See https://js.langchain.com/docs/security for more information.
 */
export class SqlDatabase
  extends Serializable
  implements SqlDatabaseOptionsParams, SqlDatabaseDataSourceParams
{
  lc_namespace = ["langchain", "sql_db"];

  toJSON() {
    return this.toJSONNotImplemented();
  }

  appDataSourceOptions: DataSourceOptions;

  appDataSource: DataSourceT;

  allTables: Array<SqlTable> = [];

  includesTables: Array<string> = [];

  ignoreTables: Array<string> = [];

  sampleRowsInTableInfo = 3;

  customDescription?: Record<string, string>;

  protected constructor(fields: SqlDatabaseDataSourceParams) {
    super(...arguments);
    this.appDataSource = fields.appDataSource;
    this.appDataSourceOptions = fields.appDataSource.options;
    if (fields?.includesTables && fields?.ignoreTables) {
      throw new Error("Cannot specify both includeTables and ignoreTables");
    }
    this.includesTables = fields?.includesTables ?? [];
    this.ignoreTables = fields?.ignoreTables ?? [];
    this.sampleRowsInTableInfo =
      fields?.sampleRowsInTableInfo ?? this.sampleRowsInTableInfo;
  }

  static async fromDataSourceParams(
    fields: SqlDatabaseDataSourceParams
  ): Promise<SqlDatabase> {
    const sqlDatabase = new SqlDatabase(fields);
    if (!sqlDatabase.appDataSource.isInitialized) {
      await sqlDatabase.appDataSource.initialize();
    }
    sqlDatabase.allTables = await getTableAndColumnsName(
      sqlDatabase.appDataSource
    );
    sqlDatabase.customDescription = Object.fromEntries(
      Object.entries(fields?.customDescription ?? {}).filter(([key, _]) =>
        sqlDatabase.allTables
          .map((table: SqlTable) => table.tableName)
          .includes(key)
      )
    );
    verifyIncludeTablesExistInDatabase(
      sqlDatabase.allTables,
      sqlDatabase.includesTables
    );
    verifyIgnoreTablesExistInDatabase(
      sqlDatabase.allTables,
      sqlDatabase.ignoreTables
    );
    return sqlDatabase;
  }

  static async fromOptionsParams(
    fields: SqlDatabaseOptionsParams
  ): Promise<SqlDatabase> {
    const { DataSource } = await import("typeorm");
    const dataSource = new DataSource(fields.appDataSourceOptions);
    return SqlDatabase.fromDataSourceParams({
      ...fields,
      appDataSource: dataSource,
    });
  }

  /**
   * Get information about specified tables.
   *
   * Follows best practices as specified in: Rajkumar et al, 2022
   * (https://arxiv.org/abs/2204.00498)
   *
   * If `sample_rows_in_table_info`, the specified number of sample rows will be
   * appended to each table description. This can increase performance as
   * demonstrated in the paper.
   */
  async getTableInfo(targetTables?: Array<string>): Promise<string> {
    let selectedTables =
      this.includesTables.length > 0
        ? this.allTables.filter((currentTable) =>
            this.includesTables.includes(currentTable.tableName)
          )
        : this.allTables;

    if (this.ignoreTables.length > 0) {
      selectedTables = selectedTables.filter(
        (currentTable) => !this.ignoreTables.includes(currentTable.tableName)
      );
    }

    if (targetTables && targetTables.length > 0) {
      verifyListTablesExistInDatabase(
        this.allTables,
        targetTables,
        "Wrong target table name:"
      );
      selectedTables = this.allTables.filter((currentTable) =>
        targetTables.includes(currentTable.tableName)
      );
    }

    return generateTableInfoFromTables(
      selectedTables,
      this.appDataSource,
      this.sampleRowsInTableInfo,
      this.customDescription
    );
  }

  /**
   * Execute a SQL command and return a string representing the results.
   * If the statement returns rows, a string of the results is returned.
   * If the statement returns no rows, an empty string is returned.
   */
  async run(command: string, fetch: "all" | "one" = "all"): Promise<string> {
    // TODO: Potential security issue here
    const res = await this.appDataSource.query(command);

    if (fetch === "all") {
      return JSON.stringify(res);
    }

    if (res?.length > 0) {
      return JSON.stringify(res[0]);
    }

    return "";
  }

  serialize(): SerializedSqlDatabase {
    return {
      _type: "sql_database",
      appDataSourceOptions: this.appDataSourceOptions,
      includesTables: this.includesTables,
      ignoreTables: this.ignoreTables,
      sampleRowsInTableInfo: this.sampleRowsInTableInfo,
    };
  }

  /** @ignore */
  static async imports() {
    try {
      const { DataSource } = await import("typeorm");
      return { DataSource };
    } catch (e) {
      console.error(e);
      throw new Error(
        "Failed to load typeorm. Please install it with eg. `yarn add typeorm`."
      );
    }
  }
}


--- libs/langchain-classic/src/text_splitter.ts ---
export * from "@langchain/textsplitters";


--- libs/langchain-classic/src/agents/agent.ts ---
import type {
  StructuredToolInterface,
  ToolInterface,
} from "@langchain/core/tools";
import type { BaseLanguageModelInterface } from "@langchain/core/language_models/base";
import { CallbackManager, Callbacks } from "@langchain/core/callbacks/manager";
import { BasePromptTemplate } from "@langchain/core/prompts";
import { AgentAction, AgentFinish, AgentStep } from "@langchain/core/agents";
import { BaseMessage } from "@langchain/core/messages";
import { ChainValues } from "@langchain/core/utils/types";
import { Serializable } from "@langchain/core/load/serializable";
import {
  Runnable,
  patchConfig,
  type RunnableConfig,
  RunnableSequence,
  RunnableLike,
} from "@langchain/core/runnables";
import { LLMChain } from "../chains/llm_chain.js";
import type {
  AgentActionOutputParser,
  AgentInput,
  RunnableMultiActionAgentInput,
  RunnableSingleActionAgentInput,
  SerializedAgent,
  StoppingMethod,
} from "./types.js";

/**
 * Record type for arguments passed to output parsers.
 */
// eslint-disable-next-line @typescript-eslint/no-explicit-any
export type OutputParserArgs = Record<string, any>;

/**
 * Error class for parse errors in LangChain. Contains information about
 * the error message and the output that caused the error.
 */
class ParseError extends Error {
  output: string;

  constructor(msg: string, output: string) {
    super(msg);
    this.output = output;
  }
}

/**
 * Abstract base class for agents in LangChain. Provides common
 * functionality for agents, such as handling inputs and outputs.
 */
export abstract class BaseAgent extends Serializable {
  declare ToolType: StructuredToolInterface;

  abstract get inputKeys(): string[];

  get returnValues(): string[] {
    return ["output"];
  }

  get allowedTools(): string[] | undefined {
    return undefined;
  }

  /**
   * Return the string type key uniquely identifying this class of agent.
   */
  _agentType(): string {
    throw new Error("Not implemented");
  }

  /**
   * Return the string type key uniquely identifying multi or single action agents.
   */
  abstract _agentActionType(): string;

  /**
   * Return response when agent has been stopped due to max iterations
   */
  returnStoppedResponse(
    earlyStoppingMethod: StoppingMethod,
    _steps: AgentStep[],
    _inputs: ChainValues,
    _callbackManager?: CallbackManager
  ): Promise<AgentFinish> {
    if (earlyStoppingMethod === "force") {
      return Promise.resolve({
        returnValues: { output: "Agent stopped due to max iterations." },
        log: "",
      });
    }

    throw new Error(`Invalid stopping method: ${earlyStoppingMethod}`);
  }

  /**
   * Prepare the agent for output, if needed
   */
  async prepareForOutput(
    _returnValues: AgentFinish["returnValues"],
    _steps: AgentStep[]
  ): Promise<AgentFinish["returnValues"]> {
    return {};
  }
}

/**
 * Abstract base class for single action agents in LangChain. Extends the
 * BaseAgent class and provides additional functionality specific to
 * single action agents.
 */
export abstract class BaseSingleActionAgent extends BaseAgent {
  _agentActionType(): string {
    return "single" as const;
  }

  /**
   * Decide what to do, given some input.
   *
   * @param steps - Steps the LLM has taken so far, along with observations from each.
   * @param inputs - User inputs.
   * @param callbackManager - Callback manager.
   *
   * @returns Action specifying what tool to use.
   */
  abstract plan(
    steps: AgentStep[],
    inputs: ChainValues,
    callbackManager?: CallbackManager,
    config?: RunnableConfig
  ): Promise<AgentAction | AgentFinish>;
}

/**
 * Abstract base class for multi-action agents in LangChain. Extends the
 * BaseAgent class and provides additional functionality specific to
 * multi-action agents.
 */
export abstract class BaseMultiActionAgent extends BaseAgent {
  _agentActionType(): string {
    return "multi" as const;
  }

  /**
   * Decide what to do, given some input.
   *
   * @param steps - Steps the LLM has taken so far, along with observations from each.
   * @param inputs - User inputs.
   * @param callbackManager - Callback manager.
   *
   * @returns Actions specifying what tools to use.
   */
  abstract plan(
    steps: AgentStep[],
    inputs: ChainValues,
    callbackManager?: CallbackManager,
    config?: RunnableConfig
  ): Promise<AgentAction[] | AgentFinish>;
}

function isAgentAction(input: unknown): input is AgentAction {
  return !Array.isArray(input) && (input as AgentAction)?.tool !== undefined;
}

export function isRunnableAgent(x: BaseAgent) {
  return (
    (x as RunnableMultiActionAgent | RunnableSingleActionAgent).runnable !==
    undefined
  );
}

// TODO: Remove in the future. Only for backwards compatibility.
// Allows for the creation of runnables with properties that will
// be passed to the agent executor constructor.
export class AgentRunnableSequence<
  // eslint-disable-next-line @typescript-eslint/no-explicit-any
  RunInput = any,
  // eslint-disable-next-line @typescript-eslint/no-explicit-any
  RunOutput = any
> extends RunnableSequence<RunInput, RunOutput> {
  streamRunnable?: boolean;

  singleAction: boolean;

  // eslint-disable-next-line @typescript-eslint/no-explicit-any
  static fromRunnables<RunInput = any, RunOutput = any>(
    [first, ...runnables]: [
      RunnableLike<RunInput>,
      ...RunnableLike[],
      // eslint-disable-next-line @typescript-eslint/no-explicit-any
      RunnableLike<any, RunOutput>
    ],
    config: { singleAction: boolean; streamRunnable?: boolean; name?: string }
  ): AgentRunnableSequence<RunInput, Exclude<RunOutput, Error>> {
    const sequence = RunnableSequence.from(
      [first, ...runnables],
      config.name
    ) as AgentRunnableSequence<RunInput, Exclude<RunOutput, Error>>;
    sequence.singleAction = config.singleAction;
    sequence.streamRunnable = config.streamRunnable;
    return sequence;
  }

  static isAgentRunnableSequence(x: Runnable): x is AgentRunnableSequence {
    return typeof (x as AgentRunnableSequence).singleAction === "boolean";
  }
}

/**
 * Class representing a single-action agent powered by runnables.
 * Extends the BaseSingleActionAgent class and provides methods for
 * planning agent actions with runnables.
 */
export class RunnableSingleActionAgent extends BaseSingleActionAgent {
  lc_namespace = ["langchain", "agents", "runnable"];

  runnable: Runnable<
    ChainValues & { steps: AgentStep[] },
    AgentAction | AgentFinish
  >;

  get inputKeys(): string[] {
    return [];
  }

  /**
   * Whether to stream from the runnable or not.
   * If true, the underlying LLM is invoked in a streaming fashion to make it
   * possible to get access to the individual LLM tokens when using
   * `streamLog` with the Agent Executor. If false then LLM is invoked in a
   * non-streaming fashion and individual LLM tokens will not be available
   * in `streamLog`.
   *
   * Note that the runnable should still only stream a single action or
   * finish chunk.
   */
  streamRunnable = true;

  defaultRunName = "RunnableAgent";

  constructor(fields: RunnableSingleActionAgentInput) {
    super(fields);
    this.runnable = fields.runnable;
    this.defaultRunName =
      fields.defaultRunName ?? this.runnable.name ?? this.defaultRunName;
    this.streamRunnable = fields.streamRunnable ?? this.streamRunnable;
  }

  async plan(
    steps: AgentStep[],
    inputs: ChainValues,
    callbackManager?: CallbackManager,
    config?: RunnableConfig
  ): Promise<AgentAction | AgentFinish> {
    const combinedInput = { ...inputs, steps };
    const combinedConfig = patchConfig(config, {
      callbacks: callbackManager,
      runName: this.defaultRunName,
    });
    if (this.streamRunnable) {
      const stream = await this.runnable.stream(combinedInput, combinedConfig);
      let finalOutput: AgentAction | AgentFinish | undefined;
      for await (const chunk of stream) {
        if (finalOutput === undefined) {
          finalOutput = chunk;
        } else {
          throw new Error(
            [
              `Multiple agent actions/finishes received in streamed agent output.`,
              `Set "streamRunnable: false" when initializing the agent to invoke this agent in non-streaming mode.`,
            ].join("\n")
          );
        }
      }
      if (finalOutput === undefined) {
        throw new Error(
          [
            "No streaming output received from underlying runnable.",
            `Set "streamRunnable: false" when initializing the agent to invoke this agent in non-streaming mode.`,
          ].join("\n")
        );
      }
      return finalOutput;
    } else {
      return this.runnable.invoke(combinedInput, combinedConfig);
    }
  }
}

/**
 * Class representing a multi-action agent powered by runnables.
 * Extends the BaseMultiActionAgent class and provides methods for
 * planning agent actions with runnables.
 */
export class RunnableMultiActionAgent extends BaseMultiActionAgent {
  lc_namespace = ["langchain", "agents", "runnable"];

  // TODO: Rename input to "intermediate_steps"
  runnable: Runnable<
    ChainValues & { steps: AgentStep[] },
    AgentAction[] | AgentAction | AgentFinish
  >;

  defaultRunName = "RunnableAgent";

  stop?: string[];

  streamRunnable = true;

  get inputKeys(): string[] {
    return [];
  }

  constructor(fields: RunnableMultiActionAgentInput) {
    super(fields);
    this.runnable = fields.runnable;
    this.stop = fields.stop;
    this.defaultRunName =
      fields.defaultRunName ?? this.runnable.name ?? this.defaultRunName;
    this.streamRunnable = fields.streamRunnable ?? this.streamRunnable;
  }

  async plan(
    steps: AgentStep[],
    inputs: ChainValues,
    callbackManager?: CallbackManager,
    config?: RunnableConfig
  ): Promise<AgentAction[] | AgentFinish> {
    const combinedInput = { ...inputs, steps };
    const combinedConfig = patchConfig(config, {
      callbacks: callbackManager,
      runName: this.defaultRunName,
    });
    let output;
    if (this.streamRunnable) {
      const stream = await this.runnable.stream(combinedInput, combinedConfig);
      let finalOutput: AgentAction | AgentFinish | AgentAction[] | undefined;
      for await (const chunk of stream) {
        if (finalOutput === undefined) {
          finalOutput = chunk;
        } else {
          throw new Error(
            [
              `Multiple agent actions/finishes received in streamed agent output.`,
              `Set "streamRunnable: false" when initializing the agent to invoke this agent in non-streaming mode.`,
            ].join("\n")
          );
        }
      }
      if (finalOutput === undefined) {
        throw new Error(
          [
            "No streaming output received from underlying runnable.",
            `Set "streamRunnable: false" when initializing the agent to invoke this agent in non-streaming mode.`,
          ].join("\n")
        );
      }
      output = finalOutput;
    } else {
      output = await this.runnable.invoke(combinedInput, combinedConfig);
    }

    if (isAgentAction(output)) {
      return [output];
    }

    return output;
  }
}

export class RunnableAgent extends RunnableMultiActionAgent {}

/**
 * Interface for input data for creating a LLMSingleActionAgent.
 */
export interface LLMSingleActionAgentInput {
  llmChain: LLMChain;
  outputParser: AgentActionOutputParser;
  stop?: string[];
}

/**
 * Class representing a single action agent using a LLMChain in LangChain.
 * Extends the BaseSingleActionAgent class and provides methods for
 * planning agent actions based on LLMChain outputs.
 * @example
 * ```typescript
 * const customPromptTemplate = new CustomPromptTemplate({
 *   tools: [new Calculator()],
 *   inputVariables: ["input", "agent_scratchpad"],
 * });
 * const customOutputParser = new CustomOutputParser();
 * const agent = new LLMSingleActionAgent({
 *   llmChain: new LLMChain({
 *     prompt: customPromptTemplate,
 *     llm: new ChatOpenAI({ model: "gpt-4o-mini", temperature: 0 }),
 *   }),
 *   outputParser: customOutputParser,
 *   stop: ["\nObservation"],
 * });
 * const executor = new AgentExecutor({
 *   agent,
 *   tools: [new Calculator()],
 * });
 * const result = await executor.invoke({
 *   input:
 *     "Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?",
 * });
 * ```
 */
export class LLMSingleActionAgent extends BaseSingleActionAgent {
  lc_namespace = ["langchain", "agents"];

  llmChain: LLMChain;

  outputParser: AgentActionOutputParser;

  stop?: string[];

  constructor(input: LLMSingleActionAgentInput) {
    super(input);
    this.stop = input.stop;
    this.llmChain = input.llmChain;
    this.outputParser = input.outputParser;
  }

  get inputKeys(): string[] {
    return this.llmChain.inputKeys;
  }

  /**
   * Decide what to do given some input.
   *
   * @param steps - Steps the LLM has taken so far, along with observations from each.
   * @param inputs - User inputs.
   * @param callbackManager - Callback manager.
   *
   * @returns Action specifying what tool to use.
   */
  async plan(
    steps: AgentStep[],
    inputs: ChainValues,
    callbackManager?: CallbackManager
  ): Promise<AgentAction | AgentFinish> {
    const output = await this.llmChain.call(
      {
        intermediate_steps: steps,
        stop: this.stop,
        ...inputs,
      },
      callbackManager
    );
    return this.outputParser.parse(
      output[this.llmChain.outputKey],
      callbackManager
    );
  }
}

/**
 * Interface for arguments used to create an agent in LangChain.
 */
export interface AgentArgs {
  outputParser?: AgentActionOutputParser;

  callbacks?: Callbacks;

  /**
   * @deprecated Use `callbacks` instead.
   */
  callbackManager?: CallbackManager;
}

/**
 * Class responsible for calling a language model and deciding an action.
 *
 * @remarks This is driven by an LLMChain. The prompt in the LLMChain *must*
 * include a variable called "agent_scratchpad" where the agent can put its
 * intermediary work.
 */
export abstract class Agent extends BaseSingleActionAgent {
  llmChain: LLMChain;

  outputParser: AgentActionOutputParser | undefined;

  private _allowedTools?: string[] = undefined;

  get allowedTools(): string[] | undefined {
    return this._allowedTools;
  }

  get inputKeys(): string[] {
    return this.llmChain.inputKeys.filter((k) => k !== "agent_scratchpad");
  }

  constructor(input: AgentInput) {
    super(input);

    this.llmChain = input.llmChain;
    this._allowedTools = input.allowedTools;
    this.outputParser = input.outputParser;
  }

  /**
   * Prefix to append the observation with.
   */
  abstract observationPrefix(): string;

  /**
   * Prefix to append the LLM call with.
   */
  abstract llmPrefix(): string;

  /**
   * Return the string type key uniquely identifying this class of agent.
   */
  abstract _agentType(): string;

  /**
   * Get the default output parser for this agent.
   */
  static getDefaultOutputParser(
    _fields?: OutputParserArgs
  ): AgentActionOutputParser {
    throw new Error("Not implemented");
  }

  /**
   * Create a prompt for this class
   *
   * @param _tools - List of tools the agent will have access to, used to format the prompt.
   * @param _fields - Additional fields used to format the prompt.
   *
   * @returns A PromptTemplate assembled from the given tools and fields.
   * */
  static createPrompt(
    _tools: StructuredToolInterface[],
    // eslint-disable-next-line @typescript-eslint/no-explicit-any
    _fields?: Record<string, any>
  ): BasePromptTemplate {
    throw new Error("Not implemented");
  }

  /** Construct an agent from an LLM and a list of tools */
  static fromLLMAndTools(
    _llm: BaseLanguageModelInterface,
    _tools: StructuredToolInterface[],

    _args?: AgentArgs
  ): Agent {
    throw new Error("Not implemented");
  }

  /**
   * Validate that appropriate tools are passed in
   */
  static validateTools(_tools: StructuredToolInterface[]): void {}

  _stop(): string[] {
    return [`\n${this.observationPrefix()}`];
  }

  /**
   * Name of tool to use to terminate the chain.
   */
  finishToolName(): string {
    return "Final Answer";
  }

  /**
   * Construct a scratchpad to let the agent continue its thought process
   */
  async constructScratchPad(
    steps: AgentStep[]
  ): Promise<string | BaseMessage[]> {
    return steps.reduce(
      (thoughts, { action, observation }) =>
        thoughts +
        [
          action.log,
          `${this.observationPrefix()}${observation}`,
          this.llmPrefix(),
        ].join("\n"),
      ""
    );
  }

  private async _plan(
    steps: AgentStep[],
    inputs: ChainValues,
    suffix?: string,
    callbackManager?: CallbackManager
  ): Promise<AgentAction | AgentFinish> {
    const thoughts = await this.constructScratchPad(steps);
    const newInputs: ChainValues = {
      ...inputs,
      agent_scratchpad: suffix ? `${thoughts}${suffix}` : thoughts,
    };

    if (this._stop().length !== 0) {
      newInputs.stop = this._stop();
    }

    const output = await this.llmChain.predict(newInputs, callbackManager);
    if (!this.outputParser) {
      throw new Error("Output parser not set");
    }
    return this.outputParser.parse(output, callbackManager);
  }

  /**
   * Decide what to do given some input.
   *
   * @param steps - Steps the LLM has taken so far, along with observations from each.
   * @param inputs - User inputs.
   * @param callbackManager - Callback manager to use for this call.
   *
   * @returns Action specifying what tool to use.
   */
  plan(
    steps: AgentStep[],
    inputs: ChainValues,
    callbackManager?: CallbackManager
  ): Promise<AgentAction | AgentFinish> {
    return this._plan(steps, inputs, undefined, callbackManager);
  }

  /**
   * Return response when agent has been stopped due to max iterations
   */
  async returnStoppedResponse(
    earlyStoppingMethod: StoppingMethod,
    steps: AgentStep[],
    inputs: ChainValues,
    callbackManager?: CallbackManager
  ): Promise<AgentFinish> {
    if (earlyStoppingMethod === "force") {
      return {
        returnValues: { output: "Agent stopped due to max iterations." },
        log: "",
      };
    }

    if (earlyStoppingMethod === "generate") {
      try {
        const action = await this._plan(
          steps,
          inputs,
          "\n\nI now need to return a final answer based on the previous steps:",
          callbackManager
        );
        if ("returnValues" in action) {
          return action;
        }

        return { returnValues: { output: action.log }, log: action.log };
      } catch (err) {
        // fine to use instanceof because we're in the same module
        // eslint-disable-next-line no-instanceof/no-instanceof
        if (!(err instanceof ParseError)) {
          throw err;
        }
        return { returnValues: { output: err.output }, log: err.output };
      }
    }

    throw new Error(`Invalid stopping method: ${earlyStoppingMethod}`);
  }

  /**
   * Load an agent from a json-like object describing it.
   */
  static async deserialize(
    data: SerializedAgent & {
      llm?: BaseLanguageModelInterface;
      tools?: ToolInterface[];
    }
  ): Promise<Agent> {
    switch (data._type) {
      case "zero-shot-react-description": {
        const { ZeroShotAgent } = await import("./mrkl/index.js");
        return ZeroShotAgent.deserialize(data);
      }
      default:
        throw new Error("Unknown agent type");
    }
  }
}


--- CONTRIBUTING.md ---
# Contributing to LangChain

👋 Hi there! Thank you for being interested in contributing to LangChain.
As an open source project in a rapidly developing field, we are extremely open
to contributions, whether it be in the form of a new feature, improved infra, or better documentation.

To contribute to this project, please follow a ["fork and pull request"](https://docs.github.com/en/get-started/quickstart/contributing-to-projects) workflow. Please do not try to push directly to this repo unless you are a maintainer.

## Quick Links

### Not sure what to work on?

If you are not sure what to work on, we have a few suggestions:

- Look at the issues with the [help wanted](https://github.com/langchain-ai/langchainjs/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22) label. These are issues that we think are good targets for contributors. If you are interested in working on one of these, please comment on the issue so that we can assign it to you. And if you have any questions let us know, we're happy to guide you!
- At the moment our main focus is reaching parity with the Python version for features and base functionality. If you are interested in working on a specific integration or feature, please let us know and we can help you get started.

### New abstractions

We aim to keep the same core APIs between the Python and JS versions of LangChain, where possible. As such we ask that if you have an idea for a new abstraction, please open an issue first to discuss it. This will help us make sure that the API is consistent across both versions. If you're not sure what to work on, we recommend looking at the links above first.

### Want to add a specific integration?

LangChain supports several different types of integrations with third-party providers and frameworks, including LLM providers (e.g. [OpenAI](https://github.com/langchain-ai/langchainjs/blob/main/libs/providers/langchain-openai/src/chat_models.ts)), vector stores (e.g. [FAISS](https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-community/src/vectorstores/faiss.ts), document loaders (e.g. [Apify](https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-community/src/document_loaders/web/apify_dataset.ts)) persistent message history stores (e.g. [Redis](https://github.com/langchain-ai/langchainjs/blob/main/libs/providers/langchain-redis/src/caches.ts)), and more.

We welcome such contributions, but ask that you read our dedicated [integration contribution guide](https://github.com/langchain-ai/langchainjs/blob/main/.github/contributing/INTEGRATIONS.md) for specific details and patterns to consider before opening a pull request.

You can also check out the [guides on extending LangChain.js](https://js.langchain.com/docs/how_to/#custom) in our docs.

#### Integration packages

Integrations should generally reside in the `libs/langchain-community` workspace and be imported as `@langchain/community/module/name`. More in-depth integrations or suites of integrations may also reside in separate packages that depend on and extend `@langchain/core`. See [`@langchain/google-genai`](https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-google-genai) for an example.

### Want to add a feature that's already in Python?

If you're interested in contributing a feature that's already in the [LangChain Python repo](https://github.com/langchain-ai/langchain) and you'd like some help getting started, you can try pasting code snippets and classes into the [LangChain Python to JS translator](https://langchain-translator.vercel.app/).

It's a chat interface wrapping a fine-tuned `gpt-3.5-turbo` instance trained on prior ported features. This allows the model to innately take into account LangChain-specific code style and imports.

It's an ongoing project, and feedback on runs will be used to improve the [LangSmith dataset](https://smith.langchain.com) for further fine-tuning! Try it out below:

<https://langchain-translator.vercel.app/>

## 🗺️ Contributing Guidelines

### 🚩 GitHub Issues

Our [issues](https://github.com/langchain-ai/langchainjs/issues) primarily contains bug reports and docs improvements. For feature requests, please defer to the [LangChain Forum](https://forum.langchain.com/).

If you start working on an issue, please assign it to yourself.

If you are adding an issue, please try to keep it focused on a single modular bug/improvement/feature.
If the two issues are related, or blocking, please link them rather than keep them as one single one.

We will try to keep these issues as up to date as possible, though
with the rapid rate of development in this field some may get out of date.
If you notice this happening, please just let us know.

### 🙋 Getting Help

Although we try to have a developer setup to make it as easy as possible for others to contribute (see below)
it is possible that some pain point may arise around environment setup, linting, documentation, or other.
Should that occur, please contact a maintainer! Not only do we want to help get you unblocked,
but we also want to make sure that the process is smooth for future contributors.

In a similar vein, we do enforce certain linting, formatting, and documentation standards in the codebase.
If you are finding these difficult (or even just annoying) to work with,
feel free to contact a maintainer for help - we do not want these to get in the way of getting
good code into the codebase.

### 🏭 Release process

As of now, LangChain has an ad hoc release process: releases are cut with high frequency by
a developer and published to [npm](https://www.npmjs.com/package/langchain).

If your contribution has made its way into a release, we will want to give you credit on Twitter (only if you want though)!
If you have a Twitter account you would like us to mention, please let us know in the PR or in another manner.

### 🛠️ Tooling

This project uses the following tools, which are worth getting familiar
with if you plan to contribute:

- **[pnpm](https://pnpm.io/) (v10.14.0)** - dependency management
- **[eslint](https://eslint.org/)** - enforcing standard lint rules
- **[prettier](https://prettier.io/)** - enforcing standard code formatting
- **[vitest](https://vitest.dev/)** - testing code

## 🚀 Quick Start

Clone this repo, then cd into it:

```bash
cd langchainjs
```

Next, try running the following common tasks:

## ✅ Common Tasks

Our goal is to make it as easy as possible for you to contribute to this project.
All of the below commands should be run from within a workspace directory (e.g. `libs/langchain`, `libs/langchain-community`) unless otherwise noted.

```bash
cd libs/langchain
```

Or, if you are working on a community integration:

```bash
cd libs/langchain-community
```

### Setup

**Prerequisite**: Node version v24.x is required. Please check node version `node -v` and update it if required.

To get started, you will need to install the dependencies for the project. To do so, run:

```bash
pnpm install
```

Then, you will need to switch directories into `libs/langchain-core` and build core by running:

```bash
cd libs/langchain-core
pnpm install
pnpm build
```

### Linting

We use [eslint](https://eslint.org/) to enforce standard lint rules.
To run the linter, run:

```bash
pnpm lint
```

### Formatting

We use [prettier](https://prettier.io) to enforce code formatting style.
To run the formatter, run:

```bash
pnpm format
```

To just check for formatting differences, without fixing them, run:

```bash
pnpm format:check
```

### Testing

In general, tests should be added within a `tests/` folder alongside the modules they
are testing.

**Unit tests** cover modular logic that does not require calls to outside APIs.

If you add new logic, please add a unit test.
Unit tests should be called `*.test.ts`.

To run only unit tests, run:

```bash
pnpm test
```

#### Running a single test

To run a single test, run the following from within a workspace:

```bash
pnpm test:single /path/to/yourtest.test.ts
```

This is useful for developing individual features.

**Integration tests** cover logic that requires making calls to outside APIs (often integration with other services).

If you add support for a new external API, please add a new integration test.
Integration tests should be called `*.int.test.ts`.

Note that most integration tests require credentials or other setup. You will likely need to set up a `libs/langchain/.env` or `libs/langchain-community/.env` file
like the example [here](https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain/.env.example).

We generally recommend only running integration tests with `pnpm test:single`, but if you want to run all integration tests, run:

```bash
pnpm test:integration
```

### Building

To build the project, run:

```bash
pnpm build
```

### Adding an Entrypoint

LangChain exposes multiple subpaths the user can import from, e.g.

```typescript
import { OpenAI } from "langchain/llms/openai";
```

We call these subpaths "entrypoints". In general, you should create a new entrypoint if you are adding a new integration with a 3rd party library. If you're adding self-contained functionality without any external dependencies, you can add it to an existing entrypoint.

In order to declare a new entrypoint that users can import from, you
should edit the `libs/langchain/langchain.config.js` or `libs/langchain-community/langchain.config.js` file. To add an
entrypoint `tools` that imports from `tools/index.ts` you'd add
the following to the `entrypoints` key inside the `config` variable:

```typescript
// ...
entrypoints: {
  // ...
  tools: "tools/index",
},
// ...
```

If you're adding a new integration which requires installing a third party dependency, you must add the entrypoint to the `requiresOptionalDependency` array, also located inside `libs/langchain/langchain.config.js` or `libs/langchain-community/langchain.config.js`.

```typescript
// ...
requiresOptionalDependency: [
  // ...
  "tools/index",
],
// ...
```

This will make sure the entrypoint is included in the published package,
and in generated documentation.

## Advanced

**Environment tests** test whether LangChain works across different JS environments, including Node.js (both ESM and CJS), Edge environments (eg. Cloudflare Workers), and browsers (using Webpack).

To run the environment tests with Docker, run the following command from the project root:

```bash
pnpm test:exports:docker
```


## Links discovered
- ["fork and pull request"](https://docs.github.com/en/get-started/quickstart/contributing-to-projects)
- [help wanted](https://github.com/langchain-ai/langchainjs/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22)
- [OpenAI](https://github.com/langchain-ai/langchainjs/blob/main/libs/providers/langchain-openai/src/chat_models.ts)
- [FAISS](https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-community/src/vectorstores/faiss.ts)
- [Apify](https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-community/src/document_loaders/web/apify_dataset.ts)
- [Redis](https://github.com/langchain-ai/langchainjs/blob/main/libs/providers/langchain-redis/src/caches.ts)
- [integration contribution guide](https://github.com/langchain-ai/langchainjs/blob/main/.github/contributing/INTEGRATIONS.md)
- [guides on extending LangChain.js](https://js.langchain.com/docs/how_to/#custom)
- [`@langchain/google-genai`](https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-google-genai)
- [LangChain Python repo](https://github.com/langchain-ai/langchain)
- [LangChain Python to JS translator](https://langchain-translator.vercel.app/)
- [LangSmith dataset](https://smith.langchain.com)
- [issues](https://github.com/langchain-ai/langchainjs/issues)
- [LangChain Forum](https://forum.langchain.com/)
- [npm](https://www.npmjs.com/package/langchain)
- [pnpm](https://pnpm.io/)
- [eslint](https://eslint.org/)
- [prettier](https://prettier.io/)
- [vitest](https://vitest.dev/)
- [prettier](https://prettier.io)
- [here](https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain/.env.example)

--- SECURITY.md ---
# Security Policy

## Reporting a Vulnerability

Please report security vulnerabilities by email to `security@langchain.dev`.
This email is an alias to a subset of our maintainers, and will ensure the issue is promptly triaged and acted upon as needed.


--- libs/langchain-community/CHANGELOG.md ---
# @langchain/community

## 1.0.4

### Patch Changes

- [#9326](https://github.com/langchain-ai/langchainjs/pull/9326) [`3e0cab6`](https://github.com/langchain-ai/langchainjs/commit/3e0cab61b32fae271936770b822cb9644f68b637) Thanks [@ayanyev](https://github.com/ayanyev)! - Milvus vector store client: ignore auto-calculated fields in collection schema during payload validation

- Updated dependencies [[`415cb0b`](https://github.com/langchain-ai/langchainjs/commit/415cb0bfd26207583befdb02367bd12a46b33d51), [`a2ad61e`](https://github.com/langchain-ai/langchainjs/commit/a2ad61e787a06a55a615f63589a65ada05927792), [`34c472d`](https://github.com/langchain-ai/langchainjs/commit/34c472d129c9c3d58042fad6479fd15e0763feaf)]:
  - @langchain/openai@1.1.2
  - @langchain/classic@1.0.4

## 1.0.3

### Patch Changes

- Updated dependencies [[`04bd55c`](https://github.com/langchain-ai/langchainjs/commit/04bd55c63d8a0cb56f85da0b61a6bd6169b383f3), [`ac0d4fe`](https://github.com/langchain-ai/langchainjs/commit/ac0d4fe3807e05eb2185ae8a36da69498e6163d4), [`39dbe63`](https://github.com/langchain-ai/langchainjs/commit/39dbe63e3d8390bb90bb8b17f00755fa648c5651), [`dfbe45f`](https://github.com/langchain-ai/langchainjs/commit/dfbe45f3cfade7a1dbe15b2d702a8e9f8e5ac93a)]:
  - @langchain/openai@1.1.1
  - @langchain/classic@1.0.3

## 1.0.2

### Patch Changes

- b17762a: fix(community): export type properly in duckduckgo_search
- e4a3b3b: improve(pdf-loader): update error message to specify support for pdf-parse v1 only
- ecc7a8a: swap problematic eval package
- Updated dependencies [8319201]
- Updated dependencies [4906522]
  - @langchain/openai@1.1.0
  - @langchain/classic@1.0.2

## 1.0.1

### Patch Changes

- Updated dependencies [dda9ea4]
  - @langchain/classic@1.0.1
  - @langchain/openai@1.0.0

## 1.0.0

This release updates the package for compatibility with LangChain v1.0. See the v1.0 [release notes](https://docs.langchain.com/oss/javascript/releases/langchain-v1) for details on what's new.

## 0.3.57

### Patch Changes

- fd4691f: use `keyEncoder` instead of insecure cache key getter
- Updated dependencies [fd4691f]
- Updated dependencies [2f19cd5]
- Updated dependencies [d38e9d6]
- Updated dependencies [3c94076]
  - langchain@0.3.35
  - @langchain/openai@0.6.14

## 0.3.56

### Patch Changes

- 6da726f: feat(@langchain/community): add sagemaker endpoint - embedding support
- 28dd44f: chore(couchbase): Deprecate CouchbaseVectorStore and create CouchbaseSearchVectorStore
- 9adccfe: chore(@langchain/community): remove Dria retriever
- 0a640ad: feat(langchain-community): add custom schema option for neon vector store
- 940e087: fix(astra): replace deprecated 'namespace' param name
- 8ac8edd: add support for advanced metadata filters in similarity search
- e9d1136: create index aurora dsql
- e0b48fd: fix(community): improve TogetherAI error handling for chat models
- c10ea3e: allow any chars in delimited identifiers in hanavector
- 9adccfe: chore(@langchain/community): remove Dria retriever
- Updated dependencies [41bd944]
- Updated dependencies [6019a7d]
- Updated dependencies [54f542c]
- Updated dependencies [707a768]
- Updated dependencies [caf5579]
- Updated dependencies [d60f40f]
  - @langchain/openai@0.6.12
  - langchain@0.3.34
  - @langchain/weaviate@0.2.3

## 0.3.55

### Patch Changes

- f201ab8: bump firebase-admin dependency (#8861)
- f201ab8: use URL encoding for paths in github document laoder (#8860)

## 0.3.54

### Patch Changes

- 4a3f5af: Update import_constants.ts (#8747)
- 8f9c617: postgres indexes getTime returning NaN due to missing alias
- 4d26533: BM25Retriever: escape regex metacharacters in getTermFrequency to prevent crashes
- 9f491d6: milvus: Fix upsert operations when autoId is false
- 9649f20: add jira issue title to metadata for documents
- 9543ba1: add personalAccessToken to jira loader
- Updated dependencies [e0bd88c]
- Updated dependencies [4a3f5af]
- Updated dependencies [424360b]
  - langchain@0.3.32
  - @langchain/openai@0.6.10


## Links discovered
- [#9326](https://github.com/langchain-ai/langchainjs/pull/9326)
- [`3e0cab6`](https://github.com/langchain-ai/langchainjs/commit/3e0cab61b32fae271936770b822cb9644f68b637)
- [@ayanyev](https://github.com/ayanyev)
- [[`415cb0b`](https://github.com/langchain-ai/langchainjs/commit/415cb0bfd26207583befdb02367bd12a46b33d51)
- [`a2ad61e`](https://github.com/langchain-ai/langchainjs/commit/a2ad61e787a06a55a615f63589a65ada05927792)
- [`34c472d`](https://github.com/langchain-ai/langchainjs/commit/34c472d129c9c3d58042fad6479fd15e0763feaf)
- [[`04bd55c`](https://github.com/langchain-ai/langchainjs/commit/04bd55c63d8a0cb56f85da0b61a6bd6169b383f3)
- [`ac0d4fe`](https://github.com/langchain-ai/langchainjs/commit/ac0d4fe3807e05eb2185ae8a36da69498e6163d4)
- [`39dbe63`](https://github.com/langchain-ai/langchainjs/commit/39dbe63e3d8390bb90bb8b17f00755fa648c5651)
- [`dfbe45f`](https://github.com/langchain-ai/langchainjs/commit/dfbe45f3cfade7a1dbe15b2d702a8e9f8e5ac93a)
- [release notes](https://docs.langchain.com/oss/javascript/releases/langchain-v1)

--- libs/langchain-core/CHANGELOG.md ---
# @langchain/core

## 1.0.6

### Patch Changes

- [#9431](https://github.com/langchain-ai/langchainjs/pull/9431) [`5709cb6`](https://github.com/langchain-ai/langchainjs/commit/5709cb64cc3e4eb300bde5ec8ae90686d2aa3d8e) Thanks [@dqbd](https://github.com/dqbd)! - fix(core): `store` should be accessible from tools

## 1.0.5

### Patch Changes

- [#9308](https://github.com/langchain-ai/langchainjs/pull/9308) [`04bd55c`](https://github.com/langchain-ai/langchainjs/commit/04bd55c63d8a0cb56f85da0b61a6bd6169b383f3) Thanks [@ro0sterjam](https://github.com/ro0sterjam)! - respect JSON schema references in interopZodTransformInputSchema

- [#9387](https://github.com/langchain-ai/langchainjs/pull/9387) [`ac0d4fe`](https://github.com/langchain-ai/langchainjs/commit/ac0d4fe3807e05eb2185ae8a36da69498e6163d4) Thanks [@hntrl](https://github.com/hntrl)! - Add `ModelProfile` and `.profile` properties to ChatModel

## 1.0.4

### Patch Changes

- 8319201: Export standard converter function utility

## 1.0.3

### Patch Changes

- 0a8a23b: feat(@langchain/core): support of ToolRuntime

## 1.0.2

### Patch Changes

- 6426eb6: fix chunks constructed with tool calls + chunks
- 619ae64: Add `BaseMessage.toFormattedString()`

## 1.0.1

### Patch changes

- cacc137: remove bad import map exports

## 1.0.0

🎉 **LangChain v1.0** is here! This release provides a focused, production-ready foundation for building agents with significant improvements to the core abstractions and APIs. See the [release notes](https://docs.langchain.com/oss/javascript/releases/langchain-v1) for more details.

### ✨ Major Features

#### Standard content blocks

A new unified API for accessing modern LLM features across all providers:

- **New `contentBlocks` property**: Provides provider-agnostic access to reasoning traces, citations, built-in tools (web search, code interpreters, etc.), and other advanced LLM features
- **Type-safe**: Full TypeScript support with type hints for all content block types
- **Backward compatible**: Content blocks can be loaded lazily with no breaking changes to existing code

Example:

```typescript
const response = await model.invoke([
  { role: "user", content: "What is the weather in Tokyo?" },
]);

// Access structured content blocks
for (const block of response.contentBlocks) {
  if (block.type === "thinking") {
    console.log("Model reasoning:", block.thinking);
  } else if (block.type === "text") {
    console.log("Response:", block.text);
  }
}
```

For more information, see our guide on [content blocks](https://docs.langchain.com/oss/javascript/langchain/messages#content).

#### Enhanced Message API

Improvements to the core message types:

- **Structured content**: Better support for multimodal content with the new content blocks API
- **Provider compatibility**: Consistent message format across all LLM providers
- **Rich metadata**: Enhanced metadata support for tracking message provenance and transformations

### 🔧 Improvements

- **Better structured output generation**: Core abstractions for generating structured outputs in the main agent loop
- **Improved type safety**: Enhanced TypeScript definitions across all core abstractions
- **Performance optimizations**: Reduced overhead in message processing and runnable composition
- **Better error handling**: More informative error messages and better error recovery

### 📦 Package Changes

The `@langchain/core` package remains focused on essential abstractions:

- Core message types and content blocks
- Base runnable abstractions
- Tool definitions and schemas
- Middleware infrastructure
- Callback system
- Output parsers
- Prompt templates

### 🔄 Migration Notes

**Backward Compatibility**: This release maintains backward compatibility with existing code. Content blocks are loaded lazily, so no changes are required to existing applications.

**New Features**: To take advantage of new features like content blocks and middleware:

1. Update to `@langchain/core@next`:

   ```bash
   npm install @langchain/core@1.0.0
   ```

2. Use the new `contentBlocks` property to access rich content:

   ```typescript
   const response = await model.invoke(messages);
   console.log(response.contentBlocks); // New API
   console.log(response.content); // Legacy API still works
   ```

3. For middleware and `createAgent`, install `langchain@next`:

   ```bash
   npm install langchain@1.0.0 @langchain/core@1.0.0
   ```

### 📚 Additional Resources

- [LangChain 1.0 Announcement](https://blog.langchain.com/langchain-langchain-1-0-alpha-releases/)
- [Migration Guide](https://docs.langchain.com/oss/javascript/migrate/langchain-v1)
- [Content Blocks Documentation](https://docs.langchain.com/oss/javascript/langchain/messages#content)
- [Agents Documentation](https://docs.langchain.com/oss/javascript/langchain/agents)

---

## 0.3.79

### Patch Changes

- 1063b43: fix chunks constructed with tool calls + chunks

## 0.3.78

### Patch Changes

- 1519a97: update chunk concat logic to match on missing ID fields
- 079e11d: omit tool call chunks without tool call id

## 0.3.76

### Patch Changes

- 41bd944: support base64 embeddings format
- e90bc0a: fix(core): prevent tool call chunks from merging incorrectly in AIMes…
- 3a99a40: Fix deserialization of RemoveMessage if represented as a plain object
- 58e9522: make mustache prompt with nested object working correctly
- e44dc1b: handle backticks in structured output

## 0.3.75

### Patch Changes

- d6d841f: fix(core): Fix deep nesting of runnables within traceables

## 0.3.74

### Patch Changes

- 4e53005: fix(core): Always inherit parent run id onto callback manager from context

## 0.3.73

### Patch Changes

- a5a2e10: add root export to satisfy bundler requirements


## Links discovered
- [#9431](https://github.com/langchain-ai/langchainjs/pull/9431)
- [`5709cb6`](https://github.com/langchain-ai/langchainjs/commit/5709cb64cc3e4eb300bde5ec8ae90686d2aa3d8e)
- [@dqbd](https://github.com/dqbd)
- [#9308](https://github.com/langchain-ai/langchainjs/pull/9308)
- [`04bd55c`](https://github.com/langchain-ai/langchainjs/commit/04bd55c63d8a0cb56f85da0b61a6bd6169b383f3)
- [@ro0sterjam](https://github.com/ro0sterjam)
- [#9387](https://github.com/langchain-ai/langchainjs/pull/9387)
- [`ac0d4fe`](https://github.com/langchain-ai/langchainjs/commit/ac0d4fe3807e05eb2185ae8a36da69498e6163d4)
- [@hntrl](https://github.com/hntrl)
- [release notes](https://docs.langchain.com/oss/javascript/releases/langchain-v1)
- [content blocks](https://docs.langchain.com/oss/javascript/langchain/messages#content)
- [LangChain 1.0 Announcement](https://blog.langchain.com/langchain-langchain-1-0-alpha-releases/)
- [Migration Guide](https://docs.langchain.com/oss/javascript/migrate/langchain-v1)
- [Content Blocks Documentation](https://docs.langchain.com/oss/javascript/langchain/messages#content)
- [Agents Documentation](https://docs.langchain.com/oss/javascript/langchain/agents)

--- libs/langchain-mcp-adapters/CHANGELOG.md ---
# @langchain/mcp-adapters

## 1.0.0

This release updates the package for compatibility with LangChain v1.0. See the v1.0 [release notes](https://docs.langchain.com/oss/javascript/releases/langchain-v1) for details on what's new.

## [0.1.7] - 2024-05-08

### Fixed

- Fixed SSE headers support to properly pass headers to eventsource
- Improved error handling for SSE connections
- Added proper support for Node.js eventsource library
- Fixed type errors in agent integration tests

### Added

- Improved test coverage to over 80%
- Added comprehensive error handling tests
- Added integration tests for different connection types

### Changed

- Updated ESLint configuration to properly exclude dist directory
- Improved build process to avoid linting errors

## [0.1.3] - 2023-03-11

### Changed

- Version bump to resolve npm publishing conflict
- Automated version management in GitHub Actions workflow

## [0.1.2] - 2023-03-10

### Added

- GitHub Actions workflows for PR validation, CI, and npm publishing
- Husky for Git hooks
- lint-staged for running linters on staged files
- Issue and PR templates
- CHANGELOG.md and CONTRIBUTING.md
- Improved npm publishing workflow with automatic version conflict resolution

### Fixed

- Fixed Husky deprecation warnings

## [0.1.0] - 2023-03-03

### Added

- Initial release
- Support for stdio and SSE transports
- MultiServerMCPClient for connecting to multiple MCP servers
- Configuration file support
- Examples for various use cases
- Integration with LangChain.js agents


## Links discovered
- [release notes](https://docs.langchain.com/oss/javascript/releases/langchain-v1)

--- libs/langchain-standard-tests/CHANGELOG.md ---
# @langchain/standard-tests

## 0.0.2

### Patch Changes

- Updated dependencies [[`5709cb6`](https://github.com/langchain-ai/langchainjs/commit/5709cb64cc3e4eb300bde5ec8ae90686d2aa3d8e)]:
  - @langchain/core@1.0.6

## 0.0.1

### Patch Changes

- Updated dependencies [[`04bd55c`](https://github.com/langchain-ai/langchainjs/commit/04bd55c63d8a0cb56f85da0b61a6bd6169b383f3), [`ac0d4fe`](https://github.com/langchain-ai/langchainjs/commit/ac0d4fe3807e05eb2185ae8a36da69498e6163d4)]:
  - @langchain/core@1.0.5


## Links discovered
- [[`5709cb6`](https://github.com/langchain-ai/langchainjs/commit/5709cb64cc3e4eb300bde5ec8ae90686d2aa3d8e)
- [[`04bd55c`](https://github.com/langchain-ai/langchainjs/commit/04bd55c63d8a0cb56f85da0b61a6bd6169b383f3)
- [`ac0d4fe`](https://github.com/langchain-ai/langchainjs/commit/ac0d4fe3807e05eb2185ae8a36da69498e6163d4)

--- libs/langchain-textsplitters/CHANGELOG.md ---
# @langchain/textsplitters

## 1.0.0

This release updates the package for compatibility with LangChain v1.0. See the v1.0 [release notes](https://docs.langchain.com/oss/javascript/releases/langchain-v1) for details on what's new.


## Links discovered
- [release notes](https://docs.langchain.com/oss/javascript/releases/langchain-v1)

--- libs/langchain/CHANGELOG.md ---
# langchain

## 1.0.6

### Patch Changes

- [#9434](https://github.com/langchain-ai/langchainjs/pull/9434) [`f7cfece`](https://github.com/langchain-ai/langchainjs/commit/f7cfecec29bf0f121e1a8b0baface5327d731122) Thanks [@deepansh946](https://github.com/deepansh946)! - Updated error handling behaviour of AgentNode

## 1.0.5

### Patch Changes

- [#9403](https://github.com/langchain-ai/langchainjs/pull/9403) [`944bf56`](https://github.com/langchain-ai/langchainjs/commit/944bf56ff0926e102c56a3073bfde6b751c97794) Thanks [@christian-bromann](https://github.com/christian-bromann)! - improvements to toolEmulator middleware

- [#9388](https://github.com/langchain-ai/langchainjs/pull/9388) [`831168a`](https://github.com/langchain-ai/langchainjs/commit/831168a5450bff706a319842626214281204346d) Thanks [@hntrl](https://github.com/hntrl)! - use `profile.maxInputTokens` in summarization middleware

- [#9393](https://github.com/langchain-ai/langchainjs/pull/9393) [`f1e2f9e`](https://github.com/langchain-ai/langchainjs/commit/f1e2f9eeb365bae78c8b5991ed41bfed58f25da6) Thanks [@christian-bromann](https://github.com/christian-bromann)! - align context editing with summarization interface

- [#9427](https://github.com/langchain-ai/langchainjs/pull/9427) [`bad7aea`](https://github.com/langchain-ai/langchainjs/commit/bad7aea86d3f60616952104c34a33de9561867c7) Thanks [@dqbd](https://github.com/dqbd)! - fix(langchain): add tool call contents and tool call ID to improve token count approximation

- [#9396](https://github.com/langchain-ai/langchainjs/pull/9396) [`ed6b581`](https://github.com/langchain-ai/langchainjs/commit/ed6b581e525cdf5d3b29abb1e17ca6169554c1b5) Thanks [@christian-bromann](https://github.com/christian-bromann)! - rename exit behavior from throw to error

## 1.0.4

### Patch Changes

- b401680: avoid invalid message order after summarization
- f63fc0f: fix(langchain): export ToolRuntime from langchain

## 1.0.3

### Patch Changes

- f1583cd: allow for model strings in summarization middleware
- e960f97: check message property when pulling chat models for vercel compat
- 66fc10c: fix(langchain): don't allow default or optional context schemas
- 0a8a23b: feat(@langchain/core): support of ToolRuntime
- b38be50: Add missing ToolMessage in toolStrategy structured output
- 42930b5: fix(langchain): improved state schema typing

## 1.0.2

### Patch Changes

- 2e45c43: fix(langchain): remove bad dynamic import for LS
- 28eceac: preserve full model name when deciding model provider

## 1.0.0

🎉 **LangChain v1.0** is here! This release provides a focused, production-ready foundation for building agents. We've streamlined the framework around three core improvements: **`createAgent`**, **standard content blocks**, and a **simplified package structure**. See the [release notes](https://docs.langchain.com/oss/javascript/releases/langchain-v1) for complete details.

### ✨ Major Features

#### `createAgent` - A new standard for building agents

`createAgent` is the new standard way to build agents in LangChain 1.0. It provides a simpler interface than `createReactAgent` from LangGraph while offering greater customization potential through middleware.

**Key features:**

- **Clean, intuitive API**: Build agents with minimal boilerplate
- **Built on LangGraph**: Get persistence, streaming, human-in-the-loop, and time travel out of the box
- **Middleware-first design**: Highly customizable through composable middleware
- **Improved structured output**: Generate structured outputs in the main agent loop without additional LLM calls

Example:

```typescript
import { createAgent } from "langchain";

const agent = createAgent({
  model: "anthropic:claude-sonnet-4-5-20250929",
  tools: [getWeather],
  systemPrompt: "You are a helpful assistant.",
});

const result = await agent.invoke({
  messages: [{ role: "user", content: "What is the weather in Tokyo?" }],
});

console.log(result.content);
```

Under the hood, `createAgent` is built on the basic agent loop—calling a model using LangGraph, letting it choose tools to execute, and then finishing when it calls no more tools.

**Built on LangGraph features (work out of the box):**

- **Persistence**: Conversations automatically persist across sessions with built-in checkpointing
- **Streaming**: Stream tokens, tool calls, and reasoning traces in real-time
- **Human-in-the-loop**: Pause agent execution for human approval before sensitive actions
- **Time travel**: Rewind conversations to any point and explore alternate paths

**Structured output improvements:**

- Generate structured outputs in the main loop instead of requiring an additional LLM call
- Models can choose between calling tools or using provider-side structured output generation
- Significant cost reduction by eliminating extra LLM calls

Example:

```typescript
import { createAgent } from "langchain";
import * as z from "zod";

const weatherSchema = z.object({
  temperature: z.number(),
  condition: z.string(),
});

const agent = createAgent({
  model: "openai:gpt-4o-mini",
  tools: [getWeather],
  responseFormat: weatherSchema,
});

const result = await agent.invoke({
  messages: [{ role: "user", content: "What is the weather in Tokyo?" }],
});

console.log(result.structuredResponse);
```

For more information, see [Agents documentation](https://docs.langchain.com/oss/javascript/langchain/agents).

#### Middleware

Middleware is what makes `createAgent` highly customizable, raising the ceiling for what you can build. Great agents require **context engineering**—getting the right information to the model at the right time. Middleware helps you control dynamic prompts, conversation summarization, selective tool access, state management, and guardrails through a composable abstraction.

**Prebuilt middleware** for common patterns:

```typescript
import {
  createAgent,
  summarizationMiddleware,
  humanInTheLoopMiddleware,
  piiRedactionMiddleware,
} from "langchain";

const agent = createAgent({
  model: "anthropic:claude-sonnet-4-5-20250929",
  tools: [readEmail, sendEmail],
  middleware: [
    piiRedactionMiddleware({ patterns: ["email", "phone", "ssn"] }),
    summarizationMiddleware({
      model: "anthropic:claude-sonnet-4-5-20250929",
      maxTokensBeforeSummary: 500,
    }),
    humanInTheLoopMiddleware({
      interruptOn: {
        sendEmail: {
          allowedDecisions: ["approve", "edit", "reject"],
        },
      },
    }),
  ] as const,
});
```

**Custom middleware** with lifecycle hooks:

| Hook            | When it runs             | Use cases                               |
| --------------- | ------------------------ | --------------------------------------- |
| `beforeAgent`   | Before calling the agent | Load memory, validate input             |
| `beforeModel`   | Before each LLM call     | Update prompts, trim messages           |
| `wrapModelCall` | Around each LLM call     | Intercept and modify requests/responses |
| `wrapToolCall`  | Around each tool call    | Intercept and modify tool execution     |
| `afterModel`    | After each LLM response  | Validate output, apply guardrails       |
| `afterAgent`    | After agent completes    | Save results, cleanup                   |

Example custom middleware:

```typescript
import { createMiddleware } from "langchain";

const contextSchema = z.object({
  userExpertise: z.enum(["beginner", "expert"]).default("beginner"),
});

const expertiseBasedToolMiddleware = createMiddleware({
  wrapModelCall: async (request, handler) => {
    const userLevel = request.runtime.context.userExpertise;
    if (userLevel === "expert") {
      const tools = [advancedSearch, dataAnalysis];
      return handler(request.replace("openai:gpt-5", tools));
    }
    const tools = [simpleSearch, basicCalculator];
    return handler(request.replace("openai:gpt-5-nano", tools));
  },
});

const agent = createAgent({
  model: "anthropic:claude-sonnet-4-5-20250929",
  tools: [simpleSearch, advancedSearch, basicCalculator, dataAnalysis],
  middleware: [expertiseBasedToolMiddleware],
  contextSchema,
});
```

For more information, see the [complete middleware guide](https://docs.langchain.com/oss/javascript/langchain/middleware).

#### Simplified Package

LangChain v1 streamlines the `langchain` package namespace to focus on essential building blocks for agents. The package exposes only the most useful and relevant functionality (most re-exported from `@langchain/core` for convenience).

**What's in the core `langchain` package:**

- `createAgent` and agent-related utilities
- Core message types and content blocks
- Middleware infrastructure
- Tool definitions and schemas
- Prompt templates
- Output parsers
- Base runnable abstractions

### 🔄 Migration Notes

#### `@langchain/classic` for Legacy Functionality

Legacy functionality has moved to [`@langchain/classic`](https://www.npmjs.com/package/@langchain/classic) to keep the core package lean and focused.

**What's in `@langchain/classic`:**

- Legacy chains and chain implementations
- The indexing API
- [`@langchain/community`](https://www.npmjs.com/package/@langchain/community) exports
- Other deprecated functionality

**To migrate legacy code:**

1. Install `@langchain/classic`:

   ```bash
   npm install @langchain/classic
   ```

2. Update your imports:

   ```typescript
   import { ... } from "langchain"; // [!code --]
   import { ... } from "@langchain/classic"; // [!code ++]

   import { ... } from "langchain/chains"; // [!code --]
   import { ... } from "@langchain/classic/chains"; // [!code ++]
   ```

#### Upgrading to v1

Install the v1 packages:

```bash
npm install langchain@1.0.0 @langchain/core@1.0.0
```

### 📚 Additional Resources

- [LangChain 1.0 Announcement](https://blog.langchain.com/langchain-langchain-1-0-alpha-releases/)
- [Migration Guide](https://docs.langchain.com/oss/javascript/migrate/langchain-v1)
- [Agents Documentation](https://docs.langchain.com/oss/javascript/langchain/agents)
- [Middleware Guide](https://blog.langchain.com/agent-middleware/)

---

## 0.3.36

### Patch Changes

- cabd762: fix(langchain): add ChatMistralAI to well known models
- Updated dependencies [e63c7cc]
- Updated dependencies [b8ffc1e]
  - @langchain/openai@0.6.16

## 0.3.35

### Patch Changes

- fd4691f: use `keyEncoder` instead of insecure cache key getter
- 2f19cd5: feat: Add Perplexity support to universal chat model
- 3c94076: fix(langchain): Bind schemas for other types of pulled hub prompts
- Updated dependencies [d38e9d6]
  - @langchain/openai@0.6.14

## 0.3.34

### Patch Changes

- 6019a7d: update JSONL loader to support complex json structures
- caf5579: prevent ConfigurableModel mutation when using withStructuredOutput or bindTools
- d60f40f: infer mistralai models
- Updated dependencies [41bd944]
- Updated dependencies [707a768]
  - @langchain/openai@0.6.12

## 0.3.33

### Patch Changes

- d2c7f09: support prompts not created from RunnableBinding

## 0.3.32

### Patch Changes

- e0bd88c: add support for conversion of ref in array schema
- Updated dependencies [4a3f5af]
- Updated dependencies [424360b]
  - @langchain/openai@0.6.10


## Links discovered
- [#9434](https://github.com/langchain-ai/langchainjs/pull/9434)
- [`f7cfece`](https://github.com/langchain-ai/langchainjs/commit/f7cfecec29bf0f121e1a8b0baface5327d731122)
- [@deepansh946](https://github.com/deepansh946)
- [#9403](https://github.com/langchain-ai/langchainjs/pull/9403)
- [`944bf56`](https://github.com/langchain-ai/langchainjs/commit/944bf56ff0926e102c56a3073bfde6b751c97794)
- [@christian-bromann](https://github.com/christian-bromann)
- [#9388](https://github.com/langchain-ai/langchainjs/pull/9388)
- [`831168a`](https://github.com/langchain-ai/langchainjs/commit/831168a5450bff706a319842626214281204346d)
- [@hntrl](https://github.com/hntrl)
- [#9393](https://github.com/langchain-ai/langchainjs/pull/9393)
- [`f1e2f9e`](https://github.com/langchain-ai/langchainjs/commit/f1e2f9eeb365bae78c8b5991ed41bfed58f25da6)
- [#9427](https://github.com/langchain-ai/langchainjs/pull/9427)
- [`bad7aea`](https://github.com/langchain-ai/langchainjs/commit/bad7aea86d3f60616952104c34a33de9561867c7)
- [@dqbd](https://github.com/dqbd)
- [#9396](https://github.com/langchain-ai/langchainjs/pull/9396)
- [`ed6b581`](https://github.com/langchain-ai/langchainjs/commit/ed6b581e525cdf5d3b29abb1e17ca6169554c1b5)
- [release notes](https://docs.langchain.com/oss/javascript/releases/langchain-v1)
- [Agents documentation](https://docs.langchain.com/oss/javascript/langchain/agents)
- [complete middleware guide](https://docs.langchain.com/oss/javascript/langchain/middleware)
- [`@langchain/classic`](https://www.npmjs.com/package/@langchain/classic)
- [`@langchain/community`](https://www.npmjs.com/package/@langchain/community)
- [LangChain 1.0 Announcement](https://blog.langchain.com/langchain-langchain-1-0-alpha-releases/)
- [Migration Guide](https://docs.langchain.com/oss/javascript/migrate/langchain-v1)
- [Agents Documentation](https://docs.langchain.com/oss/javascript/langchain/agents)
- [Middleware Guide](https://blog.langchain.com/agent-middleware/)

--- .github/contributing/INTEGRATIONS.md ---
# Contributing Integrations to LangChain

In addition to the [general contribution guidelines](https://github.com/langchain-ai/langchainjs/blob/main/CONTRIBUTING.md), there are a few extra things to consider when contributing third-party integrations to LangChain that will be covered here. The goal of this page is to help you draft PRs that take these considerations into account, and can therefore be merged sooner.

Integrations tend to fall into a set number of categories, each of which will have their own section below. Please read the [general guidelines](#general-concepts), then see the [integration-specific guidelines and example PRs](#integration-specific-guidelines-and-example-prs) section at the end of this page for additional information and examples.

## General concepts

The following guidelines apply broadly to all type of integrations:

### Creating a separate entrypoint

You should generally not export your new module from an `index.ts` file that contains many other exports. Instead, you should add a separate entrypoint for your integration in [`libs/langchain-community/langchain.config.js`](https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-community/langchain.config.js) within the `entrypoints` field in the config object:

```js
export const config = {
  internals: [ ... ],
  entrypoints: {
    load: "load/index",
    ...
    "vectorstores/chroma": "vectorstores/chroma",
    "vectorstores/hnswlib": "vectorstores/hnswlib",
    ...
  },
  ...
}
```

The entrypoint name should conform to its path in the repo. For example, if you were adding a new vector store for a hypothetical provider "langco", you might create it under `vectorstores/langco.ts`. You should add it above as:

```js
export const config = {
  internals: [ ... ],
  entrypoints: {
    load: "load/index",
    ...
    "vectorstores/chroma": "vectorstores/chroma",
    "vectorstores/hnswlib": "vectorstores/hnswlib",
    "vectorstores/langco": "vectorstores/langco",
    ...
  },
  ...
}
```

A user would then import your new vector store as `import { LangCoVectorStore } from "@langchain/community/vectorstores/langco";`.

### Third-party dependencies

You may use third-party dependencies in new integrations, but they should be added as `peerDependencies` and `devDependencies` with an entry under `peerDependenciesMeta` in [`libs/langchain-community/package.json`](https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-community/package.json), **not under any core `dependencies` list**. This keeps the overall package size small, as only people who are using your integration will need to install, and allows us to support a wider range of runtimes.

We suggest using caret syntax (`^`) for peer dependencies to support a wider range of people trying to use them as well as to be somewhat tolerant to non-major version updates, which should (theoretically) be the only breaking ones.

Please make sure all introduced dependencies are permissively licensed (MIT is recommended) and well-supported and maintained.

You must also add your new entrypoint under `requiresOptionalDependency` in the [`langchain.config.js`](https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-community/langchain.config.js) file to avoid breaking the build:

```js
export const config = {
  internals: [ ... ],
  entrypoints: {
    load: "load/index",
    ...
    "vectorstores/chroma": "vectorstores/chroma",
    "vectorstores/hnswlib": "vectorstores/hnswlib",
    "vectorstores/langco": "vectorstores/langco",
    ...
  },
  requiresOptionalDependency: [
    ...
    "vectorstores/langco",
    ...
  ],
  ...
}
```

If you have conformed to all of the above guidelines, you can just import your dependency as normal in your integration's file in the LangChain repo. Developers who import your entrypoint will then see an error message if they are missing the required peer dependency.

### Prioritize using exported third-party types for client config

Many integrations initialize instances of third-party clients, which often require vendor-specific configuration and options in addition to LangChain specific configuration. To avoid unnecessary repetition and desyncing, we suggest using imported third-party configuration types whenever available, unless there's a specific reason to only support a subset of these options.

Here's a simplified example:

```ts
import {
  LangCoClient,
  LangCoClientOptions,
} from "langco-client";

import { BaseDocumentLoader, DocumentLoader } from "../base.js";

export class LangCoDatasetLoader
  extends BaseDocumentLoader
  implements DocumentLoader
{
  protected langCoClient: LangCoClient;

  protected datasetId: string;

  protected verbose: boolean;

  constructor(
    datasetId: string,
    config: {
      verbose: boolean;
      clientOptions?: LangCoClientOptions;
    }
  ) {
    super();
    this.langCoClient = new LangCoClient(config.clientOptions ?? {});
    this.verbose = config.verbose ?? false;
  }
...
}
```

Above, we have a document loader that we're sure will always require a specific `datasetId`, and then some `config` properties that could change in the future containing a LangChain specific configuration property, `verbose`. We have also put a `clientOptions` parameter within that `config` that is passed directly into the third party client. With this structure, if the underlying client adds new options, all we need to do is bump the version.

### Documentation and integration tests

We highly appreciate documentation and integration tests showing how to set up and use your integration. Providing this will make it much easier for reviewers to verify that your integration works and will streamline the review process.

### Linting and formatting

As with all contributions, make sure you run `pnpm lint` and `pnpm format` so that everything conforms to our established style.

### Separate integration packages

While most integrations should generally reside in the `libs/langchain-community` workspace and be imported as `@langchain/community/module/name`, more in-depth integrations or suites of integrations may also reside in separate packages that depend on and extend `@langchain/core`. See [`@langchain/google-genai`](https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-google-genai) for an example.

To make creating packages like this easier, we offer the [`create-langchain-integration`](https://github.com/langchain-ai/langchainjs/blob/main/libs/create-langchain-integration/) utility that will automatically scaffold a repo with support for both ESM + CJS entrypoints. You can run it like this:

```bash
$ npx create-langchain-integration
```

The workflows and considerations for these packages are mostly the same as those in `@langchain/community`, with the exception that third-party dependencies should be hard dependencies instead of peer dependencies since the end-user will manually install your integration package anyway.

You will need to make sure that your package is compatible with the current minor version of `@langchain/core` in order for it to be interoperable with other integration packages and the latest versions of LangChain. We recommend using a tilde syntax for your integration package's `@langchain/core` dependency to support a wider range of core patch versions.

## Integration-specific guidelines and example PRs

Below are links to guides with advice and tips for specific types of integrations. These are currently out of date with the `@langchain/community` split, but will give you a rough idea of what is necessary:

- [LLM providers](https://github.com/langchain-ai/langchainjs/blob/main/.github/contributing/integrations/LLMS.md) (e.g. OpenAI's GPT-3)
- Chat model providers (TODO) (e.g. Anthropic's Claude, OpenAI's GPT-4)
- [Memory](https://github.com/langchain-ai/langchainjs/blob/main/.github/contributing/integrations/MEMORY.md) (used to give an LLM or chat model context of past conversations, e.g. Motörhead)
- [Vector stores](https://github.com/langchain-ai/langchainjs/blob/main/.github/contributing/integrations/VECTOR_STORES.md) (e.g. Pinecone)
- [Persistent message stores](https://github.com/langchain-ai/langchainjs/blob/main/.github/contributing/integrations/MESSAGE_STORES.md) (used to persistently store and load raw chat histories, e.g. Redis)
- [Document loaders](https://github.com/langchain-ai/langchainjs/blob/main/.github/contributing/integrations/DOCUMENT_LOADERS.md) (used to load documents for later storage into vector stores, e.g. Apify)
- [Embeddings](https://github.com/langchain-ai/langchainjs/blob/main/.github/contributing/integrations/EMBEDDINGS.md) (used to create embeddings of text documents or strings e.g. Cohere)
- [Tools](https://github.com/langchain-ai/langchainjs/blob/main/.github/contributing/integrations/TOOLS.md) (used for agents, e.g. the SERP API tool)

This is a living document, so please make a pull request if we're missing anything useful!


## Links discovered
- [general contribution guidelines](https://github.com/langchain-ai/langchainjs/blob/main/CONTRIBUTING.md)
- [`libs/langchain-community/langchain.config.js`](https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-community/langchain.config.js)
- [`libs/langchain-community/package.json`](https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-community/package.json)
- [`langchain.config.js`](https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-community/langchain.config.js)
- [`@langchain/google-genai`](https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-google-genai)
- [`create-langchain-integration`](https://github.com/langchain-ai/langchainjs/blob/main/libs/create-langchain-integration/)
- [LLM providers](https://github.com/langchain-ai/langchainjs/blob/main/.github/contributing/integrations/LLMS.md)
- [Memory](https://github.com/langchain-ai/langchainjs/blob/main/.github/contributing/integrations/MEMORY.md)
- [Vector stores](https://github.com/langchain-ai/langchainjs/blob/main/.github/contributing/integrations/VECTOR_STORES.md)
- [Persistent message stores](https://github.com/langchain-ai/langchainjs/blob/main/.github/contributing/integrations/MESSAGE_STORES.md)
- [Document loaders](https://github.com/langchain-ai/langchainjs/blob/main/.github/contributing/integrations/DOCUMENT_LOADERS.md)
- [Embeddings](https://github.com/langchain-ai/langchainjs/blob/main/.github/contributing/integrations/EMBEDDINGS.md)
- [Tools](https://github.com/langchain-ai/langchainjs/blob/main/.github/contributing/integrations/TOOLS.md)

--- libs/providers/langchain-anthropic/CHANGELOG.md ---
# @langchain/anthropic

## 1.1.0

### Minor Changes

- [#9424](https://github.com/langchain-ai/langchainjs/pull/9424) [`f17b2c9`](https://github.com/langchain-ai/langchainjs/commit/f17b2c9db047fab2d1db2d9aa791ec220cc9dd0a) Thanks [@hntrl](https://github.com/hntrl)! - add support for `betas` param

- [#9424](https://github.com/langchain-ai/langchainjs/pull/9424) [`f17b2c9`](https://github.com/langchain-ai/langchainjs/commit/f17b2c9db047fab2d1db2d9aa791ec220cc9dd0a) Thanks [@hntrl](https://github.com/hntrl)! - add support for native structured output

### Patch Changes

- [#9424](https://github.com/langchain-ai/langchainjs/pull/9424) [`f17b2c9`](https://github.com/langchain-ai/langchainjs/commit/f17b2c9db047fab2d1db2d9aa791ec220cc9dd0a) Thanks [@hntrl](https://github.com/hntrl)! - bump sdk version

## 1.0.1

### Patch Changes

- [#9387](https://github.com/langchain-ai/langchainjs/pull/9387) [`ac0d4fe`](https://github.com/langchain-ai/langchainjs/commit/ac0d4fe3807e05eb2185ae8a36da69498e6163d4) Thanks [@hntrl](https://github.com/hntrl)! - Add `ModelProfile` and `.profile` properties to ChatModel

## 1.0.0

This release updates the package for compatibility with LangChain v1.0. See the v1.0 [release notes](https://docs.langchain.com/oss/javascript/releases/langchain-v1) for details on what's new.

## 0.3.31

### Patch Changes

- 51f638e: fix content management param

## 0.3.30

### Patch Changes

- 6c7eb84: fix sonnet-4.5 thinking

## 0.3.29

### Patch Changes

- 93493ee: add support for context management
- 93493ee: add support for memory server tools
- 93493ee: add default init options for sonnet-4.5

## 0.3.28

### Patch Changes

- 9ed7dfa: fix unhandled tool choice 'none'

## 0.3.27

### Patch Changes

- 49c242c: fix opus 4.1 topP error when streaming


## Links discovered
- [#9424](https://github.com/langchain-ai/langchainjs/pull/9424)
- [`f17b2c9`](https://github.com/langchain-ai/langchainjs/commit/f17b2c9db047fab2d1db2d9aa791ec220cc9dd0a)
- [@hntrl](https://github.com/hntrl)
- [#9387](https://github.com/langchain-ai/langchainjs/pull/9387)
- [`ac0d4fe`](https://github.com/langchain-ai/langchainjs/commit/ac0d4fe3807e05eb2185ae8a36da69498e6163d4)
- [release notes](https://docs.langchain.com/oss/javascript/releases/langchain-v1)

--- .devcontainer/README.md ---
# Dev container

This project includes a [dev container](https://containers.dev/), which lets you use a container as a full-featured dev environment.

You can use the dev container configuration in this folder to build and run the app without needing to install any of its tools locally! You can use it in [GitHub Codespaces](https://github.com/features/codespaces) or the [VS Code Dev Containers extension](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers).

## GitHub Codespaces
[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/hwchase17/langchainjs)
You may use the button above, or follow these steps to open this repo in a Codespace:
1. Click the **Code** drop-down menu at the top of https://github.com/langchain-ai/langchainjs.
1. Click on the **Codespaces** tab.
1. Click **Create codespace on main** .

For more info, check out the [GitHub documentation](https://docs.github.com/en/free-pro-team@latest/github/developing-online-with-codespaces/creating-a-codespace#creating-a-codespace).

## VS Code Dev Containers
[![Open in Dev Containers](https://img.shields.io/static/v1?label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/langchain-ai/langchainjs)

If you already have VS Code and Docker installed, you can use the button above to get started. This will cause VS Code to automatically install the Dev Containers extension if needed, clone the source code into a container volume, and spin up a dev container for use.

You can also follow these steps to open this repo in a container using the VS Code Dev Containers extension:

1. If this is your first time using a development container, please ensure your system meets the pre-reqs (i.e. have Docker installed) in the [getting started steps](https://aka.ms/vscode-remote/containers/getting-started).

2. Open a locally cloned copy of the code:

   - Clone this repository to your local filesystem.
   - Press <kbd>F1</kbd> and select the **Dev Containers: Open Folder in Container...** command.
   - Select the cloned copy of this folder, wait for the container to start, and try things out!

You can learn more in the [Dev Containers documentation](https://code.visualstudio.com/docs/devcontainers/containers).

## Tips and tricks

* If you are working with the same repository folder in a container and Windows, you'll want consistent line endings (otherwise you may see hundreds of changes in the SCM view). Creating a `.gitattributes` file in the root of this repo will disable line ending conversion and should prevent this. See [tips and tricks](https://code.visualstudio.com/docs/devcontainers/tips-and-tricks#_resolving-git-line-ending-issues-in-containers-resulting-in-many-modified-files) for more info.
* If you'd like to review the contents of the image used in this dev container, you can check it out in the [devcontainers/images](https://github.com/devcontainers/images/tree/main/src/typescript-node) repo.


## Links discovered
- [dev container](https://containers.dev/)
- [GitHub Codespaces](https://github.com/features/codespaces)
- [VS Code Dev Containers extension](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers)
- [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)
- [GitHub documentation](https://docs.github.com/en/free-pro-team@latest/github/developing-online-with-codespaces/creating-a-codespace#creating-a-codespace)
- [![Open in Dev Containers](https://img.shields.io/static/v1?label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)
- [getting started steps](https://aka.ms/vscode-remote/containers/getting-started)
- [Dev Containers documentation](https://code.visualstudio.com/docs/devcontainers/containers)
- [tips and tricks](https://code.visualstudio.com/docs/devcontainers/tips-and-tricks#_resolving-git-line-ending-issues-in-containers-resulting-in-many-modified-files)
- [devcontainers/images](https://github.com/devcontainers/images/tree/main/src/typescript-node)

--- README.md ---
# 🦜️🔗 LangChain.js

![npm](https://img.shields.io/npm/dm/langchain) [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT) [![Twitter](https://img.shields.io/twitter/url/https/twitter.com/langchainai.svg?style=social&label=Follow%20%40LangChainAI)](https://twitter.com/langchainai)

LangChain is a framework for building LLM-powered applications. It helps you chain together interoperable components and third-party integrations to simplify AI application development — all while future-proofing decisions as the underlying technology evolves.

**Documentation**: To learn more about LangChain, check out [the docs](https://docs.langchain.com/oss/javascript/langchain/overview).

If you're looking for more advanced customization or agent orchestration, check out [LangGraph.js](https://langchain-ai.github.io/langgraphjs/). our framework for building agents and controllable workflows.

> [!NOTE]
> Looking for the Python version? Check out [LangChain](https://github.com/langchain-ai/langchain).

To help you ship LangChain apps to production faster, check out [LangSmith](https://smith.langchain.com).
[LangSmith](https://smith.langchain.com) is a unified developer platform for building, testing, and monitoring LLM applications.

## ⚡️ Quick Install

You can use npm, pnpm, or yarn to install LangChain.js

`npm install -S langchain` or `pnpm install langchain` or `yarn add langchain`

## 🚀 Why use LangChain?

LangChain helps developers build applications powered by LLMs through a standard interface for agents, models, embeddings, vector stores, and more.

Use LangChain for:

- **Real-time data augmentation**. Easily connect LLMs to diverse data sources and external/internal systems, drawing from LangChain’s vast library of integrations with model providers, tools, vector stores, retrievers, and more.
- **Model interoperability**. Swap models in and out as your engineering team experiments to find the best choice for your application’s needs. As the industry frontier evolves, adapt quickly — LangChain’s abstractions keep you moving without losing momentum.

## 📦 LangChain's ecosystem

- [LangSmith](https://www.langchain.com/langsmith) - Unified developer platform for building, testing, and monitoring LLM applications. With LangSmith, you can debug poor-performing LLM app runs, evaluate agent trajectories, gain visibility in production, and deploy agents with confidence.
- [LangGraph](https://docs.langchain.com/oss/javascript/langgraph/overview) - Build agents that can reliably handle complex tasks with LangGraph, our low-level agent orchestration framework. LangGraph offers customizable architecture, long-term memory, and human-in-the-loop workflows — and is trusted in production by companies like LinkedIn, Uber, Klarna, and GitLab.

## 🌐 Supported Environments

LangChain.js is written in TypeScript and can be used in:

- Node.js (ESM and CommonJS) - 18.x, 19.x, 20.x, 22.x
- Cloudflare Workers
- Vercel / Next.js (Browser, Serverless and Edge functions)
- Supabase Edge Functions
- Browser
- Deno

## 📖 Additional Resources

- [Getting started](https://docs.langchain.com/oss/javascript/langchain/overview): Installation, setting up the environment, simple examples
- [Learn](https://docs.langchain.com/oss/javascript/langchain/learn): Learn about the core concepts of LangChain.
- [LangChain Forum](https://forum.langchain.com): Connect with the community and share all of your technical questions, ideas, and feedback.
- [Chat LangChain](https://chat.langchain.com): Ask questions & chat with our documentation.

## 💁 Contributing

As an open-source project in a rapidly developing field, we are extremely open to contributions, whether it be in the form of a new feature, improved infrastructure, or better documentation.

For detailed information on how to contribute, see [here](https://github.com/langchain-ai/langchainjs/blob/main/CONTRIBUTING.md).

Please report any security issues or concerns following our [security guidelines](https://github.com/langchain-ai/langchainjs/blob/main/SECURITY.md).


## Links discovered
- [npm](https://img.shields.io/npm/dm/langchain)
- [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)
- [![Twitter](https://img.shields.io/twitter/url/https/twitter.com/langchainai.svg?style=social&label=Follow%20%40LangChainAI)
- [the docs](https://docs.langchain.com/oss/javascript/langchain/overview)
- [LangGraph.js](https://langchain-ai.github.io/langgraphjs/)
- [LangChain](https://github.com/langchain-ai/langchain)
- [LangSmith](https://smith.langchain.com)
- [LangSmith](https://www.langchain.com/langsmith)
- [LangGraph](https://docs.langchain.com/oss/javascript/langgraph/overview)
- [Getting started](https://docs.langchain.com/oss/javascript/langchain/overview)
- [Learn](https://docs.langchain.com/oss/javascript/langchain/learn)
- [LangChain Forum](https://forum.langchain.com)
- [Chat LangChain](https://chat.langchain.com)
- [here](https://github.com/langchain-ai/langchainjs/blob/main/CONTRIBUTING.md)
- [security guidelines](https://github.com/langchain-ai/langchainjs/blob/main/SECURITY.md)

--- .changeset/README.md ---
# Changesets

Hello and welcome! This folder has been automatically generated by `@changesets/cli`, a build tool that works
with multi-package repos, or single-package repos to help you version and publish your code. You can
find the full documentation for it [in our repository](https://github.com/changesets/changesets)

We have a quick list of common questions to get you started engaging with this project in
[our documentation](https://github.com/changesets/changesets/blob/main/docs/common-questions.md)


## Links discovered
- [in our repository](https://github.com/changesets/changesets)
- [our documentation](https://github.com/changesets/changesets/blob/main/docs/common-questions.md)

--- .changeset/fuzzy-towns-listen.md ---
---
"@langchain/anthropic": patch
---

fix betas being passed to client when streaming


--- .github/pull_request_template.md ---
<!--
Thank you for contributing to LangChain.js! Your PR will appear in our next release under the title you set above. Please make sure it highlights your valuable contribution.

To help streamline the review process, please make sure you read our contribution guidelines:
https://github.com/langchain-ai/langchainjs/blob/main/CONTRIBUTING.md

If you are adding an integration (e.g. a new LLM, vector store, or memory), please also read our additional guidelines for integrations. You must include tests (if applicable) and documentation for your integration:
https://github.com/langchain-ai/langchainjs/blob/main/.github/contributing/INTEGRATIONS.md

Replace this block with a description of the change, the issue it fixes (if applicable), and relevant context.

Finally, we'd love to show appreciation for your contribution - if you'd like us to shout you out on Twitter, please also include your handle below!
-->

<!-- Remove if not applicable -->

Fixes # (issue)


--- .github/actions/people/app/main.py ---
# Adapted from https://github.com/tiangolo/fastapi/blob/master/.github/actions/people/app/main.py

import logging
import subprocess
import sys
from collections import Counter
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import Any, Container, Dict, List, Set, Union

import httpx
import yaml
from github import Github
from pydantic import BaseModel, SecretStr
from pydantic_settings import BaseSettings

github_graphql_url = "https://api.github.com/graphql"
questions_category_id = "DIC_kwDOIPDwls4CS6Ve"

# discussions_query = """
# query Q($after: String, $category_id: ID) {
#   repository(name: "langchainjs", owner: "langchain-ai") {
#     discussions(first: 100, after: $after, categoryId: $category_id) {
#       edges {
#         cursor
#         node {
#           number
#           author {
#             login
#             avatarUrl
#             url
#           }
#           title
#           createdAt
#           comments(first: 100) {
#             nodes {
#               createdAt
#               author {
#                 login
#                 avatarUrl
#                 url
#               }
#               isAnswer
#               replies(first: 10) {
#                 nodes {
#                   createdAt
#                   author {
#                     login
#                     avatarUrl
#                     url
#                   }
#                 }
#               }
#             }
#           }
#         }
#       }
#     }
#   }
# }
# """

# issues_query = """
# query Q($after: String) {
#   repository(name: "langchainjs", owner: "langchain-ai") {
#     issues(first: 100, after: $after) {
#       edges {
#         cursor
#         node {
#           number
#           author {
#             login
#             avatarUrl
#             url
#           }
#           title
#           createdAt
#           state
#           comments(first: 100) {
#             nodes {
#               createdAt
#               author {
#                 login
#                 avatarUrl
#                 url
#               }
#             }
#           }
#         }
#       }
#     }
#   }
# }
# """

prs_query = """
query Q($after: String) {
  repository(name: "langchainjs", owner: "langchain-ai") {
    pullRequests(first: 100, after: $after, states: MERGED) {
      edges {
        cursor
        node {
          changedFiles
          additions
          deletions
          number
          labels(first: 100) {
            nodes {
              name
            }
          }
          author {
            login
            avatarUrl
            url
            ... on User {
              twitterUsername
            }
          }
          title
          createdAt
          state
          reviews(first:100) {
            nodes {
              author {
                login
                avatarUrl
                url
                ... on User {
                  twitterUsername
                }
              }
              state
            }
          }
        }
      }
    }
  }
}
"""


class Author(BaseModel):
    login: str
    avatarUrl: str
    url: str
    twitterUsername: Union[str, None] = None


# Issues and Discussions


class CommentsNode(BaseModel):
    createdAt: datetime
    author: Union[Author, None] = None


class Replies(BaseModel):
    nodes: List[CommentsNode]


class DiscussionsCommentsNode(CommentsNode):
    replies: Replies


class Comments(BaseModel):
    nodes: List[CommentsNode]


class DiscussionsComments(BaseModel):
    nodes: List[DiscussionsCommentsNode]


class IssuesNode(BaseModel):
    number: int
    author: Union[Author, None] = None
    title: str
    createdAt: datetime
    state: str
    comments: Comments


class DiscussionsNode(BaseModel):
    number: int
    author: Union[Author, None] = None
    title: str
    createdAt: datetime
    comments: DiscussionsComments


class IssuesEdge(BaseModel):
    cursor: str
    node: IssuesNode


class DiscussionsEdge(BaseModel):
    cursor: str
    node: DiscussionsNode


class Issues(BaseModel):
    edges: List[IssuesEdge]


class Discussions(BaseModel):
    edges: List[DiscussionsEdge]


class IssuesRepository(BaseModel):
    issues: Issues


class DiscussionsRepository(BaseModel):
    discussions: Discussions


class IssuesResponseData(BaseModel):
    repository: IssuesRepository


class DiscussionsResponseData(BaseModel):
    repository: DiscussionsRepository


class IssuesResponse(BaseModel):
    data: IssuesResponseData


class DiscussionsResponse(BaseModel):
    data: DiscussionsResponseData


# PRs


class LabelNode(BaseModel):
    name: str


class Labels(BaseModel):
    nodes: List[LabelNode]


class ReviewNode(BaseModel):
    author: Union[Author, None] = None
    state: str


class Reviews(BaseModel):
    nodes: List[ReviewNode]


class PullRequestNode(BaseModel):
    number: int
    labels: Labels
    author: Union[Author, None] = None
    changedFiles: int
    additions: int
    deletions: int
    title: str
    createdAt: datetime
    state: str
    reviews: Reviews
    # comments: Comments


class PullRequestEdge(BaseModel):
    cursor: str
    node: PullRequestNode


class PullRequests(BaseModel):
    edges: List[PullRequestEdge]


class PRsRepository(BaseModel):
    pullRequests: PullRequests


class PRsResponseData(BaseModel):
    repository: PRsRepository


class PRsResponse(BaseModel):
    data: PRsResponseData


class Settings(BaseSettings):
    input_token: SecretStr
    github_repository: str
    httpx_timeout: int = 30


def get_graphql_response(
    *,
    settings: Settings,
    query: str,
    after: Union[str, None] = None,
    category_id: Union[str, None] = None,
) -> Dict[str, Any]:
    headers = {"Authorization": f"token {settings.input_token.get_secret_value()}"}
    # category_id is only used by one query, but GraphQL allows unused variables, so
    # keep it here for simplicity
    variables = {"after": after, "category_id": category_id}
    response = httpx.post(
        github_graphql_url,
        headers=headers,
        timeout=settings.httpx_timeout,
        json={"query": query, "variables": variables, "operationName": "Q"},
    )
    if response.status_code != 200:
        logging.error(
            f"Response was not 200, after: {after}, category_id: {category_id}"
        )
        logging.error(response.text)
        raise RuntimeError(response.text)
    data = response.json()
    if "errors" in data:
        logging.error(f"Errors in response, after: {after}, category_id: {category_id}")
        logging.error(data["errors"])
        logging.error(response.text)
        raise RuntimeError(response.text)
    return data


# def get_graphql_issue_edges(*, settings: Settings, after: Union[str, None] = None):
#     data = get_graphql_response(settings=settings, query=issues_query, after=after)
#     graphql_response = IssuesResponse.model_validate(data)
#     return graphql_response.data.repository.issues.edges


# def get_graphql_question_discussion_edges(
#     *,
#     settings: Settings,
#     after: Union[str, None] = None,
# ):
#     data = get_graphql_response(
#         settings=settings,
#         query=discussions_query,
#         after=after,
#         category_id=questions_category_id,
#     )
#     graphql_response = DiscussionsResponse.model_validate(data)
#     return graphql_response.data.repository.discussions.edges


def get_graphql_pr_edges(*, settings: Settings, after: Union[str, None] = None):
    if after is None:
        print("Querying PRs...")
    else:
        print(f"Querying PRs with cursor {after}...")
    data = get_graphql_response(
        settings=settings,
        query=prs_query,
        after=after
    )
    graphql_response = PRsResponse.model_validate(data)
    return graphql_response.data.repository.pullRequests.edges


# def get_issues_experts(settings: Settings):
#     issue_nodes: List[IssuesNode] = []
#     issue_edges = get_graphql_issue_edges(settings=settings)

#     while issue_edges:
#         for edge in issue_edges:
#             issue_nodes.append(edge.node)
#         last_edge = issue_edges[-1]
#         issue_edges = get_graphql_issue_edges(settings=settings, after=last_edge.cursor)

#     commentors = Counter()
#     last_month_commentors = Counter()
#     authors: Dict[str, Author] = {}

#     now = datetime.now(tz=timezone.utc)
#     one_month_ago = now - timedelta(days=30)

#     for issue in issue_nodes:
#         issue_author_name = None
#         if issue.author:
#             authors[issue.author.login] = issue.author
#             issue_author_name = issue.author.login
#         issue_commentors = set()
#         for comment in issue.comments.nodes:
#             if comment.author:
#                 authors[comment.author.login] = comment.author
#                 if comment.author.login != issue_author_name:
#                     issue_commentors.add(comment.author.login)
#         for author_name in issue_commentors:
#             commentors[author_name] += 1
#             if issue.createdAt > one_month_ago:
#                 last_month_commentors[author_name] += 1

#     return commentors, last_month_commentors, authors


# def get_discussions_experts(settings: Settings):
#     discussion_nodes: List[DiscussionsNode] = []
#     discussion_edges = get_graphql_question_discussion_edges(settings=settings)

#     while discussion_edges:
#         for discussion_edge in discussion_edges:
#             discussion_nodes.append(discussion_edge.node)
#         last_edge = discussion_edges[-1]
#         discussion_edges = get_graphql_question_discussion_edges(
#             settings=settings, after=last_edge.cursor
#         )

#     commentors = Counter()
#     last_month_commentors = Counter()
#     authors: Dict[str, Author] = {}

#     now = datetime.now(tz=timezone.utc)
#     one_month_ago = now - timedelta(days=30)

#     for discussion in discussion_nodes:
#         discussion_author_name = None
#         if discussion.author:
#             authors[discussion.author.login] = discussion.author
#             discussion_author_name = discussion.author.login
#         discussion_commentors = set()
#         for comment in discussion.comments.nodes:
#             if comment.author:
#                 authors[comment.author.login] = comment.author
#                 if comment.author.login != discussion_author_name:
#                     discussion_commentors.add(comment.author.login)
#             for reply in comment.replies.nodes:
#                 if reply.author:
#                     authors[reply.author.login] = reply.author
#                     if reply.author.login != discussion_author_name:
#                         discussion_commentors.add(reply.author.login)
#         for author_name in discussion_commentors:
#             commentors[author_name] += 1
#             if discussion.createdAt > one_month_ago:
#                 last_month_commentors[author_name] += 1
#     return commentors, last_month_commentors, authors


# def get_experts(settings: Settings):
#     (
#         discussions_commentors,
#         discussions_last_month_commentors,
#         discussions_authors,
#     ) = get_discussions_experts(settings=settings)
#     commentors = discussions_commentors
#     last_month_commentors = discussions_last_month_commentors
#     authors = {**discussions_authors}
#     return commentors, last_month_commentors, authors


def _logistic(x, k):
    return x / (x + k)


def get_contributors(settings: Settings):
    pr_nodes: List[PullRequestNode] = []
    pr_edges = get_graphql_pr_edges(settings=settings)

    while pr_edges:
        for edge in pr_edges:
            pr_nodes.append(edge.node)
        last_edge = pr_edges[-1]
        pr_edges = get_graphql_pr_edges(settings=settings, after=last_edge.cursor)

    contributors = Counter()
    contributor_scores = Counter()
    recent_contributor_scores = Counter()
    reviewers = Counter()
    authors: Dict[str, Author] = {}

    for pr in pr_nodes:
        pr_reviewers: Set[str] = set()
        for review in pr.reviews.nodes:
            if review.author:
                authors[review.author.login] = review.author
                pr_reviewers.add(review.author.login)
        for reviewer in pr_reviewers:
            reviewers[reviewer] += 1
        if pr.author:
            authors[pr.author.login] = pr.author
            contributors[pr.author.login] += 1
            files_changed = pr.changedFiles
            lines_changed = pr.additions + pr.deletions
            score = _logistic(files_changed, 20) + _logistic(lines_changed, 100)
            contributor_scores[pr.author.login] += score
            three_months_ago = (datetime.now(timezone.utc) - timedelta(days=3*30))
            if pr.createdAt > three_months_ago:
                recent_contributor_scores[pr.author.login] += score
    return contributors, contributor_scores, recent_contributor_scores, reviewers, authors


def get_top_users(
    *,
    counter: Counter,
    min_count: int,
    authors: Dict[str, Author],
    skip_users: Container[str],
):
    users = []
    for commentor, count in counter.most_common():
        if commentor in skip_users:
            continue
        if count >= min_count:
            author = authors[commentor]
            users.append(
                {
                    "login": commentor,
                    "count": count,
                    "avatarUrl": author.avatarUrl,
                    "twitterUsername": author.twitterUsername,
                    "url": author.url,
                }
            )
    return users


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    settings = Settings()
    logging.info(f"Using config: {settings.model_dump_json()}")
    g = Github(settings.input_token.get_secret_value())
    repo = g.get_repo(settings.github_repository)
    # question_commentors, question_last_month_commentors, question_authors = get_experts(
    #     settings=settings
    # )
    contributors, contributor_scores, recent_contributor_scores, reviewers, pr_authors = get_contributors(
        settings=settings
    )
    # authors = {**question_authors, **pr_authors}
    authors = {**pr_authors}
    maintainers_logins = {
        "hwchase17",
        "nfcampos",
        "jacoblee93",
        "dqbd",
        "bracesproul",
    }
    hidden_logins = {
        "dev2049",
        "vowelparrot",
        "obi1kenobi",
        "langchain-infra",
        "efriis",
        "eyurtsev",
        "rlancemartin"
        "akira",
        "agola11",
        "baskaryan",
        "hinthornw",
    }
    bot_names = {"dosubot", "github-actions", "CodiumAI-Agent"}
    maintainers = []
    for login in maintainers_logins:
        user = authors[login]
        maintainers.append(
            {
                "login": login,
                "count": contributors[login], #+ question_commentors[login],
                "avatarUrl": user.avatarUrl,
                "twitterUsername": user.twitterUsername,
                "url": user.url,
            }
        )

    # min_count_expert = 10
    # min_count_last_month = 3
    min_score_contributor = 1
    min_count_reviewer = 5
    skip_users = maintainers_logins | bot_names | hidden_logins
    # experts = get_top_users(
    #     counter=question_commentors,
    #     min_count=min_count_expert,
    #     authors=authors,
    #     skip_users=skip_users,
    # )
    # last_month_active = get_top_users(
    #     counter=question_last_month_commentors,
    #     min_count=min_count_last_month,
    #     authors=authors,
    #     skip_users=skip_users,
    # )
    top_recent_contributors = get_top_users(
        counter=recent_contributor_scores,
        min_count=min_score_contributor,
        authors=authors,
        skip_users=skip_users,
    )
    top_contributors = get_top_users(
        counter=contributor_scores,
        min_count=min_score_contributor,
        authors=authors,
        skip_users=skip_users,
    )
    top_reviewers = get_top_users(
        counter=reviewers,
        min_count=min_count_reviewer,
        authors=authors,
        skip_users=skip_users,
    )

    people = {
        "maintainers": maintainers,
        # "experts": experts,
        # "last_month_active": last_month_active,
        "top_recent_contributors": top_recent_contributors,
        "top_contributors": top_contributors,
        "top_reviewers": top_reviewers,
    }
    people_path = Path("./docs/core_docs/data/people.yml")
    people_old_content = people_path.read_text(encoding="utf-8")
    new_people_content = yaml.dump(
        people, sort_keys=False, width=200, allow_unicode=True
    )
    if (
        people_old_content == new_people_content
    ):
        logging.info("The LangChain People data hasn't changed, finishing.")
        sys.exit(0)
    people_path.write_text(new_people_content, encoding="utf-8")
    logging.info("Setting up GitHub Actions git user")
    subprocess.run(["git", "config", "user.name", "github-actions"], check=True)
    subprocess.run(
        ["git", "config", "user.email", "github-actions@github.com"], check=True
    )
    branch_name = "langchain/langchain-people"
    logging.info(f"Creating a new branch {branch_name}")
    subprocess.run(["git", "checkout", "-B", branch_name], check=True)
    logging.info("Adding updated file")
    subprocess.run(
        ["git", "add", str(people_path)], check=True
    )
    logging.info("Committing updated file")
    message = "👥 Update LangChain people data"
    result = subprocess.run(["git", "commit", "-m", message], check=True)
    logging.info("Pushing branch")
    subprocess.run(["git", "push", "origin", branch_name, "-f"], check=True)
    logging.info("Creating PR")
    pr = repo.create_pull(title=message, body=message, base="main", head=branch_name)
    logging.info(f"Created PR: {pr.number}")
    logging.info("Finished")

--- dependency_range_tests/scripts/langchain/node/update_resolutions_latest.js ---
const fs = require("fs");

const communityPackageJsonPath = "package.json";
const currentPackageJson = JSON.parse(
  fs.readFileSync(communityPackageJsonPath)
);
currentPackageJson.pnpm = { overrides: {} };

const INTERNAL_PACKAGES = ["@langchain/eslint"];

if (
  currentPackageJson.peerDependencies?.["@langchain/core"] &&
  !currentPackageJson.peerDependencies["@langchain/core"].includes("rc")
) {
  currentPackageJson.peerDependencies = {
    ...currentPackageJson.peerDependencies,
  };
}

/**
 * Link workspace dependencies via file path
 */
const workspaceDependencies = [
  ...Object.entries(currentPackageJson.devDependencies),
  ...Object.entries(currentPackageJson.dependencies),
].filter(([, depVersion]) => depVersion.includes("workspace:"));

for (const [depName, depVersion] of workspaceDependencies) {
  /**
   * for the peer dependency @langchain/core, we want to make sure to install max version
   * defined above
   */
  if (depName === "@langchain/core") {
    delete currentPackageJson.devDependencies[depName];
    continue;
  }

  const libName = depName.split("/")[1];

  if (INTERNAL_PACKAGES.includes(depName)) {
    /**
     * reference the workspace dependency as a file path
     */
    currentPackageJson.devDependencies[depName] = `file:/internal/${libName}`;
    continue;
  }

  /**
   * reference the workspace dependency as a file path
   */
  currentPackageJson.devDependencies[
    depName
  ] = `file:/libs/langchain-${libName}`;
  /**
   * ensure that peer dependencies are also installed from the file path
   * e.g. @langchain/openai depends on @langchain/core which should be resolved from the file path
   */
  currentPackageJson.pnpm.overrides[
    depName
  ] = `file:/libs/langchain-${libName}`;
}

fs.writeFileSync(
  communityPackageJsonPath,
  JSON.stringify(currentPackageJson, null, 2)
);


--- dependency_range_tests/scripts/langchain/node/update_resolutions_lowest.js ---
const fs = require("fs");
const semver = require("semver");

const communityPackageJsonPath = "package.json";

const INTERNAL_PACKAGES = ["@langchain/eslint"];

const currentPackageJson = JSON.parse(
  fs.readFileSync(communityPackageJsonPath)
);
currentPackageJson.pnpm = { overrides: {} };

if (
  currentPackageJson.peerDependencies?.["@langchain/core"] &&
  !currentPackageJson.peerDependencies["@langchain/core"].includes("rc")
) {
  const minVersion = semver.minVersion(
    currentPackageJson.peerDependencies["@langchain/core"]
  ).version;
  currentPackageJson.peerDependencies = {
    ...currentPackageJson.peerDependencies,
    "@langchain/core": minVersion,
  };
}

/**
 * Convert workspace dev dependencies to install latest as they are only used for testing
 */
const workspaceDependencies = [
  ...Object.entries(currentPackageJson.devDependencies),
  ...Object.entries(currentPackageJson.dependencies),
].filter(([, depVersion]) => depVersion.includes("workspace:"));

for (const [depName, depVersion] of workspaceDependencies) {
  /**
   * for the peer dependency @langchain/core, we want to make sure to install min version
   * defined above
   */
  if (depName === "@langchain/core") {
    delete currentPackageJson.devDependencies[depName];
    continue;
  }

  const libName = depName.split("/")[1];

  if (INTERNAL_PACKAGES.includes(depName)) {
    /**
     * reference the workspace dependency as a file path
     */
    currentPackageJson.devDependencies[depName] = `file:/internal/${libName}`;
    continue;
  }

  /**
   * reference the workspace dependency as a file path
   */
  currentPackageJson.devDependencies[
    depName
  ] = `file:/libs/langchain-${libName}`;
  /**
   * ensure that peer dependencies are also installed from the file path
   * e.g. @langchain/openai depends on @langchain/core which should be resolved from the file path
   */
  currentPackageJson.pnpm.overrides[
    depName
  ] = `file:/libs/langchain-${libName}`;
}

fs.writeFileSync(
  communityPackageJsonPath,
  JSON.stringify(currentPackageJson, null, 2)
);


--- dependency_range_tests/scripts/with_standard_tests/node/update_workspace_dependencies.js ---
const fs = require("fs");

const standardTestsPackageJsonPath =
  "/app/monorepo/libs/langchain-standard-tests/package.json";

const currentPackageJson = JSON.parse(
  fs.readFileSync(standardTestsPackageJsonPath)
);

if (currentPackageJson.dependencies["@langchain/core"]) {
  currentPackageJson.dependencies = {
    ...currentPackageJson.dependencies,
    "@langchain/core": "latest",
  };
}

fs.writeFileSync(
  standardTestsPackageJsonPath,
  JSON.stringify(currentPackageJson, null, 2)
);


--- dependency_range_tests/scripts/with_standard_tests/anthropic/node/update_resolutions_latest.js ---
const fs = require("fs");

const communityPackageJsonPath =
  "/app/monorepo/libs/providers/langchain-anthropic/package.json";
const currentPackageJson = JSON.parse(
  fs.readFileSync(communityPackageJsonPath)
);

if (currentPackageJson.devDependencies["@langchain/core"]) {
  delete currentPackageJson.devDependencies["@langchain/core"];
  currentPackageJson.peerDependencies["@langchain/core"] = "*";
}

fs.writeFileSync(
  communityPackageJsonPath,
  JSON.stringify(currentPackageJson, null, 2)
);


--- dependency_range_tests/scripts/with_standard_tests/cohere/node/update_resolutions_latest.js ---
const fs = require("fs");

const communityPackageJsonPath =
  "/app/monorepo/libs/providers/langchain-cohere/package.json";
const currentPackageJson = JSON.parse(
  fs.readFileSync(communityPackageJsonPath)
);

if (currentPackageJson.devDependencies["@langchain/core"]) {
  delete currentPackageJson.devDependencies["@langchain/core"];
  currentPackageJson.peerDependencies["@langchain/core"] = "*";
}

fs.writeFileSync(
  communityPackageJsonPath,
  JSON.stringify(currentPackageJson, null, 2)
);


--- dependency_range_tests/scripts/with_standard_tests/community/node/update_resolutions_latest.js ---
const fs = require("fs");

const communityPackageJsonPath =
  "/app/monorepo/libs/langchain-community/package.json";
const currentPackageJson = JSON.parse(
  fs.readFileSync(communityPackageJsonPath)
);

if (currentPackageJson.devDependencies["@langchain/core"]) {
  delete currentPackageJson.devDependencies["@langchain/core"];
  currentPackageJson.peerDependencies["@langchain/core"] = "*";
}

if (currentPackageJson.dependencies["@langchain/openai"]) {
  delete currentPackageJson.dependencies["@langchain/openai"];
  currentPackageJson.dependencies["@langchain/openai"] = "latest";
}

fs.writeFileSync(
  communityPackageJsonPath,
  JSON.stringify(currentPackageJson, null, 2)
);


--- dependency_range_tests/scripts/with_standard_tests/google-vertexai/node/update_resolutions_latest.js ---
const fs = require("fs");

const communityPackageJsonPath =
  "/app/monorepo/libs/providers/langchain-google-vertexai/package.json";
const currentPackageJson = JSON.parse(
  fs.readFileSync(communityPackageJsonPath)
);

if (currentPackageJson.devDependencies["@langchain/core"]) {
  delete currentPackageJson.devDependencies["@langchain/core"];
  currentPackageJson.peerDependencies["@langchain/core"] = "*";
}

if (currentPackageJson.dependencies["@langchain/google-gauth"]) {
  delete currentPackageJson.dependencies["@langchain/google-gauth"];
}

fs.writeFileSync(
  communityPackageJsonPath,
  JSON.stringify(currentPackageJson, null, 2)
);


--- dependency_range_tests/scripts/with_standard_tests/openai/node/update_resolutions_latest.js ---
const fs = require("fs");

const communityPackageJsonPath =
  "/app/monorepo/libs/providers/langchain-openai/package.json";
const currentPackageJson = JSON.parse(
  fs.readFileSync(communityPackageJsonPath)
);

if (currentPackageJson.devDependencies["@langchain/core"]) {
  delete currentPackageJson.devDependencies["@langchain/core"];
  currentPackageJson.peerDependencies["@langchain/core"] = "*";
}

fs.writeFileSync(
  communityPackageJsonPath,
  JSON.stringify(currentPackageJson, null, 2)
);


--- dependency_range_tests/scripts/with_standard_tests/anthropic/node/update_resolutions_lowest.js ---
const fs = require("fs");
const semver = require("semver");

const communityPackageJsonPath =
  "/app/monorepo/libs/providers/langchain-anthropic/package.json";

const currentPackageJson = JSON.parse(
  fs.readFileSync(communityPackageJsonPath)
);

if (
  currentPackageJson.peerDependencies?.["@langchain/core"] &&
  !currentPackageJson.peerDependencies["@langchain/core"].includes("rc")
) {
  const minVersion = semver.minVersion(
    currentPackageJson.peerDependencies["@langchain/core"]
  ).version;
  currentPackageJson.peerDependencies = {
    ...currentPackageJson.peerDependencies,
    "@langchain/core": minVersion,
  };
}

if (currentPackageJson.devDependencies?.["@langchain/core"]) {
  delete currentPackageJson.devDependencies["@langchain/core"];
}

fs.writeFileSync(
  communityPackageJsonPath,
  JSON.stringify(currentPackageJson, null, 2)
);


--- dependency_range_tests/scripts/with_standard_tests/cohere/node/update_resolutions_lowest.js ---
const fs = require("fs");
const semver = require("semver");

const communityPackageJsonPath =
  "/app/monorepo/libs/providers/langchain-cohere/package.json";

const currentPackageJson = JSON.parse(
  fs.readFileSync(communityPackageJsonPath)
);

if (
  currentPackageJson.peerDependencies?.["@langchain/core"] &&
  !currentPackageJson.peerDependencies["@langchain/core"].includes("rc")
) {
  const minVersion = semver.minVersion(
    currentPackageJson.peerDependencies["@langchain/core"]
  ).version;
  currentPackageJson.peerDependencies = {
    ...currentPackageJson.peerDependencies,
    "@langchain/core": minVersion,
  };
}

if (currentPackageJson.devDependencies?.["@langchain/core"]) {
  delete currentPackageJson.devDependencies["@langchain/core"];
}

fs.writeFileSync(
  communityPackageJsonPath,
  JSON.stringify(currentPackageJson, null, 2)
);


--- environment_tests/README.md ---
# Environment Tests

This directory contains tests that verify LangChain packages work correctly in different JavaScript/TypeScript environments.

## Overview

The environment tests create isolated Docker containers that mimic real user environments, ensuring our packages work correctly with:

- Different module systems (ESM, CJS)
- Different bundlers (esbuild, Vite, Webpack via Next.js)
- Different runtimes (Node.js, Bun, Cloudflare Workers)
- TypeScript compilation

## Architecture

### Test Runner (`scripts/test-runner.ts`)

The TypeScript test runner:

1. Creates a sandbox environment at `/app` in the Docker container
2. Copies the test package files (excluding build artifacts)
3. Sets up workspace packages by:
   - Copying available local packages to `/app/libs/`
   - Replacing `workspace:*` dependencies with published versions for unavailable packages
4. Runs `pnpm install --prod` to install dependencies
5. Executes the build and test commands

### Docker Setup

Each test environment runs in its own Docker container with:

- The test package mounted at `/package`
- Workspace packages mounted at their respective paths
- The test runner script mounted at `/scripts`
- A clean `/app` directory as the test sandbox

### Test Environments

- **test-exports-esm**: Tests ESM imports and exports
- **test-exports-cjs**: Tests CommonJS require/exports
- **test-exports-esbuild**: Tests bundling with esbuild
- **test-exports-tsc**: Tests TypeScript compilation
- **test-exports-cf**: Tests Cloudflare Workers compatibility
- **test-exports-vercel**: Tests Next.js/Vercel compatibility
- **test-exports-vite**: Tests Vite bundling
- **test-exports-bun**: Tests Bun runtime compatibility

## Running Tests

```bash
docker compose -f environment_tests/docker-compose.yml run <environment>
# e.g. for test-exports-esbuild
docker compose -f environment_tests/docker-compose.yml run test-exports-esbuild
```

## Adding New Tests

1. Create a new directory `test-exports-{name}/`
2. Add a `package.json` with:
   - Dependencies using `workspace:*` for local packages
   - A `build` script (if needed)
   - A `test` script that runs your tests
3. Add test files in `src/`
4. Add the service to `docker-compose.yml`:
   ```yaml
   test-exports-{name}:
     image: node:20
     environment:
       PUPPETEER_SKIP_DOWNLOAD: "true"
       PLAYWRIGHT_SKIP_BROWSER_DOWNLOAD: "true"
     working_dir: /app
     volumes:
       - ../pnpm-workspace.yaml:/pnpm-workspace.yaml
       - ../turbo.json:/turbo.json
       - ../environment_tests/test-exports-{name}:/package
       - ../environment_tests/scripts:/scripts
       - ../langchain:/langchain
       - ../langchain-core:/langchain-core
       # ... other packages
     command: bash /scripts/docker-entrypoint.sh
   ```

## How It Works

1. Docker mounts the test package and workspace packages into the container
2. The entrypoint script installs `tsx` (or uses Bun directly) and runs the test runner
3. The test runner:
   - Creates a clean sandbox environment
   - Copies and prepares packages with proper dependency resolution
   - Runs the test package's build and test scripts
4. Tests verify that imports, exports, and functionality work as expected

This approach ensures we test against real package installations, not source code, providing confidence that our published packages work correctly in user environments.


--- environment_tests/test-exports-cf/README.md ---
# test-exports-cf

This package was generated with `wrangler init` with the purpose of testing compatibility with Cloudlfare Workers.


--- environment_tests/test-exports-vercel/README.md ---
This is a [Next.js](https://nextjs.org/) project bootstrapped with [`create-next-app`](https://github.com/vercel/next.js/tree/canary/packages/create-next-app).

## Getting Started

First, run the development server:

```bash
npm run dev
# or
yarn dev
# or
pnpm dev
```

Open [http://localhost:3000](http://localhost:3000) with your browser to see the result.

You can start editing the page by modifying `pages/index.tsx`. The page auto-updates as you edit the file.

[API routes](https://nextjs.org/docs/api-routes/introduction) can be accessed on [http://localhost:3000/api/hello](http://localhost:3000/api/hello). This endpoint can be edited in `pages/api/hello.ts`.

The `pages/api` directory is mapped to `/api/*`. Files in this directory are treated as [API routes](https://nextjs.org/docs/api-routes/introduction) instead of React pages.

This project uses [`next/font`](https://nextjs.org/docs/basic-features/font-optimization) to automatically optimize and load Inter, a custom Google Font.

## Learn More

To learn more about Next.js, take a look at the following resources:

- [Next.js Documentation](https://nextjs.org/docs) - learn about Next.js features and API.
- [Learn Next.js](https://nextjs.org/learn) - an interactive Next.js tutorial.

You can check out [the Next.js GitHub repository](https://github.com/vercel/next.js/) - your feedback and contributions are welcome!

## Deploy on Vercel

The easiest way to deploy your Next.js app is to use the [Vercel Platform](https://vercel.com/new?utm_medium=default-template&filter=next.js&utm_source=create-next-app&utm_campaign=create-next-app-readme) from the creators of Next.js.

Check out our [Next.js deployment documentation](https://nextjs.org/docs/deployment) for more details.


## Links discovered
- [Next.js](https://nextjs.org/)
- [`create-next-app`](https://github.com/vercel/next.js/tree/canary/packages/create-next-app)
- [http://localhost:3000](http://localhost:3000)
- [API routes](https://nextjs.org/docs/api-routes/introduction)
- [http://localhost:3000/api/hello](http://localhost:3000/api/hello)
- [`next/font`](https://nextjs.org/docs/basic-features/font-optimization)
- [Next.js Documentation](https://nextjs.org/docs)
- [Learn Next.js](https://nextjs.org/learn)
- [the Next.js GitHub repository](https://github.com/vercel/next.js/)
- [Vercel Platform](https://vercel.com/new?utm_medium=default-template&filter=next.js&utm_source=create-next-app&utm_campaign=create-next-app-readme)
- [Next.js deployment documentation](https://nextjs.org/docs/deployment)

--- environment_tests/test-exports-vite/index.html ---
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <link rel="icon" type="image/svg+xml" href="/vite.svg" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Vite + TS</title>
  </head>
  <body>
    <div id="app"></div>
    <script type="module" src="/src/main.ts"></script>
  </body>
</html>


--- environment_tests/test-exports-tsc/main.ts ---
import { ChatOpenAI } from "@langchain/openai";
import { ChatPromptTemplate } from "@langchain/core/prompts";

const model = new ChatOpenAI({
  openAIApiKey: "sk-XXXX",
});

const prompt = ChatPromptTemplate.fromMessages([
  ["system", "You are a helpful assistant"],
  ["placeholder", "{chat_history}"],
  ["human", "{input}"],
  ["placeholder", "{agent_scratchpad}"],
]);


--- environment_tests/test-exports-vercel/next.config.js ---
/** @type {import('next').NextConfig} */
const nextConfig = {
  webpack: (config, { isServer, nextRuntime, webpack }) => {
    // Handle node: protocol imports
    const nodeImports = [
      "node:async_hooks",
      "node:fs",
      "node:fs/promises",
      "node:path",
    ];
    nodeImports.forEach((nodeImport) => {
      let moduleName = nodeImport.replace("node:", "");
      // Special case for fs/promises - use fs instead since fs/promises isn't a valid webpack module
      if (moduleName === "fs/promises") {
        moduleName = "fs";
      }
      config.plugins.push(
        new webpack.NormalModuleReplacementPlugin(
          new RegExp(`^${nodeImport.replace(/[.*+?^${}()|[\]\\]/g, "\\$&")}$`),
          moduleName
        )
      );
    });

    // For client-side builds, provide fallbacks to disable Node.js modules
    if (!isServer) {
      config.resolve.fallback = {
        ...config.resolve.fallback,
        // Disable all Node.js modules that aren't available in browser/edge
        async_hooks: false,
        fs: false,
        path: false,
        typeorm: false,
      };
    }

    return config;
  },
};

module.exports = nextConfig;


--- environment_tests/scripts/test-runner.ts ---
#!/usr/bin/env node
import { spawn } from "node:child_process";
import fs from "node:fs/promises";
import path from "node:path";

// Type definition for workspace packages
interface WorkspacePackage {
  pkg: PackageJson;
  path: string;
}

interface PackageJson {
  name?: string;
  version?: string;
  private?: boolean;
  dependencies?: Record<string, string>;
  devDependencies?: Record<string, string>;
  scripts?: Record<string, string>;
}

// In Docker, packages are mounted at specific paths
const dockerPackages: WorkspacePackage[] = [
  { pkg: { name: "langchain" }, path: "/langchain" },
  { pkg: { name: "@langchain/core" }, path: "/langchain-core" },
  { pkg: { name: "@langchain/classic" }, path: "/langchain-classic" },
  { pkg: { name: "@langchain/openai" }, path: "/langchain-openai" },
  { pkg: { name: "@langchain/anthropic" }, path: "/langchain-anthropic" },
  { pkg: { name: "@langchain/community" }, path: "/langchain-community" },
  { pkg: { name: "@langchain/cohere" }, path: "/langchain-cohere" },
  { pkg: { name: "@langchain/ollama" }, path: "/langchain-ollama" },
  {
    pkg: { name: "@langchain/google-gauth" },
    path: "/langchain-google-gauth",
  },
  {
    pkg: { name: "@langchain/standard-tests" },
    path: "/langchain-standard-tests",
  },
  {
    pkg: { name: "@langchain/textsplitters" },
    path: "/langchain-textsplitters",
  },
  { pkg: { name: "@langchain/build" }, path: "/langchain-build" },
  { pkg: { name: "@langchain/eslint" }, path: "/langchain-eslint" },
];

class EnvironmentTestRunner {
  private testRoot: string;
  private packageName: string;
  private isBun: boolean;
  private availablePackages: Set<string> = new Set();

  constructor() {
    this.testRoot = "/app";
    this.packageName = path.basename("/package"); // Package is always mounted at /package
    this.isBun = process.env.BUN_ENV === "true";
  }

  private async execCommand(
    command: string,
    args: string[],
    cwd?: string
  ): Promise<void> {
    return new Promise((resolve, reject) => {
      const proc = spawn(command, args, {
        cwd,
        stdio: "inherit",
        shell: true,
      });

      proc.on("close", (code) => {
        if (code !== 0) {
          reject(new Error(`Command failed: ${command} ${args.join(" ")}`));
        } else {
          resolve();
        }
      });
    });
  }

  /**
   * Copy a directory from src to dest, excluding files that match the excludePatterns
   * @param src - The source directory
   * @param dest - The destination directory
   * @param excludePatterns - An array of file names to exclude from the copy
   */
  private async copyDirectory(
    src: string,
    dest: string,
    excludePatterns: string[] = []
  ): Promise<void> {
    await fs.mkdir(dest, { recursive: true });

    const entries = await fs.readdir(src, { withFileTypes: true });

    await Promise.all(
      entries.map(async (entry) => {
        const srcPath = path.join(src, entry.name);
        const destPath = path.join(dest, entry.name);

        // Skip excluded patterns
        if (excludePatterns.some((pattern) => entry.name === pattern)) {
          return;
        }

        if (entry.isDirectory()) {
          await this.copyDirectory(srcPath, destPath, excludePatterns);
        } else {
          await fs.copyFile(srcPath, destPath);
        }
      })
    );
  }

  /**
   * Update the package.json file at the given filePath to use workspace dependencies
   * @param filePath - The path to the package.json file to update
   * @param packages - The list of workspace packages
   */
  private async updatePackageJson(
    filePath: string,
    packages: WorkspacePackage[]
  ): Promise<void> {
    const content = await fs.readFile(filePath, "utf8");
    let packageJson: PackageJson = JSON.parse(content);

    // Get list of available package names
    const availablePackageNames = new Set(
      packages.map(({ pkg }) => pkg.name).filter(Boolean)
    );

    // Update both dependencies and devDependencies
    for (const depType of ["dependencies", "devDependencies"] as const) {
      if (!packageJson[depType]) {
        continue;
      }

      // Iterate through all dependencies
      for (const [depName, depVersion] of Object.entries(
        packageJson[depType]!
      )) {
        // If this dependency is one of our local packages, keep it as workspace dependency
        if (availablePackageNames.has(depName)) {
          packageJson[depType]![depName] = "workspace:*";
        } else if (depVersion === "workspace:*") {
          // Only replace workspace:* with npm version if package is NOT available locally
          if (depName === "@langchain/core") {
            packageJson[depType]![depName] = ">=0.3.58 <0.4.0";
          } else if (depName === "langchain") {
            packageJson[depType]![depName] = "^0.3.30";
          } else {
            // For other workspace dependencies not available locally, use latest
            packageJson[depType]![depName] = "*";
          }
        }
        // Otherwise, keep the existing version (non-workspace dependencies)
      }
    }

    await fs.writeFile(filePath, JSON.stringify(packageJson, null, 2));
  }

  /**
   * Copy the test package files to the test root
   */
  private async prepareSandbox(): Promise<void> {
    console.log("🔧 Preparing sandbox environment...");

    // Copy test package files
    const excludePatterns = [
      "node_modules",
      "dist",
      "dist-cjs",
      "dist-esm",
      "build",
      ".next",
      ".turbo",
    ];

    await this.copyDirectory("/package", this.testRoot, excludePatterns);

    // Copy .eslintrc.json if it exists
    try {
      await fs.copyFile(
        "/package/.eslintrc.json",
        path.join(this.testRoot, ".eslintrc.json")
      );
    } catch {
      // File doesn't exist, that's okay
    }

    // Enable corepack for pnpm
    if (!this.isBun) {
      await this.execCommand("corepack", ["enable"]);
    }
  }

  /**
   * Setup the workspace packages
   */
  private async setupPackages(): Promise<void> {
    console.log("📦 Setting up workspace packages...");

    // Filter to only include packages that exist
    const packages: WorkspacePackage[] = [];
    for (const dockerPkg of dockerPackages) {
      await fs.access(dockerPkg.path);
      const pkgJsonPath = path.join(dockerPkg.path, "package.json");
      const pkgJson = JSON.parse(
        await fs.readFile(pkgJsonPath, "utf-8")
      ) as PackageJson;
      packages.push({ pkg: pkgJson, path: dockerPkg.path });

      // Track available packages for verification
      if (pkgJson.name) {
        this.availablePackages.add(pkgJson.name);
      }
    }

    const libsDir = path.join(this.testRoot, "libs");
    await fs.mkdir(libsDir, { recursive: true });

    // Copy available packages
    await Promise.all(
      packages.map(async ({ pkg, path: pkgPath }) => {
        if (!pkg.name || !pkgPath) {
          return;
        }

        let destDirName: string;
        if (pkg.name === "langchain") {
          destDirName = "langchain";
        } else {
          destDirName = pkg.name.replace("@langchain/", "langchain-");
        }
        const destDir = path.join(libsDir, destDirName);
        await fs.mkdir(destDir, { recursive: true });

        console.log(`  ✓ Copying ${pkg.name} from ${pkgPath}`);
        await this.copyDirectory(pkgPath, destDir, ["node_modules"]);
      })
    );

    // Update package.json files
    console.log("📝 Updating package.json files...");

    // Update root package.json
    await this.updatePackageJson(
      path.join(this.testRoot, "package.json"),
      packages
    );

    // Update package.json files in libs
    const libsDirs = await fs.readdir(libsDir);
    await Promise.all(
      libsDirs.map(async (dir) => {
        const pkgJsonPath = path.join(libsDir, dir, "package.json");
        await this.updatePackageJson(pkgJsonPath, packages);
      })
    );
  }

  /**
   * Install dependencies for the test
   */
  private async installDependencies(): Promise<void> {
    console.log("📥 Installing dependencies...");

    if (this.isBun) {
      // Read the existing package.json to add workspaces field
      const packageJsonPath = path.join(this.testRoot, "package.json");
      const packageJson = JSON.parse(
        await fs.readFile(packageJsonPath, "utf-8")
      );

      // Add workspaces configuration for Bun
      packageJson.workspaces = [".", "libs/*"];

      // Write back the updated package.json
      await fs.writeFile(packageJsonPath, JSON.stringify(packageJson, null, 2));

      await this.execCommand(
        "bun",
        ["install", "--ignore-scripts"],
        this.testRoot
      );
    } else {
      // Setup pnpm workspace
      const workspaceYaml = `packages:
  - "."
  - "libs/*"
`;
      await fs.writeFile(
        path.join(this.testRoot, "pnpm-workspace.yaml"),
        workspaceYaml
      );

      await this.execCommand("pnpm", ["install", "--prod"], this.testRoot);
    }
  }

  /**
   * Verify that all local langchain packages are using workspace dependencies
   * and not production packages from npm
   */
  private async verifyLocalPackages(): Promise<void> {
    if (this.isBun) {
      console.log("🔍 Skipping local package verification for Bun...");
      return;
    }

    console.log("🔍 Verifying local package usage...");

    const nodeModulesDir = path.join(this.testRoot, "node_modules");
    const pnpmDir = path.join(nodeModulesDir, ".pnpm");

    const errors: string[] = [];

    // Only verify packages that are actually available in this container
    for (const pkgName of this.availablePackages) {
      const pkgPath = pkgName.startsWith("@")
        ? path.join(nodeModulesDir, ...pkgName.split("/"))
        : path.join(nodeModulesDir, pkgName);

      try {
        // Check if the package exists in node_modules
        const stats = await fs.lstat(pkgPath);

        if (stats.isSymbolicLink()) {
          // Good - it's a symlink (workspace dependency)
          const linkTarget = await fs.readlink(pkgPath);
          console.log(`  ✓ ${pkgName} → ${linkTarget} (workspace)`);
        } else {
          // Bad - it's a real directory (npm package)
          errors.push(`${pkgName} is not a workspace dependency!`);

          // Try to get version info to help debug
          try {
            const pkgJsonPath = path.join(pkgPath, "package.json");
            const pkgJson = JSON.parse(await fs.readFile(pkgJsonPath, "utf-8"));
            errors.push(`  Found version ${pkgJson.version} from npm registry`);
          } catch {
            // Ignore errors reading package.json
          }
        }
      } catch (error: any) {
        // Package doesn't exist in this test environment, which is OK
        if (error.code !== "ENOENT") {
          errors.push(`Error checking ${pkgName}: ${error.message}`);
        }
      }
    }

    // Also check pnpm store for any downloaded versions of our available packages
    const pnpmEntries = await fs.readdir(pnpmDir);

    // Only check for the packages that we're explicitly managing as workspace dependencies
    const problematicDownloads = pnpmEntries.filter((entry) => {
      // Check if this pnpm entry is for one of our available packages
      for (const pkgName of this.availablePackages) {
        const pnpmPackageName = pkgName.replace("@", "").replace("/", "+");
        // Look for exact package matches (not sub-dependencies like @langchain/weaviate)
        if (
          entry.startsWith(`${pnpmPackageName}@`) &&
          !entry.includes("workspace")
        ) {
          return true;
        }
      }
      return false;
    });

    if (problematicDownloads.length > 0) {
      errors.push(
        "Found production versions of workspace packages in pnpm store:"
      );
      problematicDownloads.forEach((entry) => {
        errors.push(`  - ${entry}`);
      });
    }

    if (errors.length > 0) {
      console.error("❌ Verification failed!");
      errors.forEach((error) => console.error(`   ${error}`));
      throw new Error(
        "Local packages verification failed. Production packages were installed instead of workspace packages."
      );
    }

    console.log("  ✅ All local packages verified!");
  }

  /**
   * Run the build for the test
   */
  private async runBuild(): Promise<void> {
    // Skip build for Bun since it can run TypeScript directly
    if (this.isBun) {
      console.log("🔨 Skipping build for Bun (runs TypeScript directly)...");
      return;
    }

    console.log("🔨 Running build...");
    await this.execCommand("pnpm", ["run", "build"], this.testRoot);
  }

  /**
   * Run the tests for the test
   */
  private async runTests(): Promise<void> {
    console.log("🧪 Running tests...");
    const cmd = this.isBun ? "bun" : "pnpm";
    await this.execCommand(cmd, ["run", "test"], this.testRoot);
  }

  /**
   * Run the test
   */
  public async run(): Promise<void> {
    try {
      console.log(`🚀 Starting environment test for ${this.packageName}`);

      const runtimeIcon = this.isBun ? "🐹" : "🐢";
      console.log(`${runtimeIcon} Runtime: ${this.isBun ? "Bun" : "Node.js"}`);

      await this.prepareSandbox();
      await this.setupPackages();
      await this.installDependencies();
      await this.verifyLocalPackages();
      await this.runBuild();
      await this.runTests();

      console.log("✅ All tests passed!");
    } catch (error) {
      console.error("❌ Test failed:", error);
      process.exit(1);
    }
  }
}

// Run the test
const runner = new EnvironmentTestRunner();
runner.run().catch((error) => {
  console.error("Fatal error:", error);
  process.exit(1);
});


--- environment_tests/test-exports-vite/vite.config.js ---
export default {
  build: {
    rollupOptions: {
      external: [/^node:/, "typeorm", "reflect-metadata"],
    },
  },
};


--- environment_tests/test-exports-vite/src/chain.ts ---
// Import a few things we'll use to test the exports
import { LLMChain } from "@langchain/classic/chains";
import { ChatOpenAI } from "@langchain/openai";
import {
  ChatPromptTemplate,
  HumanMessagePromptTemplate,
} from "@langchain/core/prompts";
import { CallbackManager } from "@langchain/core/callbacks/manager";

export function setupChain(element: HTMLButtonElement) {
  const runChain = async () => {
    const llm = new ChatOpenAI({
      // Don't do this in your app, it would leak your API key
      openAIApiKey: import.meta.env.VITE_OPENAI_API_KEY,
      streaming: true,
      callbackManager: CallbackManager.fromHandlers({
        handleLLMNewToken: async (token) =>
          console.log("handleLLMNewToken", token),
      }),
    });

    // Test count tokens
    const n = await llm.getNumTokens("Hello");
    console.log("getNumTokens", n);

    // Test a chain + prompt + model
    const chain = new LLMChain({
      llm,
      prompt: ChatPromptTemplate.fromMessages([
        HumanMessagePromptTemplate.fromTemplate("{input}"),
      ]),
    });
    const res = await chain.run("hello");

    console.log("runChain", res);
  };
  element.addEventListener("click", runChain);
}


--- environment_tests/test-exports-bun/scripts/combine-dependencies.js ---
import * as fs from "fs";

const langchainPackageJson = JSON.parse(fs.readFileSync("../langchain/package.json"));
const testPackageJson = JSON.parse(fs.readFileSync("./package.json"));

testPackageJson.dependencies = { ...testPackageJson.dependencies, ...langchainPackageJson.dependencies };

fs.writeFileSync("./package.json", JSON.stringify(testPackageJson, null, 2));

--- internal/build/README.md ---
# LangChain Build System

A modern build system for LangChain JavaScript/TypeScript packages that provides fast compilation, type checking, automated secret management, and advanced code generation for monorepo workspaces.

## Overview

This build system is designed to handle the complex requirements of LangChain's multi-package monorepo. It automatically discovers packages in the workspace, compiles them with optimal settings, and includes specialized tooling for LangChain's security patterns and dynamic loading capabilities.

### Key Features

- 🚀 **Fast Compilation**: Uses [tsdown](https://github.com/privatenumber/tsdown) for high-performance TypeScript bundling with Rolldown
- 📦 **Monorepo Aware**: Automatically discovers and builds all non-private packages in pnpm workspaces
- 🔍 **Secret Management**: Built-in scanning and validation of LangChain's `lc_secrets` patterns
- 📝 **Type Generation**: Generates both ESM and CommonJS outputs with TypeScript declarations
- ✅ **Quality Checks**: Integrated type checking with [arethetypeswrong](https://github.com/arethetypeswrong/arethetypeswrong), [publint](https://github.com/bluwy/publint), and unused dependency detection
- 🗺️ **Import Maps**: Automatic generation of import maps for convenient bulk imports
- 📋 **Import Constants**: Dynamic detection and export of optional dependency entrypoints
- 🎯 **Selective Building**: Build all packages or target specific ones with flexible filtering
- 👀 **Watch Mode**: Real-time compilation with file watching capabilities
- 🛠️ **Rich CLI**: Full-featured command-line interface with comprehensive options

## Architecture

The build system consists of:

```
infra/build/
├── index.ts              # Main build orchestrator
├── cli.ts                # Command-line interface
├── types.ts              # TypeScript type definitions
├── utils.ts              # Utility functions
├── plugins/
│   ├── README.md         # Plugin documentation
│   ├── lc-secrets.ts     # LangChain secrets scanning plugin
│   ├── import-map.ts     # Import map generation plugin
│   └── import-constants.ts # Import constants generation plugin
├── package.json          # Build system dependencies
└── README.md             # This documentation
```

### Core Technologies

- **[tsdown](https://github.com/privatenumber/tsdown)** - Fast TypeScript bundler with Rolldown
- **[TypeScript Compiler API](https://github.com/microsoft/TypeScript)** - For source code analysis and type checking
- **[unplugin-unused](https://github.com/unplugin/unplugin-unused)** - For unused dependency detection
- **Node.js built-ins** - File system operations and process management

## Usage

### CLI Commands

```bash
# Get help
pnpm build:new --help

# Build all packages in the workspace
pnpm build:new

# Build with watch mode for development
pnpm build:new --watch

# Build specific packages
pnpm build:new @langchain/core
pnpm build:new @langchain/core langchain @langchain/openai

# Exclude packages from build
pnpm build:new --exclude @langchain/community
pnpm build:new -e @langchain/aws -e @langchain/openai

# Skip various build steps
pnpm build:new --no-emit          # Skip type declarations
pnpm build:new --skip-unused      # Skip unused dependency check
pnpm build:new --skip-clean       # Skip cleaning build directory
pnpm build:new --skip-sourcemap   # Skip sourcemap generation
```

## Development

### Adding New Packages

1. Create package directory under appropriate workspace
2. Add `package.json` with proper exports field
3. Add `tsconfig.json` extending workspace config
4. Run build - it will be automatically discovered

### package.json Requirements

Each package must have a properly configured `exports` field that includes an `input` property to tell the build system which source file to compile for each entrypoint:

```json
{
  "name": "@langchain/example",
  "exports": {
    ".": {
      "input": "./src/index.ts", // ← Required: Source file for this entrypoint
      "import": "./dist/index.js",
      "require": "./dist/index.cjs",
      "types": "./dist/index.d.ts"
    },
    "./tools": {
      "input": "./src/tools/index.ts", // ← Required: Source file for tools entrypoint
      "import": "./dist/tools/index.js",
      "require": "./dist/tools/index.cjs",
      "types": "./dist/tools/index.d.ts"
    }
  }
}
```

**Important**: The `input` property is required for the build system to understand which TypeScript source file should be compiled for each export. Without this property, the entrypoint will be ignored during build.


## Links discovered
- [tsdown](https://github.com/privatenumber/tsdown)
- [arethetypeswrong](https://github.com/arethetypeswrong/arethetypeswrong)
- [publint](https://github.com/bluwy/publint)
- [TypeScript Compiler API](https://github.com/microsoft/TypeScript)
- [unplugin-unused](https://github.com/unplugin/unplugin-unused)

--- internal/eslint/README.md ---
# @langchain/eslint

Shared ESLint configuration for LangChain.js projects.

## Installation

```bash
pnpm add -D @langchain/eslint
```

## Usage

Create an `eslint.config.ts` file in your project root:

```ts
import { langchainConfig } from "@langchain/eslint";

export default langchainConfig;
```

Or for specific presets:

```ts
import { base, node, browser } from "@langchain/eslint";

export default [...base, ...node];
```

## Available Configurations

- `langchainConfig` - Full LangChain configuration (includes all rules)
- `base` - Base TypeScript + Prettier configuration
- `node` - Node.js specific rules
- `browser` - Browser specific rules

## Customization

You can override any rules by adding your own configuration after importing:

```ts
import { langchainConfig, type Linter } from "@langchain/eslint";

const config: Linter.Config[] = [
  ...langchainConfig,
  {
    files: ["**/*.ts"],
    rules: {
      // Your custom rules
      "@typescript-eslint/no-explicit-any": "error",
    },
  },
];

export default config;
```


--- internal/model-profiles/README.md ---
# Model Profiles Generator

A CLI tool for automatically generating TypeScript model profile files from the [models.dev](https://models.dev) API. This tool fetches model capabilities and constraints, applies provider-level and model-specific overrides, and generates type-safe TypeScript files using the TypeScript AST API.

## Overview

The model-profiles generator simplifies the process of maintaining model capability profiles across LangChain provider packages.

### Key Features

- 🔄 **Automatic Data Fetching**: Fetches latest model data from the models.dev API
- 🎯 **Provider-Level Overrides**: Apply overrides to all models for a provider
- 🔧 **Model-Specific Overrides**: Fine-tune individual model profiles
- 📝 **TypeScript AST Generation**: Uses TypeScript compiler API for type-safe code generation
- 🎨 **Prettier Integration**: Automatically formats generated code using your project's Prettier config
- 📦 **Monorepo Friendly**: Works seamlessly with pnpm workspaces and `--filter` commands
- ✅ **Type Safety**: Generates code that matches the `ModelProfile` interface from `@langchain/core`

## Architecture

The model-profiles generator consists of:

```text
internal/model-profiles/
├── src/
│   ├── cli.ts              # Command-line interface
│   ├── config.ts            # TOML config parsing and override logic
│   ├── generator.ts         # TypeScript code generation and API integration
│   ├── api-schema.ts        # TypeScript types for models.dev API
│   └── tests/               # Test suite
│       ├── config.test.ts
│       └── generator.test.ts
├── package.json             # Tool dependencies
├── vitest.config.ts         # Test configuration
└── README.md                # This documentation
```

## Usage

### Basic Usage

Create a TOML configuration file (e.g., `profiles.toml`) in a provider package:

```toml
provider = "openai"
output = "src/chat_models/profiles.ts"
```

Then run the generator:

```bash
# From the model-profiles package
pnpm --filter @langchain/model-profiles make --config profiles.toml

# Or if running from within a provider package
pnpm --filter @langchain/model-profiles make --config profiles.toml
```

### Configuration File Format

The TOML configuration file supports the following structure:

```toml
# Required: Provider ID from models.dev
provider = "openai"

# Required: Output path for generated TypeScript file (relative to config file)
output = "src/chat_models/profiles.ts"

# Optional: Provider-level overrides (applied to all models)
[overrides]
maxInputTokens = 100000
toolCalling = true
structuredOutput = true
imageUrlInputs = true

# Optional: Model-specific overrides (override provider-level settings)
[overrides."gpt-4"]
maxOutputTokens = 8192

[overrides."gpt-3.5-turbo"]
maxInputTokens = 16385
imageUrlInputs = false
```


## Links discovered
- [models.dev](https://models.dev)

--- internal/test-helpers/README.md ---
# Test Helpers

This package exports a number of utilities that are helpful in testing.

## `env`

This module provides utilities for managing environment variables (`process.env`) within your tests, particularly for Jest. It ensures that environment variable changes are isolated to individual tests and automatically cleaned up afterward.

### Usage

Import the `env` helper from `@langchain/test-helpers/env`.

```typescript
import { env } from "@langchain/test-helpers/env";
```

#### `env.useVariables(variables, options)`

This is the primary function for setting multiple environment variables for your tests. You call it at the top level of your test file, and it uses `beforeEach` and `afterEach` hooks to manage the environment.

**Arguments:**

- `variables`: An object where keys are environment variable names and values are what you want to set them to.
  - `string`: Sets the variable to that string value.
  - `undefined`: Deletes the environment variable.
  - `env.passthrough`: A special symbol that tells the helper to keep the original value of the environment variable, if it exists.
- `options`: An optional configuration object.
  - `replace` (boolean, default: `false`): If `true`, all existing environment variables on `process.env` will be cleared before applying the ones you've specified. If `false`, your variables will be merged with the existing environment.

**Example: Setting specific environment variables**

```ts
// Set specific env vars while preserving others
env.useVariables({
  API_KEY: "test-key",
  DEBUG: "true",
});
```

**Example: Replacing the environment**

This is useful for creating isolated test environments. You can use `env.passthrough` to preserve specific variables from the original environment.

```ts
// Replace all env vars with just these
env.useVariables(
  {
    API_KEY: "test-key",
    // Preserve the existing value of PRESERVE_THIS
    PRESERVE_THIS: env.passthrough,
  },
  { replace: true }
);
```

#### `env.useVariable(key, value, options)`

A convenience wrapper around `useVariables` for when you only need to set a single environment variable.

**Example:**

```ts
// Set a single env var while preserving others
env.useVariable("API_KEY", "test-key");

// Replace all env vars with just this one
env.useVariable("API_KEY", "test-key", { replace: true });
```

#### `env.applyEnv(envObject)`

This is a lower-level function used internally by the helpers, but it's exported for direct use. It takes an object and applies its key-value pairs to `process.env`. If a value is `undefined`, the corresponding key is deleted from `process.env`. This function applies changes immediately and does **not** handle automatic cleanup.

```ts
env.applyEnv({
  KEY_A: "VALUE_A",
  KEY_B: undefined, // This will delete process.env.KEY_B
});
```


--- internal/build/plugins/README.md ---
# Build System Plugins

The LangChain build system uses a plugin architecture to extend functionality. This document covers all available plugins and how to create custom ones.

## Available Plugins

### 🔐 lc-secrets Plugin

Scans TypeScript files for `lc_secrets` patterns and generates TypeScript interfaces for environment variables.

#### Purpose

The `lc_secrets` pattern is a LangChain convention where classes declare which environment variables they need for sensitive configuration (API keys, tokens, etc.). This plugin automatically:

1. Scans all TypeScript files for classes with `lc_secrets` getters
2. Validates environment variable naming conventions
3. Generates TypeScript interfaces for type safety
4. Reports validation errors

#### Usage

```typescript
import { lcSecretsPlugin } from "./plugins/lc-secrets.js";

await build({
  plugins: [
    lcSecretsPlugin({
      enabled: true,
      strict: true,
      packagePath: "/path/to/package",
      outputPath: "load/import_type.ts",
      excludePatterns: [".test.ts"],
    }),
  ],
});
```

#### What it detects

```typescript
class OpenAIProvider {
  get lc_secrets(): { [key: string]: string } {
    return {
      apiKey: "OPENAI_API_KEY", // ✅ Valid - maps to process.env.OPENAI_API_KEY
      organization: "OPENAI_ORG_ID", // ✅ Valid - maps to process.env.OPENAI_ORG_ID
      "api-key": "INVALID_KEY", // ❌ Must be uppercase
      "SPACED KEY": "ANOTHER_KEY", // ❌ No whitespace allowed
    };
  }
}
```

#### Generated Output

Creates `src/load/import_type.ts`:

```typescript
// Auto-generated by lc-secrets plugin. Do not edit manually.

export interface OptionalImportMap {}

export interface SecretMap {
  OPENAI_API_KEY?: string;
  OPENAI_ORG_ID?: string;
}
```

#### Configuration Options

| Option            | Type     | Default                                          | Description                         |
| ----------------- | -------- | ------------------------------------------------ | ----------------------------------- |
| `enabled`         | boolean  | `true`                                           | Enable/disable secret scanning      |
| `strict`          | boolean  | `true`                                           | Throw errors on validation failures |
| `packagePath`     | string   | `process.cwd()`                                  | Package root directory              |
| `outputPath`      | string   | `"load/import_type.ts"`                          | Output file path relative to src/   |
| `excludePatterns` | string[] | `[".test.ts", "test.ts", ".spec.ts", "spec.ts"]` | File patterns to exclude            |

---

### 📋 import-constants Plugin

Generates arrays of entrypoint names that require optional dependencies for dynamic loading.

#### Purpose

LangChain packages often have entrypoints that depend on optional packages that users may not have installed. This plugin generates constants that help the runtime determine which modules can be safely imported.

#### Usage

```typescript
import { importConstantsPlugin } from "./plugins/import-constants.js";

await build({
  plugins: [
    importConstantsPlugin({
      enabled: true,
      packagePath: "/path/to/package",
      packageInfo: packageJson,
      optionalEntrypoints: ["anthropic", "openai"],
    }),
  ],
});
```

#### Generated Output

Creates `src/load/import_constants.ts`:

```typescript
// Auto-generated by import-constants plugin. Do not edit manually.

export const optionalImportEntrypoints: string[] = [
  "langchain_community/anthropic",
  "langchain_community/openai",
];
```

#### Configuration Options

| Option                | Type        | Description                                              |
| --------------------- | ----------- | -------------------------------------------------------- |
| `enabled`             | boolean     | Enable/disable import constants generation               |
| `packagePath`         | string      | Package root directory                                   |
| `packageInfo`         | PackageJson | Package.json content for reading entrypoints             |
| `outputPath`          | string      | Output file path (default: `"load/import_constants.ts"`) |
| `optionalEntrypoints` | string[]    | List of entrypoints requiring optional dependencies      |
| `deprecatedNodeOnly`  | string[]    | Entrypoints to exclude (deprecated)                      |

---

### 🗺️ import-map Plugin

Creates consolidated re-export files for all package entrypoints with namespace aliases.

#### Purpose

Import maps provide a single file that re-exports all entrypoints from a package, making it easy to access all functionality through one import. This is useful for:

- Bulk imports in testing
- Dynamic loading scenarios
- Package introspection
- Backward compatibility

#### Usage

```typescript
import { importMapPlugin } from "./plugins/import-map.js";

await build({
  plugins: [
    importMapPlugin({
      enabled: true,
      packagePath: "/path/to/package",
      packageInfo: packageJson,
      deprecatedNodeOnly: ["legacy-tool"],
      extraImportMapEntries: [
        {
          modules: ["CustomModule"],
          alias: ["custom"],
          path: "external-package",
        },
      ],
    }),
  ],
});
```

#### Generated Output

Creates `src/load/import_map.ts`:

```typescript
// Auto-generated by import-map plugin. Do not edit manually.

export * as index from "../index.js";
export * as tools__calculator from "../tools/calculator.js";
export * as providers__openai from "../providers/openai.js";

// Extra imports (if configured)
import { CustomModule } from "external-package";

const custom = {
  CustomModule,
};
export { custom };
```

#### How It Works

1. **Discovers** entrypoints from `package.json` exports field (using `input` property)
2. **Filters** based on configuration (deprecated, optional deps, etc.)
3. **Transforms** paths from `src/` to compiled output in `dist/`
4. **Namespaces** exports using double underscores: `tools/calculator` → `tools__calculator`
5. **Handles** complex extra import scenarios with custom aliases

#### Configuration Options

| Option                        | Type                  | Description                                        |
| ----------------------------- | --------------------- | -------------------------------------------------- |
| `enabled`                     | boolean               | Enable/disable import map generation               |
| `packagePath`                 | string                | Package root directory                             |
| `packageInfo`                 | PackageJson           | Package.json content for reading entrypoints       |
| `outputPath`                  | string                | Output file path (default: `"load/import_map.ts"`) |
| `deprecatedNodeOnly`          | string[]              | Entrypoints to exclude (deprecated, Node.js only)  |
| `requiresOptionalDependency`  | string[]              | Entrypoints to exclude (requires optional deps)    |
| `deprecatedOmitFromImportMap` | string[]              | Entrypoints to exclude (deprecated from map)       |
| `extraImportMapEntries`       | ExtraImportMapEntry[] | Complex import/export patterns                     |

#### Extra Import Map Entries

For complex scenarios where you need to import from external packages and re-export with custom aliases:

```typescript
interface ExtraImportMapEntry {
  modules: Array<string>; // What to import: ['ModuleA', 'ModuleB']
  alias: Array<string>; // Alias parts: ['custom', 'tools']
  path: string; // Where to import from: 'external-package'
}

// Results in:
// import { ModuleA, ModuleB } from "external-package";
// const custom__tools = { ModuleA, ModuleB };
// export { custom__tools };
```


--- internal/build/cli.ts ---
#!/usr/bin/env node
import { parseArgs } from "node:util";
import { compilePackages } from "./index.js";
import type { CompilePackageOptions } from "./types.js";

/**
 * CLI configuration with descriptions for auto-generated help
 */
const cliName = "pnpm build:new";
const cliConfig = {
  name: cliName,
  description:
    "CLI program for compiling or watching packages in the repository",
  options: {
    watch: {
      type: "boolean" as const,
      short: "w",
      default: false,
      description: "Watch for changes and recompile automatically",
    },
    tsconfigPath: {
      type: "string" as const,
      default: "tsconfig.json",
      description: "Path to tsconfig.json file (default: tsconfig.json)",
    },
    help: {
      type: "boolean" as const,
      short: "h",
      default: false,
      description: "Show this help message",
    },
    exclude: {
      type: "string" as const,
      short: "e",
      multiple: true,
      description:
        "Exclude packages from the build (can be specified multiple times)",
    },
    noEmit: {
      type: "boolean" as const,
      default: false,
      description: "Skip emitting type declarations",
    },
    skipUnused: {
      type: "boolean" as const,
      default: false,
      description: "Skip unused dependency check on packages",
    },
    skipClean: {
      type: "boolean" as const,
      default: false,
      description: "Skip cleaning the build directory",
    },
    skipSourcemap: {
      type: "boolean" as const,
      default: false,
      description:
        "Skip generating sourcemaps (e.g. `.map` and `.d.ts.map` files)",
    },
  },
  /**
   * only supported in later node versions
   */
  positionals: {
    name: "package-query...",
    description:
      " Optional queries to filter packages (e.g., package name patterns)\n                   Multiple queries can be provided and will be processed together",
  },
  examples: [
    { command: cliName, description: "Compile all packages" },
    {
      command: `${cliName} --watch`,
      description: "Watch and recompile all packages",
    },
    {
      command: `${cliName} langchain`,
      description: 'Compile packages matching "langchain"',
    },
    {
      command: `${cliName} langchain core`,
      description: 'Compile packages matching "langchain" or "core"',
    },
    {
      command: `${cliName} --no-emit`,
      description: "Compile all packages without emitting type declarations",
    },
    {
      command: `${cliName} --watch core openai`,
      description: 'Watch packages matching "core" or "openai"',
    },
    {
      command: `${cliName} --exclude langchain-community`,
      description: "Compile all packages except langchain-community",
    },
    {
      command: `${cliName} --exclude langchain-community --exclude langchain-aws`,
      description:
        "Compile all packages except langchain-community and langchain-aws",
    },
    {
      command: `${cliName} -e community -e aws langchain`,
      description:
        'Compile packages matching "langchain" but exclude those matching "community" or "aws"',
    },
  ],
};

/**
 * Generate help text from CLI configuration
 */
function generateHelp(config: typeof cliConfig): string {
  const lines: string[] = [];

  lines.push(`Usage: ${config.name} [options] [${config.positionals.name}]`);
  lines.push("");
  lines.push("Options:");

  Object.entries(config.options).forEach(([key, option]) => {
    const shortFlag =
      "short" in option && option.short ? `-${option.short}, ` : "    ";
    const longFlag = `--${key}`;
    const padding = " ".repeat(Math.max(0, 15 - longFlag.length));
    lines.push(
      `  ${shortFlag}${longFlag}${padding}${option.description}${
        "default" in option && option.default
          ? ` (default: ${option.default})`
          : ""
      }`
    );
  });

  lines.push("");
  lines.push("Arguments:");
  lines.push(
    `  ${config.positionals.name.padEnd(15)}${config.positionals.description}`
  );

  lines.push("");
  lines.push("Examples:");
  config.examples.forEach((example) => {
    lines.push(`  # ${example.description}`);
    lines.push(`  ${example.command}`);
    lines.push("");
  });

  lines.push(
    `Copyright © ${new Date().getFullYear()} LangChain, Inc. All rights reserved.`
  );
  return lines.join("\n");
}

/**
 * CLI program for compiling or watching packages in the repository
 */
async function main() {
  const { values, positionals } = parseArgs({
    args: process.argv.slice(2),
    options: cliConfig.options,
    allowPositionals: true,
  });

  if (values.help) {
    console.log(generateHelp(cliConfig));
    process.exit(0);
  }

  const packageQuery = positionals;
  const tsconfigPath = values.tsconfigPath;
  const watch = values.watch;
  const noEmit = values.noEmit;
  const skipUnused = values.skipUnused;
  const skipClean = values.skipClean;
  const skipSourcemap = values.skipSourcemap;
  const exclude = Array.isArray(values.exclude)
    ? values.exclude
    : values.exclude
      ? [values.exclude]
      : [];

  const opts: CompilePackageOptions = {
    packageQuery,
    tsconfigPath,
    watch,
    exclude,
    noEmit,
    skipUnused,
    skipClean,
    skipSourcemap,
  };

  try {
    const packageLabel =
      packageQuery.length > 0
        ? `packages matching: ${packageQuery.map((q) => `"${q}"`).join(", ")}`
        : "all packages";
    console.log(`${watch ? "Watching" : "Compiling"} ${packageLabel}...`);
    if (exclude.length > 0) {
      console.log(`Excluding: ${exclude.map((q) => `"${q}"`).join(", ")}`);
    }
    await compilePackages(opts);

    if (!watch) {
      console.log("✅ Compilation completed successfully!");
    }
  } catch (error) {
    console.error("❌ Compilation failed:", error);
    process.exit(1);
  }
}

// Handle uncaught errors
process.on("uncaughtException", (error) => {
  console.error("❌ Uncaught exception:", error);
  process.exit(1);
});

process.on("unhandledRejection", (reason) => {
  console.error("❌ Unhandled rejection:", reason);
  process.exit(1);
});

// Handle graceful shutdown for watch mode
process.on("SIGINT", () => {
  console.log("\n👋 Gracefully shutting down...");
  process.exit(0);
});

process.on("SIGTERM", () => {
  console.log("\n👋 Gracefully shutting down...");
  process.exit(0);
});

main().catch((error) => {
  console.error("❌ CLI execution failed:", error);
  process.exit(1);
});


--- internal/build/constants.ts ---
/**
 * These are special
 */

export const optionalEntrypoints: Record<string, string[]> = {
  langchain: [
    "chat_models/universal",
    "cache/file_system",
    "storage/file_system",
    "hub",
    "hub/node",
  ],
  "@langchain/classic": [
    "agents/load",
    "agents/toolkits/sql",
    "tools/sql",
    "tools/webbrowser",
    "chains/load",
    "chains/query_constructor",
    "chains/query_constructor/ir",
    "chains/sql_db",
    "chains/graph_qa/cypher",
    "chat_models/universal",
    "document_loaders/fs/buffer",
    "document_loaders/fs/directory",
    "document_loaders/fs/json",
    "document_loaders/fs/multi_file",
    "document_loaders/fs/text",
    "sql_db",
    "output_parsers/expression",
    "retrievers/self_query",
    "retrievers/self_query/functional",
    "cache/file_system",
    "stores/file/node",
    "storage/file_system",
    "hub",
    "hub/node",
  ],
};

export const deprecatedOmitFromImportMap: Record<string, string[]> = {
  langchain: ["hub", "hub/node"],
  "@langchain/core": [
    "context",
    "callbacks/dispatch/web",
    "callbacks/dispatch",
  ],
  "@langchain/classic": ["hub", "hub/node"],
};

export const extraImportMapEntries: Record<
  string,
  {
    modules: string[];
    alias: string[];
    path: string;
  }[]
> = {
  langchain: [
    {
      modules: ["PromptTemplate"],
      alias: ["prompts", "prompt"],
      path: "@langchain/core/prompts",
    },
    {
      modules: [
        "AIMessage",
        "AIMessageChunk",
        "BaseMessage",
        "BaseMessageChunk",
        "ChatMessage",
        "ChatMessageChunk",
        "FunctionMessage",
        "FunctionMessageChunk",
        "HumanMessage",
        "HumanMessageChunk",
        "SystemMessage",
        "SystemMessageChunk",
        "ToolMessage",
        "ToolMessageChunk",
      ],
      alias: ["schema", "messages"],
      path: "@langchain/core/messages",
    },
    {
      modules: [
        "AIMessage",
        "AIMessageChunk",
        "BaseMessage",
        "BaseMessageChunk",
        "ChatMessage",
        "ChatMessageChunk",
        "FunctionMessage",
        "FunctionMessageChunk",
        "HumanMessage",
        "HumanMessageChunk",
        "SystemMessage",
        "SystemMessageChunk",
        "ToolMessage",
        "ToolMessageChunk",
      ],
      alias: ["schema"],
      path: "@langchain/core/messages",
    },
    {
      modules: [
        "AIMessagePromptTemplate",
        "ChatMessagePromptTemplate",
        "ChatPromptTemplate",
        "HumanMessagePromptTemplate",
        "MessagesPlaceholder",
        "SystemMessagePromptTemplate",
      ],
      alias: ["prompts", "chat"],
      path: "@langchain/core/prompts",
    },
    {
      modules: ["ImagePromptTemplate"],
      alias: ["prompts", "image"],
      path: "@langchain/core/prompts",
    },
    {
      modules: ["PipelinePromptTemplate"],
      alias: ["prompts", "pipeline"],
      path: "@langchain/core/prompts",
    },
    {
      modules: ["StringPromptValue"],
      alias: ["prompts", "base"],
      path: "@langchain/core/prompt_values",
    },
    {
      modules: [
        "RouterRunnable",
        "RunnableAssign",
        "RunnableBinding",
        "RunnableBranch",
        "RunnableEach",
        "RunnableMap",
        "RunnableParallel",
        "RunnablePassthrough",
        "RunnablePick",
        "RunnableRetry",
        "RunnableSequence",
        "RunnableWithFallbacks",
        "RunnableWithMessageHistory",
      ],
      alias: ["schema", "runnable"],
      path: "@langchain/core/runnables",
    },
    {
      modules: ["ChatGenerationChunk", "GenerationChunk"],
      alias: ["schema", "output"],
      path: "@langchain/core/outputs",
    },
  ],
  "@langchain/classic": [
    {
      modules: ["StringOutputParser"],
      alias: ["schema", "output_parser"],
      path: "@langchain/core/output_parsers",
    },
  ],
};


--- internal/eslint/eslint.config.ts ---
import { langchainConfig } from "./src/index.js";

export default langchainConfig;


--- internal/test-helpers/eslint.config.ts ---
import { langchainConfig, type ConfigArray } from "@langchain/eslint";

const config: ConfigArray = [
  ...langchainConfig,
  {
    files: ["src/**/*.ts"],
    rules: {
      "no-process-env": "off",
    },
  },
];

export default config;


--- internal/build/index.ts ---
import { resolve, extname } from "node:path";
import { fileURLToPath } from "node:url";

import { build, type Format, type AttwOptions } from "tsdown";
import type { PackageJson } from "type-fest";
import type { Options as UnusedOptions } from "unplugin-unused";

import { lcSecretsPlugin } from "./plugins/lc-secrets.js";
import { importConstantsPlugin } from "./plugins/import-constants.js";
import { importMapPlugin } from "./plugins/import-map.js";
import { findWorkspacePackages } from "./utils.js";
import {
  extraImportMapEntries,
  optionalEntrypoints,
  deprecatedOmitFromImportMap,
} from "./constants.js";
import type { CompilePackageOptions } from "./types.js";

const __dirname = fileURLToPath(import.meta.url);
const root = resolve(__dirname, "..", "..", "..");

export async function compilePackages(opts: CompilePackageOptions) {
  const packages = await findWorkspacePackages(root, opts);
  if (packages.length === 0) {
    const query = opts.packageQuery
      ? `matching "${opts.packageQuery}"`
      : "with no package query";
    throw new Error(`No packages found ${query}!`);
  }

  await Promise.all(
    packages.map(({ pkg, path }) => buildProject(path, pkg, opts))
  );
}

async function buildProject(
  path: string,
  pkg: PackageJson,
  opts: CompilePackageOptions
) {
  const input = Object.entries(pkg.exports || {}).filter(
    ([exp]) => !extname(exp)
  ) as [string, PackageJson.ExportConditions][];
  const entry = input.map(([, { input }]) => input).filter(Boolean) as string[];
  const watch = opts.watch ?? false;
  const sourcemap = !opts.skipSourcemap;
  const exportsCJS = Object.values(pkg.exports || {}).some(
    (exp) => typeof exp === "object" && exp && "require" in exp
  );
  const format: Format[] = exportsCJS ? ["esm", "cjs"] : ["esm"];

  /**
   * don't clean if we:
   * - user passes `--skipClean` or
   * - have watch mode enabled (it would confuse the IDE due to missing type for a short moment)
   * - if `--noEmit` is enabled (we don't want to clean previous builds if we're not emitting anything)
   */
  const clean = !opts.skipClean && !watch && !opts.noEmit;

  /**
   * generate type declarations if not disabled
   */
  const dts = !opts.noEmit
    ? {
        parallel: true,
        cwd: path,
        sourcemap,
        tsgo: true,
      }
    : false;

  /**
   * if there are no entrypoints, skip the package
   */
  if (entry.length === 0) {
    return;
  }

  /**
   * build checks to run, automatically disabled if watch is enabled
   */
  const buildChecks = {
    unused:
      !watch && !opts.skipUnused
        ? ({
            root: path,
            level: "error" as const,
          } as UnusedOptions)
        : false,
    attw: {
      profile: exportsCJS ? "node16" : "esmOnly",
      level: "error",
    } as AttwOptions,
    /**
     * skip publint if:
     * - watch is enabled, to avoid running publint on every change
     * - noEmit is enabled, as not emitting types fails this check
     */
    publint:
      !watch && !opts.noEmit
        ? ({
            pkgDir: path,
            level: "error" as const,
            strict: true,
          } as const)
        : false,
  };

  /**
   * plugins for serialization, automatically disabled if:
   * - watch is enabled or
   * - packages doesn't export a an "./load" entrypoint
   */
  const hasSerializationFeature =
    typeof pkg.exports === "object" &&
    !Array.isArray(pkg.exports) &&
    pkg.exports?.["./load"];
  const plugins =
    !watch && hasSerializationFeature
      ? [
          lcSecretsPlugin({
            // Enable/disable based on environment
            enabled: process.env.SKIP_SECRET_SCANNING !== "true",
            // Use lenient validation in development
            strict: process.env.NODE_ENV === "production",
            // package path for the secret map
            packagePath: path,
          }),
          importConstantsPlugin({
            // Enable/disable based on environment
            enabled: process.env.SKIP_IMPORT_CONSTANTS !== "true",
            // package path for reading package.json
            packagePath: path,
            // package info for reading package.json
            packageInfo: pkg,
            // Add optional entrypoints for langchain package
            optionalEntrypoints: optionalEntrypoints[pkg.name!] || [],
          }),
          importMapPlugin({
            // Enable/disable based on environment
            enabled: process.env.SKIP_IMPORT_MAP !== "true",
            // package path for the import map
            packagePath: path,
            // package info for reading entrypoints
            packageInfo: pkg,
            // Add extra import map entries for langchain package
            extraImportMapEntries: extraImportMapEntries[pkg.name!] || [],
            // Exclude deprecated entrypoints from import map
            // or imports that would cause circular dependencies
            deprecatedOmitFromImportMap:
              deprecatedOmitFromImportMap[pkg.name!] || [],
          }),
        ]
      : [];

  await build({
    entry,
    clean,
    cwd: path,
    dts,
    sourcemap,
    unbundle: true,
    platform: "node",
    target: "es2022",
    outDir: "./dist",
    format,
    watch,
    tsconfig: resolve(path, opts.tsconfigPath ?? "tsconfig.json"),
    ignoreWatch: [
      `${path}/.turbo`,
      `${path}/dist`,
      `${path}/node_modules`,
      /**
       * ignore files that are generated by the plugins
       */
      `${path}/src/load/import_constants.ts`,
      `${path}/src/load/import_map.ts`,
      `${path}/src/load/import_type.ts`,
    ],
    inputOptions: {
      cwd: path,
    },
    plugins,
    ...buildChecks,
  });
}


--- libs/langchain-community/README.md ---
# 🦜️🧑‍🤝‍🧑 LangChain Community

[![CI](https://github.com/langchain-ai/langchainjs/actions/workflows/ci.yml/badge.svg)](https://github.com/langchain-ai/langchainjs/actions/workflows/ci.yml) ![npm](https://img.shields.io/npm/dm/@langchain/community) [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT) [![Twitter](https://img.shields.io/twitter/url/https/twitter.com/langchainai.svg?style=social&label=Follow%20%40LangChainAI)](https://twitter.com/langchainai)

## Quick Install

```bash
$ pnpm install @langchain/community
```

This package, along with the main LangChain package, depends on [`@langchain/core`](https://npmjs.com/package/@langchain/core/).
If you are using this package with other LangChain packages, you should make sure that all of the packages depend on the same instance of @langchain/core.
You can do so by adding appropriate field to your project's `package.json` like this:

```json
{
  "name": "your-project",
  "version": "0.0.0",
  "dependencies": {
    "@langchain/community": "^0.0.0",
    "@langchain/core": "^0.3.0"
  },
  "resolutions": {
    "@langchain/core": "^0.3.0"
  },
  "overrides": {
    "@langchain/core": "^0.3.0"
  },
  "pnpm": {
    "overrides": {
      "@langchain/core": "^0.3.0"
    }
  }
}
```

The field you need depends on the package manager you're using, but we recommend adding a field for the common `yarn`, `npm`, and `pnpm` to maximize compatibility.

## 🤔 What is this?

LangChain Community contains third-party integrations that implement the base interfaces defined in LangChain Core, making them ready-to-use in any LangChain application.

![LangChain Stack](../../docs/core_docs/static/svg/langchain_stack_062024.svg)

## 💁 Contributing

As an open-source project in a rapidly developing field, we are extremely open to contributions, whether it be in the form of a new feature, improved infrastructure, or better documentation.

For detailed information on how to contribute, see [here](../../CONTRIBUTING.md).


## Links discovered
- [![CI](https://github.com/langchain-ai/langchainjs/actions/workflows/ci.yml/badge.svg)
- [npm](https://img.shields.io/npm/dm/@langchain/community)
- [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)
- [![Twitter](https://img.shields.io/twitter/url/https/twitter.com/langchainai.svg?style=social&label=Follow%20%40LangChainAI)
- [`@langchain/core`](https://npmjs.com/package/@langchain/core/)
- [LangChain Stack](https://raw.githubusercontent.com/langchain-ai/langchainjs/main/libs/langchain-community/../../docs/core_docs/static/svg/langchain_stack_062024.svg)
- [here](https://raw.githubusercontent.com/langchain-ai/langchainjs/main/libs/langchain-community/../../CONTRIBUTING.md)

--- libs/langchain-core/README.md ---
# 🦜🍎️ @langchain/core

![npm](https://img.shields.io/npm/dm/@langchain/core) [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT) [![Twitter](https://img.shields.io/twitter/url/https/twitter.com/langchainai.svg?style=social&label=Follow%20%40LangChainAI)](https://twitter.com/langchainai)

`@langchain/core` contains the core abstractions and schemas of LangChain.js, including base classes for language models,
chat models, vectorstores, retrievers, and runnables.

## 💾 Quick Install

```bash
pnpm install @langchain/core
```

## 🤔 What is this?

`@langchain/core` contains the base abstractions that power the rest of the LangChain ecosystem.
These abstractions are designed to be as modular and simple as possible.
Examples of these abstractions include those for language models, document loaders, embedding models, vectorstores, retrievers, and more.
The benefit of having these abstractions is that any provider can implement the required interface and then easily be used in the rest of the LangChain ecosystem.

For example, you can install other provider-specific packages like this:

```bash
pnpm install @langchain/openai
```

And use them as follows:

```typescript
import { StringOutputParser } from "@langchain/core/output_parsers";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { ChatOpenAI } from "@langchain/openai";

const prompt = ChatPromptTemplate.fromTemplate(
  `Answer the following question to the best of your ability:\n{question}`
);

const model = new ChatOpenAI({
  model: "gpt-4o-mini",
  temperature: 0.8,
});

const outputParser = new StringOutputParser();

const chain = prompt.pipe(model).pipe(outputParser);

const stream = await chain.stream({
  question: "Why is the sky blue?",
});

for await (const chunk of stream) {
  console.log(chunk);
}

/*
The
 sky
 appears
 blue
 because
 of
 a
 phenomenon
 known
 as
 Ray
leigh
 scattering
*/
```

Note that for compatibility, all used LangChain packages (including the base LangChain package, which itself depends on core!) must share the same version of `@langchain/core`.
This means that you may need to install/resolve a specific version of `@langchain/core` that matches the dependencies of your used packages.

## 📦 Creating your own package

Other LangChain packages should add this package as a dependency and extend the classes within.
For an example, see the [@langchain/anthropic](https://github.com/langchain-ai/langchainjs/tree/main/libs/providers/langchain-anthropic) in this repo.

Because all used packages must share the same version of core, packages should never directly depend on `@langchain/core`. Instead they should have core as a peer dependency and a dev dependency. We suggest using a tilde dependency to allow for different (backwards-compatible) patch versions:

```json
{
  "name": "@langchain/anthropic",
  "version": "0.0.3",
  "description": "Anthropic integrations for LangChain.js",
  "type": "module",
  "author": "LangChain",
  "license": "MIT",
  "dependencies": {
    "@anthropic-ai/sdk": "^0.10.0"
  },
  "peerDependencies": {
    "@langchain/core": "~0.3.0"
  },
  "devDependencies": {
    "@langchain/core": "~0.3.0"
  }
}
```

We suggest making all packages cross-compatible with ESM and CJS using a build step like the one in
[@langchain/anthropic](https://github.com/langchain-ai/langchainjs/tree/main/libs/providers/langchain-anthropic), then running `pnpm build` before running `npm publish`.

## 💁 Contributing

Because `@langchain/core` is a low-level package whose abstractions will change infrequently, most contributions should be made in the higher-level LangChain package.

Bugfixes or suggestions should be made using the same guidelines as the main package.
See [here](https://github.com/langchain-ai/langchainjs/tree/main/CONTRIBUTING.md) for detailed information.

Please report any security issues or concerns following our [security guidelines](https://github.com/langchain-ai/langchainjs/tree/main/SECURITY.md).


## Links discovered
- [npm](https://img.shields.io/npm/dm/@langchain/core)
- [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)
- [![Twitter](https://img.shields.io/twitter/url/https/twitter.com/langchainai.svg?style=social&label=Follow%20%40LangChainAI)
- [@langchain/anthropic](https://github.com/langchain-ai/langchainjs/tree/main/libs/providers/langchain-anthropic)
- [here](https://github.com/langchain-ai/langchainjs/tree/main/CONTRIBUTING.md)
- [security guidelines](https://github.com/langchain-ai/langchainjs/tree/main/SECURITY.md)

--- libs/langchain-mcp-adapters/README.md ---
# LangChain.js MCP Adapters

[![npm version](https://img.shields.io/npm/v/@langchain/mcp-adapters.svg)](https://www.npmjs.com/package/@langchain/mcp-adapters)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

This library provides a lightweight wrapper that makes [Anthropic Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) tools compatible with [LangChain.js](https://github.com/langchain-ai/langchainjs) and [LangGraph.js](https://github.com/langchain-ai/langgraphjs).

## Features

- 🔌 **Transport Options**

  - Connect to MCP servers via stdio (local) or Streamable HTTP (remote)
    - Streamable HTTP automatically falls back to SSE for compatibility with legacy MCP server implementations
  - Support for custom headers in SSE connections for authentication
  - Configurable reconnection strategies for both transport types

- 🔄 **Multi-Server Management**

  - Connect to multiple MCP servers simultaneously
  - Auto-organize tools by server or access them as a flattened collection

- 🧩 **Agent Integration**

  - Compatible with LangChain.js and LangGraph.js
  - Optimized for OpenAI, Anthropic, and Google models
  - Supports rich content responses including text, images, and embedded resources

- 🛠️ **Development Features**
  - Uses `debug` package for debug logging
  - Flexible configuration options
  - Robust error handling

## Installation

```bash
npm install @langchain/mcp-adapters
```

# Example: Connect to one or more servers via `MultiServerMCPClient`

The library allows you to connect to one or more MCP servers and load tools from them, without needing to manage your own MCP client instances.

```ts
import { createAgent } from "langchain";
import { ChatOpenAI } from "@langchain/openai";
import { MultiServerMCPClient } from "@langchain/mcp-adapters";

// Create client and connect to server
const client = new MultiServerMCPClient({
  // Global tool configuration options
  // Whether to throw on errors if a tool fails to load (optional, default: true)
  throwOnLoadError: true,
  // Whether to prefix tool names with the server name (optional, default: false)
  prefixToolNameWithServerName: false,
  // Optional additional prefix for tool names (optional, default: "")
  additionalToolNamePrefix: "",

  // Use standardized content block format in tool outputs
  useStandardContentBlocks: true,

  // Server configuration
  mcpServers: {
    // adds a STDIO connection to a server named "math"
    math: {
      transport: "stdio",
      command: "npx",
      args: ["-y", "@modelcontextprotocol/server-math"],
      // Restart configuration for stdio transport
      restart: {
        enabled: true,
        maxAttempts: 3,
        delayMs: 1000,
      },
    },

    // here's a filesystem server
    filesystem: {
      transport: "stdio",
      command: "npx",
      args: ["-y", "@modelcontextprotocol/server-filesystem"],
    },

    // Sreamable HTTP transport example, with auth headers and automatic SSE fallback disabled (defaults to enabled)
    weather: {
      url: "https://example.com/weather/mcp",
      headers: {
        Authorization: "Bearer token123",
      }
      automaticSSEFallback: false
    },

    // OAuth 2.0 authentication (recommended for secure servers)
    "oauth-protected-server": {
      url: "https://protected.example.com/mcp",
      authProvider: new MyOAuthProvider({
        // Your OAuth provider implementation
        redirectUrl: "https://myapp.com/oauth/callback",
        clientMetadata: {
          redirect_uris: ["https://myapp.com/oauth/callback"],
          client_name: "My MCP Client",
          scope: "mcp:read mcp:write"
        }
      }),
      // Can still include custom headers for non-auth purposes
      headers: {
        "User-Agent": "My-MCP-Client/1.0"
      }
    },

    // how to force SSE, for old servers that are known to only support SSE (streamable HTTP falls back automatically if unsure)
    github: {
      transport: "sse", // also works with "type" field instead of "transport"
      url: "https://example.com/mcp",
      reconnect: {
        enabled: true,
        maxAttempts: 5,
        delayMs: 2000,
      },
    },
  },
});

const tools = await client.getTools();

// Create an OpenAI model
const model = new ChatOpenAI({
  model: "gpt-4o-mini",
  temperature: 0,
});

// Create the React agent
const agent = createAgent({
  llm: model,
  tools,
});

// Run the agent
try {
  const mathResponse = await agent.invoke({
    messages: [{ role: "user", content: "what's (3 + 5) x 12?" }],
  });
  console.log(mathResponse);
} catch (error) {
  console.error("Error during agent execution:", error);
  // Tools throw ToolException for tool-specific errors
  if (error.name === "ToolException") {
    console.error("Tool execution failed:", error.message);
  }
}

await client.close();
```

# Example: Manage the MCP Client yourself

This example shows how you can manage your own MCP client and use it to get LangChain tools. These tools can be used anywhere LangChain tools are used, including with LangGraph prebuilt agents, as shown below.

The example below requires some prerequisites:

```bash
npm install @langchain/mcp-adapters @langchain/langgraph @langchain/core @langchain/openai

export OPENAI_API_KEY=<your_api_key>
```

```ts
import { Client } from "@modelcontextprotocol/sdk/client/index.js";
import { StdioClientTransport } from "@modelcontextprotocol/sdk/client/stdio.js";

import { createAgent } from "langchain";
import { ChatOpenAI } from "@langchain/openai";
import { loadMcpTools } from "@langchain/mcp-adapters";

// Initialize the ChatOpenAI model
const model = new ChatOpenAI({ model: "gpt-4" });

// Automatically starts and connects to a MCP reference server
const transport = new StdioClientTransport({
  command: "npx",
  args: ["-y", "@modelcontextprotocol/server-math"],
});

// Initialize the client
const client = new Client({
  name: "math-client",
  version: "1.0.0",
});

try {
  // Connect to the transport
  await client.connect(transport);

  // Get tools with custom configuration
  const tools = await loadMcpTools("math", client, {
    // Whether to throw errors if a tool fails to load (optional, default: true)
    throwOnLoadError: true,
    // Whether to prefix tool names with the server name (optional, default: false)
    prefixToolNameWithServerName: false,
    // Optional additional prefix for tool names (optional, default: "")
    additionalToolNamePrefix: "",
    // Use standardized content block format in tool outputs (default: false)
    useStandardContentBlocks: false,
  });

  // Create and run the agent
  const agent = createAgent({ llm: model, tools });
  const agentResponse = await agent.invoke({
    messages: [{ role: "user", content: "what's (3 + 5) x 12?" }],
  });
  console.log(agentResponse);
} catch (e) {
  console.error(e);
} finally {
  // Clean up connection
  await client.close();
}
```

For more detailed examples, see the [examples](./examples) directory.

## Notifications and Progress

You can subscribe to server notifications and tool progress events directly on the `MultiServerMCPClient` via top‑level callbacks.

```ts
import { MultiServerMCPClient } from "@langchain/mcp-adapters";

const client = new MultiServerMCPClient({
  mcpServers: {
    everything: {
      transport: "stdio",
      command: "npx",
      args: ["-y", "@modelcontextprotocol/server-everything"],
    },
  },

  // Receive log/notification messages from the server
  onMessage: (log, source) => {
    console.log(`[${source.server}] ${log.data}`);
  },

  // Receive progress updates (e.g. from long‑running tool calls)
  onProgress: (progress, source) => {
    const pct =
      progress.percentage ??
      (progress.progress != null && progress.total
        ? Math.round((progress.progress / progress.total) * 100)
        : undefined);
    if (pct != null) {
      const origin =
        source.type === "tool" ? `${source.server}/${source.name}` : "unknown";
      console.log(`[progress:${origin}] ${pct}%`);
    }
  },

  // Optional: react to server-side list changes
  onToolsListChanged: (evt, source) => {
    console.log(`[${source.server}] tools changed (${evt.tools?.length ?? 0})`);
  },
});

const tools = await client.getTools();
// ... invoke tools as usual ...
await client.close();
```

Available notification callbacks you can register:

- **onMessage**: server log/diagnostic messages
- **onProgress**: progress events (includes `percentage` or `progress`/`total`) with `source` describing origin (e.g., tool name/server)
- **onInitialized**, **onCancelled**
- **onPromptsListChanged**, **onResourcesListChanged**, **onResourcesUpdated**, **onRootsListChanged**, **onToolsListChanged**

## Tool Hooks (modify args/results)

Use hooks to customize tool calls:

```ts
import { MultiServerMCPClient } from "@langchain/mcp-adapters";

const client = new MultiServerMCPClient({
  mcpServers: {
    math: {
      transport: "stdio",
      command: "npx",
      args: ["-y", "@modelcontextprotocol/server-math"],
    },
  },

  // Change args/headers before the tool call
  beforeToolCall: ({ serverName, name, args }) => {
    // Add/override an argument
    const nextArgs = { ...(args as Record<string, unknown>), injected: true };
    // For HTTP/SSE transports, you may also add per-call headers
    return {
      args: nextArgs,
      headers: { "X-Request-ID": crypto.randomUUID() },
    };
  },

  // Change the tool result after execution
  afterToolCall: (res) => {
    // Option A: return a 2‑tuple [content, artifact]
    if (res.name === "someTool") return { result: ["modified-output", []] };

    // Option B: return a LangChain ToolMessage
    // return { result: new ToolMessage({ content: "overridden", tool_call_id: "id" }) };

    // Option C: return a LangGraph Command instance
    // return { result: new Command(...) }

    // Or pass-through (no change)
    return { result: res.result };
  },
});

const tools = await client.getTools();
const t = tools.find((tool) => tool.name.includes("add"));
const out = await t?.invoke({ a: 1, b: 2 });
```

Notes:

- **beforeToolCall** can return `{ args?, headers? }`. Headers are supported for HTTP/SSE. Stdio connections do not support custom headers.
- **afterToolCall** may return either a 2‑tuple `[content, artifact]`, a `ToolMessage`, a `Command` instance, or nothing (to keep the original result).

## Tool Configuration Options

> [!TIP]
> The `useStandardContentBlocks` defaults to `false` for backward compatibility, however we recommend setting it to `true` for new applications, as this will likely become the default in a future release.

When loading MCP tools either directly through `loadMcpTools` or via `MultiServerMCPClient`, you can configure the following options:

| Option                         | Type                                   | Default                                               | Description                                                                          |
| ------------------------------ | -------------------------------------- | ----------------------------------------------------- | ------------------------------------------------------------------------------------ |
| `throwOnLoadError`             | `boolean`                              | `true`                                                | Whether to throw an error if a tool fails to load                                    |
| `prefixToolNameWithServerName` | `boolean`                              | `false`                                               | If true, prefixes all tool names with the server name (e.g., `serverName__toolName`) |
| `additionalToolNamePrefix`     | `string`                               | `""`                                                  | Additional prefix to add to tool names (e.g., `prefix__serverName__toolName`)        |
| `useStandardContentBlocks`     | `boolean`                              | `false`                                               | See [Tool Output Mapping](#tool-output-mapping); set true for new applications       |
| `outputHandling`               | `"content"`, `"artifact"`, or `object` | `resource` -> `"artifact"`, all others -> `"content"` | See [Tool Output Mapping](#tool-output-mapping)                                      |
| `defaultToolTimeout`           | `number`                               | `0`                                                   | Default timeout for all tools (overridable on a per-tool basis)                      |

## Tool Output Mapping

> [!TIP]
> This section is important if you are working with multimodal tools, tools that produce embedded resources, or tools that produce large outputs that you may not want to be included in LLM input context. If you are writing a new application that only works with tools that produce simple text or JSON output, we recommend setting `useStandardContentBlocks` to `true` and leaving `outputHandling` undefined (will use defaults).

MCP tools return arrays of content blocks. A content block can contain text, an image, audio, or an embedded resource. The right way to map these outputs into LangChain `ToolMessage` objects can differ based on the needs of your application, which is why we introduced the `useStandardContentBlocks` and `outputHandling` configuration options.

The `useStandardContentBlocks` field determines how individual MCP content blocks are transformed into a structure recognized by LangChain ChatModel providers (e.g. `ChatOpenAI`, `ChatAnthropic`, etc). The `outputHandling` field allows you to specify whether a given type of content should be sent to the LLM, or set aside for some other part of your application to use in some future processing step (e.g. to use a dataframe from a database query in a code execution environment).

### Standardizing the Format of Tool Outputs

In `@langchain/core` version 0.3.48 we created a new set of content block types that offer a standardized structure for multimodal inputs. As you might guess from the name, the `useStandardContentBlocks` setting determines whether `@langchain/mcp-adapters` converts tool outputs to this format. For backward compatibility with older versions of `@langchain/mcp-adapters`, it also determines whether tool message artifacts are converted. See the conversion rules below for more info.

> [!IMPORTANT] > `ToolMessage.content` and `ToolMessage.artifact` will always be arrays of content block objects as described by the rules below, except in one special case. When the `outputHandling` option routes `text` output to the `ToolMessage.content` field and the only content block produced by a tool call is a `text` block, `ToolMessage.content` will be a `string` containing the text content produced by the tool.

**When `useStandardContentBlocks` is `true` (recommended for new applications):**

- **Text**: Returned as [`StandardTextBlock`](https://v03.api.js.langchain.com/types/_langchain_core.messages.StandardTextBlock.html) objects.
- **Images**: Returned as base64 [`StandardImageBlock`](https://v03.api.js.langchain.com/types/_langchain_core.messages.StandardImageBlock.html) objects.
- **Audio**: Returned as base64 [`StandardAudioBlock`](https://v03.api.js.langchain.com/types/_langchain_core.messages.StandardAudioBlock.html) objects.
- **Embedded Resources**: Returned as [`StandardFileBlock`](https://v03.api.js.langchain.com/types/_langchain_core.messages.StandardFileBlock.html), with a `source_type` of `text` or `base64` depending on whether the resource was binary or text. URI resources are fetched eagerly from the server and the results of the fetch are returned following these same rules. We treat all embedded resource URIs as resolvable by the server, and we do not attempt to fetch external URIs.

**When `useStandardContentBlocks` is `false` (default for backward compatibility):**

- Tool outputs routed to `ToolMessage.artifact` (controlled by the `outputHandling` option):
  - **Embedded Resources**: Embedded resources containing only a URI are fetched eagerly from the server and the results of the fetch operation are stored in the artifact array without transformation. Otherwise embedded resources are stored in the `artifact` array in their original MCP content block structure without modification.
  - **All other content types**: Stored in the `artifact` array in their original MCP content block structure without modification.
- Tool outputs routed to the `ToolMessage.content` array (controlled by the `outputHandling` option):
  - **Text**: Returned as [`MessageContentText`](https://v03.api.js.langchain.com/types/_langchain_core.messages.MessageContentText.html) objects, unless it is the only content block in the output, in which case it's assigned directly to `ToolMessage.content` as a `string`.
  - **Images**: Returned as [`MessageContentImageUrl`](https://v03.api.js.langchain.com/types/_langchain_core.messages.MessageContentImageUrl.html) objects with base64 data URLs (`data:image/png;base64,<data>`)
  - **Audio**: Returned as [`StandardAudioBlock`](https://v03.api.js.langchain.com/types/_langchain_core.messages.StandardAudioBlock.html) objects.
  - **Embedded Resources**: Returned as [`StandardFileBlock`](https://v03.api.js.langchain.com/types/_langchain_core.messages.StandardFileBlock.html), with a `source_type` of `text` or `base64` depending on whether the resource was binary or text. URI resources are fetched eagerly from the server and the results of the fetch are returned following these same rules. We treat all embedded resource URIs as resolvable by the server, and we do not attempt to fetch external URIs.

### Determining Which Tool Outputs will be Visible to the LLM

The `outputHandling` option allows you to determine which tool output types are assigned to `ToolMessage.content`, and which are assigned to `ToolMessage.artifact`. Data in [`ToolMessage.content`](https://v03.api.js.langchain.com/classes/_langchain_core.messages_tool.ToolMessage.html#content) is used as input context when the LLM is invoked, while [`ToolMessage.artifact`](https://v03.api.js.langchain.com/classes/_langchain_core.messages_tool.ToolMessage.html#artifact) is not.

**By default** `@langchain/mcp-adapters` maps MCP `resource` content blocks to `ToolMessage.artifact`, and maps all other MCP content block types to `ToolMessage.content`. The value of [`useStandardContentBlocks`](#standardizing-the-format-of-tool-outputs) determines how the structure of each content block is transformed during this process.

> [!TIP]
> Examples where `ToolMessage.artifact` can be useful include cases when you need to send multimodal tool outputs via `HumanMessage` or `SystemMessage` because the LLM provider API doesn't accept multimodal tool outputs, or cases where one tool might produce a large output to be indirectly manipulated by some other tool (e.g. a query tool that loads dataframes into a Python code execution environment).

The `outputHandling` option can be assigned to `"content"`, `"artifact"`, or an object that maps MCP content block types to either `content` or `artifact`.

When working with `MultiServerMCPClient`, the `outputHandling` field can be assigned to the top-level config object and/or to individual server entries in `mcpServers`. Entries in `mcpServers` override those in the top-level config, and entries in the top-level config override the defaults.

For example, consider the following configuration:

```typescript
const clientConfig = {
  useStandardContentBlocks: true,
  outputHandling: {
    image: "artifact",
    audio: "artifact",
  },
  mcpServers: {
    camera-server: {
      url: "...",
      outputHandling: {
        image: content
      },
    },
    microphone: {
      url: "...",
      outputHandling: {
        audio: content
      },
    },
  },
}
```

When calling tools from the `camera` MCP server, the following `outputHandling` config will be used:

```typescript
{
  text: "content", // default
  image: "content", // default and top-level config overridden by "camera" server config
  audio: "artifact", // default overridden by top-level config
  resource: "artifact", // default
}
```

Similarly, when calling tools on the `microphone` MCP server, the following `outputHandling` config will be used:

```typescript
{
  text: "content", // default
  image: "artifact", // default overridden by top-level config
  audio: "content", // default and top-level config overridden by "microphone" server config
  resource: "artifact", // default
}
```

## Tool Timeout Configuration

### Using `defaultToolTimeout`

You can configure a global timeout for all tools by setting the `defaultToolTimeout` field in the client params. You can include a `defaultToolTimeout` field in the server config to set the timeout for all tools for that server, or globally for the entire client by setting it in the top-level config.

This timeout will be used as the default timeout for all tools unless overridden by a tool-specific timeout.

```typescript
const client = new MultiServerMCPClient({
  mcpServers: {
    "data-processor": {
      command: "python",
      args: ["data_server.py"],
      defaultToolTimeout: 30000, // timeout will be 30 seconds
    },
    "image-processor": {
      transport: "stdio",
      command: "node",
      args: ["image_server.js"],
      // timeout will be 10 seconds (set in the top-level config)
    },
  },
  defaultToolTimeout: 10000, // 10 seconds
});

const tools = await client.getTools();
const slowTool = tools.find((t) => t.name.includes("process_large_dataset"));

// Will timeout after 30 seconds (defaultToolTimeout)
const result = await slowTool.invoke({ dataset: "huge_file.csv" });
```

### Using `withConfig`

MCP tools support timeout configuration through LangChain's standard `RunnableConfig` interface. This allows you to set custom timeouts on a per-tool-call basis:

```typescript
const client = new MultiServerMCPClient({
  mcpServers: {
    "data-processor": {
      command: "python",
      args: ["data_server.py"],
    },
  },
  useStandardContentBlocks: true,
});

const tools = await client.getTools();
const slowTool = tools.find((t) => t.name.includes("process_large_dataset"));

// You can use withConfig to set tool-specific timeouts before handing
// the tool off to a LangGraph ToolNode or some other part of your
// application
const slowToolWithTimeout = slowTool.withConfig({ timeout: 300000 }); // 5 min timeout

// This invocation will respect the 5 minute timeout
const result = await slowToolWithTimeout.invoke({ dataset: "huge_file.csv" });

// or you can invoke directly without withConfig
const directResult = await slowTool.invoke(
  { dataset: "huge_file.csv" },
  { timeout: 300000 }
);

// Quick timeout for fast operations
const quickResult = await fastTool.invoke(
  { query: "simple_lookup" },
  { timeout: 5000 } // 5 seconds
);

// Default timeout (60 seconds from MCP SDK) when no config provided
const normalResult = await tool.invoke({ input: "normal_processing" });
```

Timeouts can be configured using the following `RunnableConfig` fields:

| Parameter | Type        | Default   | Description                                                   |
| --------- | ----------- | --------- | ------------------------------------------------------------- |
| `timeout` | number      | 60000     | Timeout in milliseconds for the tool call                     |
| `signal`  | AbortSignal | undefined | An AbortSignal that, when asserted, will cancel the tool call |

## OAuth 2.0 Authentication

For secure MCP servers that require OAuth 2.0 authentication, you can use the `authProvider` option instead of manually managing headers. This provides automatic token refresh, error handling, and standards-compliant OAuth flows.

New in v0.4.6.

### Basic OAuth Setup

```ts
import type { OAuthClientProvider } from "@modelcontextprotocol/sdk/client/auth.js";

class MyOAuthProvider implements OAuthClientProvider {
  constructor(
    private config: {
      redirectUrl: string;
      clientMetadata: OAuthClientMetadata;
    }
  ) {}

  get redirectUrl() {
    return this.config.redirectUrl;
  }
  get clientMetadata() {
    return this.config.clientMetadata;
  }

  // Implement token storage (localStorage, database, etc.)
  tokens(): OAuthTokens | undefined {
    const stored = localStorage.getItem("mcp_tokens");
    return stored ? JSON.parse(stored) : undefined;
  }

  async saveTokens(tokens: OAuthTokens): Promise<void> {
    localStorage.setItem("mcp_tokens", JSON.stringify(tokens));
  }

  // Implement other required methods...
  // See MCP SDK documentation for complete examples
}

const client = new MultiServerMCPClient({
  mcpServers: {
    "secure-server": {
      url: "https://secure-mcp-server.example.com/mcp",
      authProvider: new MyOAuthProvider({
        redirectUrl: "https://myapp.com/oauth/callback",
        clientMetadata: {
          redirect_uris: ["https://myapp.com/oauth/callback"],
          client_name: "My MCP Client",
          scope: "mcp:read mcp:write",
        },
      }),
    },
  },
  useStandardContentBlocks: true,
});
```

### OAuth Features

The `authProvider` automatically handles:

- ✅ **Token Refresh**: Automatically refreshes expired access tokens using refresh tokens
- ✅ **401 Error Recovery**: Automatically retries requests after successful authentication
- ✅ **PKCE Security**: Uses Proof Key for Code Exchange for enhanced security
- ✅ **Standards Compliance**: Follows OAuth 2.0 and RFC 6750 specifications
- ✅ **Transport Compatibility**: Works with both StreamableHTTP and SSE transports

### OAuth vs Manual Headers

| Aspect            | OAuth Provider          | Manual Headers                    |
| ----------------- | ----------------------- | --------------------------------- |
| **Token Refresh** | ✅ Automatic            | ❌ Manual implementation required |
| **401 Handling**  | ✅ Automatic retry      | ❌ Manual error handling required |
| **Security**      | ✅ PKCE, secure flows   | ⚠️ Depends on implementation      |
| **Standards**     | ✅ RFC 6750 compliant   | ⚠️ Requires manual compliance     |
| **Complexity**    | ✅ Simple configuration | ❌ Complex implementation         |

**Recommendation**: Use `authProvider` for production OAuth servers, and `headers` only for simple token-based auth or debugging.

## Reconnection Strategies

Both transport types support automatic reconnection:

### Stdio Transport Restart

```ts
{
  transport: "stdio",
  command: "npx",
  args: ["-y", "@modelcontextprotocol/server-math"],
  restart: {
    enabled: true,      // Enable automatic restart
    maxAttempts: 3,     // Maximum restart attempts
    delayMs: 1000       // Delay between attempts in ms
  }
}
```

### SSE Transport Reconnect

```ts
{
  transport: "sse",
  url: "https://example.com/mcp-server",
  headers: { "Authorization": "Bearer token123" },
  reconnect: {
    enabled: true,      // Enable automatic reconnection
    maxAttempts: 5,     // Maximum reconnection attempts
    delayMs: 2000       // Delay between attempts in ms
  }
}
```

## Error Handling

The library provides different error types to help with debugging:

- **MCPClientError**: For client connection and initialization issues
- **ToolException**: For errors during tool execution
- **ZodError**: For configuration validation errors (invalid connection settings, etc.)

Example error handling:

```ts
try {
  const client = new MultiServerMCPClient({
    mcpServers: {
      math: {
        transport: "stdio",
        command: "npx",
        args: ["-y", "@modelcontextprotocol/server-math"],
      },
    },
    useStandardContentBlocks: true,
  });

  const tools = await client.getTools();
  const result = await tools[0].invoke({ expression: "1 + 2" });
} catch (error) {
  if (error.name === "MCPClientError") {
    // Handle connection issues
    console.error(`Connection error (${error.serverName}):`, error.message);
  } else if (error.name === "ToolException") {
    // Handle tool execution errors
    console.error("Tool execution failed:", error.message);
  } else if (error.name === "ZodError") {
    // Handle configuration validation errors
    console.error("Configuration error:", error.issues);
    // Zod errors contain detailed information about what went wrong
    error.issues.forEach((issue) => {
      console.error(`- Path: ${issue.path.join(".")}, Error: ${issue.message}`);
    });
  } else {
    // Handle other errors
    console.error("Unexpected error:", error);
  }
}
```

### Common Zod Validation Errors

The library uses Zod for validating configuration. Here are some common validation errors:

- **Missing required parameters**: For example, omitting `command` for stdio transport or `url` for SSE transport
- **Invalid parameter types**: For example, providing a number where a string is expected
- **Invalid connection configuration**: For example, using an invalid URL format for SSE transport

Example Zod error for an invalid SSE URL:

```json
{
  "issues": [
    {
      "code": "invalid_string",
      "validation": "url",
      "path": ["mcpServers", "weather", "url"],
      "message": "Invalid url"
    }
  ],
  "name": "ZodError"
}
```

### Debug Logging

This package makes use of the [debug](https://www.npmjs.com/package/debug) package for debug logging.

Logging is disabled by default, and can be enabled by setting the `DEBUG` environment variable as per
the instructions in the debug package.

To output all debug logs from this package:

```bash
DEBUG='@langchain/mcp-adapters:*'
```

To output debug logs only from the `client` module:

```bash
DEBUG='@langchain/mcp-adapters:client'
```

To output debug logs only from the `tools` module:

```bash
DEBUG='@langchain/mcp-adapters:tools'
```

## License

MIT

## Acknowledgements

Big thanks to [@vrknetha](https://github.com/vrknetha), [@knacklabs](https://www.knacklabs.ai) for the initial implementation!

## Contributing

Contributions are welcome! Please check out our [contributing guidelines](CONTRIBUTING.md) for more information.


## Links discovered
- [![npm version](https://img.shields.io/npm/v/@langchain/mcp-adapters.svg)
- [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)
- [Anthropic Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction)
- [LangChain.js](https://github.com/langchain-ai/langchainjs)
- [LangGraph.js](https://github.com/langchain-ai/langgraphjs)
- [examples](https://raw.githubusercontent.com/langchain-ai/langchainjs/main/libs/langchain-mcp-adapters/./examples)
- [`StandardTextBlock`](https://v03.api.js.langchain.com/types/_langchain_core.messages.StandardTextBlock.html)
- [`StandardImageBlock`](https://v03.api.js.langchain.com/types/_langchain_core.messages.StandardImageBlock.html)
- [`StandardAudioBlock`](https://v03.api.js.langchain.com/types/_langchain_core.messages.StandardAudioBlock.html)
- [`StandardFileBlock`](https://v03.api.js.langchain.com/types/_langchain_core.messages.StandardFileBlock.html)
- [`MessageContentText`](https://v03.api.js.langchain.com/types/_langchain_core.messages.MessageContentText.html)
- [`MessageContentImageUrl`](https://v03.api.js.langchain.com/types/_langchain_core.messages.MessageContentImageUrl.html)
- [`ToolMessage.content`](https://v03.api.js.langchain.com/classes/_langchain_core.messages_tool.ToolMessage.html#content)
- [`ToolMessage.artifact`](https://v03.api.js.langchain.com/classes/_langchain_core.messages_tool.ToolMessage.html#artifact)
- [debug](https://www.npmjs.com/package/debug)
- [@vrknetha](https://github.com/vrknetha)
- [@knacklabs](https://www.knacklabs.ai)
- [contributing guidelines](https://raw.githubusercontent.com/langchain-ai/langchainjs/main/libs/langchain-mcp-adapters/CONTRIBUTING.md)

--- libs/langchain-standard-tests/README.md ---
# LangChain.js Standard Tests

This package contains the base standard tests for LangChain.js. It includes unit, and integration test classes.
This package is not intended to be used outside of the LangChain.js project, and thus it is not published to npm.

At the moment, we only have support for standard tests for chat models.

## Usage

Each LangChain.js integration should contain both unit and integration standard tests.
The package should have `@langchain/standard-tests` as a dev workspace dependency like so:

`package.json`:

```json
{
  "devDependencies": {
    "@langchain/standard-tests": "workspace:*"
  }
}
```

To use the standard tests, you could create two files:

- `src/tests/chat_models.standard.test.ts` - chat model unit tests
- `src/tests/chat_models.standard.int.test.ts` - chat model integration tests

Your unit test file should look like this:

`chat_models.standard.test.ts`:

```typescript
/* eslint-disable no-process-env */
import { test, expect } from "@jest/globals";
import { ChatModelUnitTests } from "@langchain/standard-tests";
import { AIMessageChunk } from "@langchain/core/messages";
import { MyChatModel, MyChatModelCallOptions } from "../chat_models.js";

class MyChatModelStandardUnitTests extends ChatModelUnitTests<
  MyChatModelCallOptions,
  AIMessageChunk
> {
  constructor() {
    super({
      Cls: MyChatModel,
      chatModelHasToolCalling: true, // Set to true if the model has tool calling support
      chatModelHasStructuredOutput: true, // Set to true if the model has withStructuredOutput support
      constructorArgs: {}, // Any additional constructor args
    });
    // This must be set so method like `.bindTools` or `.withStructuredOutput`
    // which we call after instantiating the model will work.
    // (constructor will throw if API key is not set)
    process.env.CHAT_MODEL_API_KEY = "test";
  }

  testChatModelInitApiKey() {
    // Unset the API key env var here so this test can properly check
    // the API key class arg.
    process.env.CHAT_MODEL_API_KEY = "";
    super.testChatModelInitApiKey();
    // Re-set the API key env var here so other tests can run properly.
    process.env.CHAT_MODEL_API_KEY = "test";
  }
}

const testClass = new MyChatModelStandardUnitTests();

test("MyChatModelStandardUnitTests", () => {
  const testResults = testClass.runTests();
  expect(testResults).toBe(true);
});
```

To use the standard tests, extend the `ChatModelUnitTests` class, passing in your chat model's call options and message chunk types. Super the constructor with your chat model class, any additional constructor args, and set the `chatModelHasToolCalling` and `chatModelHasStructuredOutput` flags if supported.

Set the model env var in the constructor directly to `process.env` for the tests to run properly. You can optionally override test methods to replace or add code before/after the test runs.

Run all tests by calling `.runTests()`, which returns `true` if all tests pass, `false` otherwise. Tests are called in `try`/`catch` blocks so that failing tests are caught and marked as failed, but the rest still run.

For integration tests, extend `ChatModelIntegrationTests` instead. Integration tests have an optional arg for all methods (except `withStructuredOutput`) to pass in "invoke" time call options. For example, in the OpenAI integration test:

```typescript
async testUsageMetadataStreaming() {
  // ChatOpenAI does not support streaming tokens by
  // default, so we must pass in a call option to
  // enable streaming tokens.
  const callOptions: ChatOpenAI["ParsedCallOptions"] = {
    stream_options: {
      include_usage: true,
    },
  };
  await super.testUsageMetadataStreaming(callOptions);
}
```

This overrides the base `testUsageMetadataStreaming` to pass a `callOptions` arg enabling streaming tokens.


--- libs/langchain-textsplitters/README.md ---
# 🦜✂️ @langchain/textsplitters

This package contains various implementations of LangChain.js text splitters, most commonly used as part of retrieval-augmented generation (RAG) pipelines.

## Installation

```bash npm2yarn
npm install @langchain/textsplitters @langchain/core
```

## Development

To develop the `@langchain/textsplitters` package, you'll need to follow these instructions:

### Install dependencies

```bash
pnpm install
```

### Build the package

```bash
pnpm build
```

Or from the repo root:

```bash
pnpm build --filter @langchain/textsplitters
```

### Run tests

Test files should live within a `tests/` file in the `src/` folder. Unit tests should end in `.test.ts` and integration tests should
end in `.int.test.ts`:

```bash
$ pnpm test
$ pnpm test:int
```

### Lint & Format

Run the linter & formatter to ensure your code is up to standard:

```bash
pnpm lint && pnpm format
```

### Adding new entrypoints

If you add a new file to be exported, either import & re-export from `src/index.ts`, or add it to the `exports` field in the `package.json` file and run `pnpm build` to generate the new entrypoint.


--- libs/langchain/README.md ---
# 🦜️🔗 LangChain.js

![npm](https://img.shields.io/npm/dm/langchain) [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT) [![Twitter](https://img.shields.io/twitter/url/https/twitter.com/langchainai.svg?style=social&label=Follow%20%40LangChainAI)](https://twitter.com/langchainai)

LangChain is a framework for building LLM-powered applications. It helps you chain together interoperable components and third-party integrations to simplify AI application development — all while future-proofing decisions as the underlying technology evolves.

**Documentation**: To learn more about LangChain, check out [the docs](https://docs.langchain.com/oss/javascript/langchain/overview).

If you're looking for more advanced customization or agent orchestration, check out [LangGraph.js](https://langchain-ai.github.io/langgraphjs/). our framework for building agents and controllable workflows.

> [!NOTE]
> Looking for the Python version? Check out [LangChain](https://github.com/langchain-ai/langchain).

To help you ship LangChain apps to production faster, check out [LangSmith](https://smith.langchain.com).
[LangSmith](https://smith.langchain.com) is a unified developer platform for building, testing, and monitoring LLM applications.

## ⚡️ Quick Install

You can use npm, pnpm, or yarn to install LangChain.js

`npm install -S langchain` or `pnpm install langchain` or `yarn add langchain`

## 🚀 Why use LangChain?

LangChain helps developers build applications powered by LLMs through a standard interface for agents, models, embeddings, vector stores, and more.

Use LangChain for:

- **Real-time data augmentation**. Easily connect LLMs to diverse data sources and external/internal systems, drawing from LangChain’s vast library of integrations with model providers, tools, vector stores, retrievers, and more.
- **Model interoperability**. Swap models in and out as your engineering team experiments to find the best choice for your application’s needs. As the industry frontier evolves, adapt quickly — LangChain’s abstractions keep you moving without losing momentum.

## 📦 LangChain's ecosystem

- [LangSmith](https://www.langchain.com/langsmith) - Unified developer platform for building, testing, and monitoring LLM applications. With LangSmith, you can debug poor-performing LLM app runs, evaluate agent trajectories, gain visibility in production, and deploy agents with confidence.
- [LangGraph](https://docs.langchain.com/oss/javascript/langgraph/overview) - Build agents that can reliably handle complex tasks with LangGraph, our low-level agent orchestration framework. LangGraph offers customizable architecture, long-term memory, and human-in-the-loop workflows — and is trusted in production by companies like LinkedIn, Uber, Klarna, and GitLab.

## 🌐 Supported Environments

LangChain.js is written in TypeScript and can be used in:

- Node.js (ESM and CommonJS) - 18.x, 19.x, 20.x, 22.x
- Cloudflare Workers
- Vercel / Next.js (Browser, Serverless and Edge functions)
- Supabase Edge Functions
- Browser
- Deno

## 📖 Additional Resources

- [Getting started](https://docs.langchain.com/oss/javascript/langchain/overview): Installation, setting up the environment, simple examples
- [Learn](https://docs.langchain.com/oss/javascript/langchain/learn): Learn about the core concepts of LangChain.
- [LangChain Forum](https://forum.langchain.com): Connect with the community and share all of your technical questions, ideas, and feedback.
- [Chat LangChain](https://chat.langchain.com): Ask questions & chat with our documentation.

## 💁 Contributing

As an open-source project in a rapidly developing field, we are extremely open to contributions, whether it be in the form of a new feature, improved infrastructure, or better documentation.

For detailed information on how to contribute, see [here](https://github.com/langchain-ai/langchainjs/blob/main/CONTRIBUTING.md).

Please report any security issues or concerns following our [security guidelines](https://github.com/langchain-ai/langchainjs/blob/main/SECURITY.md).


## Links discovered
- [npm](https://img.shields.io/npm/dm/langchain)
- [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)
- [![Twitter](https://img.shields.io/twitter/url/https/twitter.com/langchainai.svg?style=social&label=Follow%20%40LangChainAI)
- [the docs](https://docs.langchain.com/oss/javascript/langchain/overview)
- [LangGraph.js](https://langchain-ai.github.io/langgraphjs/)
- [LangChain](https://github.com/langchain-ai/langchain)
- [LangSmith](https://smith.langchain.com)
- [LangSmith](https://www.langchain.com/langsmith)
- [LangGraph](https://docs.langchain.com/oss/javascript/langgraph/overview)
- [Getting started](https://docs.langchain.com/oss/javascript/langchain/overview)
- [Learn](https://docs.langchain.com/oss/javascript/langchain/learn)
- [LangChain Forum](https://forum.langchain.com)
- [Chat LangChain](https://chat.langchain.com)
- [here](https://github.com/langchain-ai/langchainjs/blob/main/CONTRIBUTING.md)
- [security guidelines](https://github.com/langchain-ai/langchainjs/blob/main/SECURITY.md)

--- libs/create-langchain-integration/template/README.md ---
# @langchain/<ADD_PACKAGE_NAME_HERE>

This package contains the LangChain.js integrations for <ADD_NAME_HERE> through their SDK.

## Installation

```bash npm2yarn
npm install @langchain/<ADD_PACKAGE_NAME_HERE> @langchain/core
```

This package, along with the main LangChain package, depends on [`@langchain/core`](https://npmjs.com/package/@langchain/core/).
If you are using this package with other LangChain packages, you should make sure that all of the packages depend on the same instance of `@langchain/core`.
You can do so by adding appropriate field to your project's `package.json` like this:

```json
{
  "name": "your-project",
  "version": "0.0.0",
  "dependencies": {
    "@langchain/<ADD_PACKAGE_NAME_HERE>": "^0.0.0",
    "@langchain/core": "^0.3.0"
  },
  "resolutions": {
    "@langchain/core": "^0.3.0"
  },
  "overrides": {
    "@langchain/core": "^0.3.0"
  },
  "pnpm": {
    "overrides": {
      "@langchain/core": "^0.3.0"
    }
  }
}
```

The field you need depends on the package manager you're using, but we recommend adding a field for the common `yarn`, `npm`, and `pnpm` to maximize compatibility.

## Chat Models

This package contains the `<ADD_CLASS_NAME_HERE>` class, which is the recommended way to interface with the <ADD_NAME_HERE> series of models.

To use, install the requirements, and configure your environment.

```bash
export <ADD_ENV_NAME_HERE>=your-api-key
```

Then initialize

```typescript
import { <ADD_CLASS_NAME_HERE> } from "@langchain/<ADD_PACKAGE_NAME_HERE>";

const model = new ExampleChatClass({
  apiKey: process.env.EXAMPLE_API_KEY,
});
const response = await model.invoke(new HumanMessage("Hello world!"));
```

### Streaming

```typescript
import { <ADD_CLASS_NAME_HERE> } from "@langchain/<ADD_PACKAGE_NAME_HERE>";

const model = new ExampleChatClass({
  apiKey: process.env.EXAMPLE_API_KEY,
});
const response = await model.stream(new HumanMessage("Hello world!"));
```

## Embeddings

This package also adds support for <ADD_NAME_HERE> embeddings model.

```typescript
import { <ADD_CLASS_NAME_HERE> } from "@langchain/<ADD_PACKAGE_NAME_HERE>";

const embeddings = new ExampleEmbeddingClass({
  apiKey: process.env.EXAMPLE_API_KEY,
});
const res = await embeddings.embedQuery("Hello world");
```

## Development

To develop the <ADD_NAME_HERE> package, you'll need to follow these instructions:

### Install dependencies

```bash
pnpm install
```

### Build the package

```bash
pnpm build
```

Or from the repo root:

```bash
pnpm build --filter @langchain/<ADD_PACKAGE_NAME_HERE>
```

### Run tests

Test files should live within a `tests/` file in the `src/` folder. Unit tests should end in `.test.ts` and integration tests should
end in `.int.test.ts`:

```bash
$ pnpm test
$ pnpm test:int
```

### Lint & Format

Run the linter & formatter to ensure your code is up to standard:

```bash
pnpm lint && pnpm format
```

### Adding new entrypoints

If you add a new file to be exported, either import & re-export from `src/index.ts`, or add it to the `exports` field in the `package.json` file and run `pnpm build` to generate the new entrypoint.


## Links discovered
- [`@langchain/core`](https://npmjs.com/package/@langchain/core/)

--- libs/providers/langchain-anthropic/README.md ---
# @langchain/anthropic

This package contains the LangChain.js integrations for Anthropic through their SDK.

## Installation

```bash npm2yarn
npm install @langchain/anthropic @langchain/core
```

This package, along with the main LangChain package, depends on [`@langchain/core`](https://npmjs.com/package/@langchain/core/).
If you are using this package with other LangChain packages, you should make sure that all of the packages depend on the same instance of @langchain/core.
You can do so by adding appropriate fields to your project's `package.json` like this:

```json
{
  "name": "your-project",
  "version": "0.0.0",
  "dependencies": {
    "@langchain/anthropic": "^0.0.9",
    "@langchain/core": "^0.3.0"
  },
  "resolutions": {
    "@langchain/core": "^0.3.0"
  },
  "overrides": {
    "@langchain/core": "^0.3.0"
  },
  "pnpm": {
    "overrides": {
      "@langchain/core": "^0.3.0"
    }
  }
}
```

The field you need depends on the package manager you're using, but we recommend adding a field for the common `yarn`, `npm`, and `pnpm` to maximize compatibility.

## Chat Models

This package contains the `ChatAnthropic` class, which is the recommended way to interface with the Anthropic series of models.

To use, install the requirements, and configure your environment.

```bash
export ANTHROPIC_API_KEY=your-api-key
```

Then initialize

```typescript
import { ChatAnthropic } from "@langchain/anthropic";

const model = new ChatAnthropic({
  apiKey: process.env.ANTHROPIC_API_KEY,
});
const response = await model.invoke({
  role: "user",
  content: "Hello world!",
});
```

### Streaming

```typescript
import { ChatAnthropic } from "@langchain/anthropic";

const model = new ChatAnthropic({
  apiKey: process.env.ANTHROPIC_API_KEY,
  model: "claude-3-sonnet-20240229",
});
const response = await model.stream({
  role: "user",
  content: "Hello world!",
});
```

## Development

To develop the Anthropic package, you'll need to follow these instructions:

### Install dependencies

```bash
pnpm install
```

### Build the package

```bash
pnpm build
```

Or from the repo root:

```bash
pnpm build --filter @langchain/anthropic
```

### Run tests

Test files should live within a `tests/` file in the `src/` folder. Unit tests should end in `.test.ts` and integration tests should
end in `.int.test.ts`:

```bash
$ pnpm test
$ pnpm test:int
```

### Lint & Format

Run the linter & formatter to ensure your code is up to standard:

```bash
pnpm lint && pnpm format
```

### Adding new entrypoints

If you add a new file to be exported, either import & re-export from `src/index.ts`, or add it to the `exports` field in the `package.json` file and run `pnpm build` to generate the new entrypoint.

## Publishing

After running `pnpm build`, publish a new version with:

```bash
$ npm publish
```


## Links discovered
- [`@langchain/core`](https://npmjs.com/package/@langchain/core/)

--- libs/providers/langchain-aws/README.md ---
# @langchain/aws

This package contains the LangChain.js integrations for AWS through their SDK.

## Installation

```bash
npm install @langchain/aws
```

This package, along with the main LangChain package, depends on [`@langchain/core`](https://npmjs.com/package/@langchain/core/).
If you are using this package with other LangChain packages, you should make sure that all of the packages depend on the same instance of @langchain/core.
You can do so by adding appropriate fields to your project's `package.json` like this:

```json
{
  "name": "your-project",
  "version": "0.0.0",
  "dependencies": {
    "@langchain/aws": "^0.0.1",
    "@langchain/core": "^0.3.0"
  },
  "resolutions": {
    "@langchain/core": "^0.3.0"
  },
  "overrides": {
    "@langchain/core": "^0.3.0"
  },
  "pnpm": {
    "overrides": {
      "@langchain/core": "^0.3.0"
    }
  }
}
```

The field you need depends on the package manager you're using, but we recommend adding a field for the common `yarn`, `npm`, and `pnpm` to maximize compatibility.

## Chat Models

This package contains the `ChatBedrockConverse` class, which is the recommended way to interface with the AWS Bedrock Converse series of models.

To use, install the requirements, and configure your environment following the traditional authentication methods.

```bash
export BEDROCK_AWS_REGION=
export BEDROCK_AWS_SECRET_ACCESS_KEY=
export BEDROCK_AWS_ACCESS_KEY_ID=
```

Alternatively, set the `AWS_BEARER_TOKEN_BEDROCK` environment variable locally for API Key authentication. For additional API key details, refer to [docs](https://docs.aws.amazon.com/bedrock/latest/userguide/api-keys.html).

```bash
export BEDROCK_AWS_REGION=
export AWS_BEARER_TOKEN_BEDROCK=
```

Then initialize

```typescript
import { ChatBedrockConverse } from "@langchain/aws";

const model = new ChatBedrockConverse({
  region: process.env.BEDROCK_AWS_REGION ?? "us-east-1",
  credentials: {
    secretAccessKey: process.env.BEDROCK_AWS_SECRET_ACCESS_KEY,
    accessKeyId: process.env.BEDROCK_AWS_ACCESS_KEY_ID,
  },
});

const response = await model.invoke(new HumanMessage("Hello world!"));
```

### Streaming

```typescript
import { ChatBedrockConverse } from "@langchain/aws";

const model = new ChatBedrockConverse({
  region: process.env.BEDROCK_AWS_REGION ?? "us-east-1",
  credentials: {
    secretAccessKey: process.env.BEDROCK_AWS_SECRET_ACCESS_KEY,
    accessKeyId: process.env.BEDROCK_AWS_ACCESS_KEY_ID,
  },
});

const response = await model.stream(new HumanMessage("Hello world!"));
```

## Development

To develop the AWS package, you'll need to follow these instructions:

### Install dependencies

```bash
pnpm install
```

### Build the package

```bash
pnpm build
```

Or from the repo root:

```bash
pnpm build --filter @langchain/aws
```

### Run tests

Test files should live within a `tests/` file in the `src/` folder. Unit tests should end in `.test.ts` and integration tests should
end in `.int.test.ts`:

```bash
$ pnpm test
$ pnpm test:int
```

### Lint & Format

Run the linter & formatter to ensure your code is up to standard:

```bash
pnpm lint && pnpm format
```

### Adding new entrypoints

If you add a new file to be exported, either import & re-export from `src/index.ts`, or add it to the `exports` field in the `package.json` file and run `pnpm build` to generate the new entrypoint.

## Publishing

After running `pnpm build`, publish a new version with:

```bash
$ npm publish
```


## Links discovered
- [`@langchain/core`](https://npmjs.com/package/@langchain/core/)
- [docs](https://docs.aws.amazon.com/bedrock/latest/userguide/api-keys.html)

--- libs/providers/langchain-azure-cosmosdb/README.md ---
# @langchain/azure-cosmosdb 

This package contains the [Azure CosmosDB](https://learn.microsoft.com/azure/cosmos-db/) vector store integrations.

Learn more about how to use this package in the LangChain documentation:
- [Azure CosmosDB for NoSQL](https://js.langchain.com/docs/integrations/vector_stores/azure_cosmosdb_nosql)
- [Azure CosmosDB for MongoDB vCore](https://js.langchain.com/docs/integrations/vector_stores/azure_cosmosdb_mongodb)

## Installation

```bash npm2yarn
npm install @langchain/azure-cosmosdb @langchain/core
```

This package, along with the main LangChain package, depends on [`@langchain/core`](https://npmjs.com/package/@langchain/core/).
If you are using this package with other LangChain packages, you should make sure that all of the packages depend on the same instance of @langchain/core.
You can do so by adding appropriate fields to your project's `package.json` like this:

```json
{
  "name": "your-project",
  "version": "0.0.0",
  "dependencies": {
    "@langchain/core": "^0.3.0",
    "@langchain/azure-cosmosdb": "^0.2.5"
  },
  "resolutions": {
    "@langchain/core": "0.3.0"
  },
  "overrides": {
    "@langchain/core": "0.3.0"
  },
  "pnpm": {
    "overrides": {
      "@langchain/core": "0.3.0"
    }
  }
}
```

The field you need depends on the package manager you're using, but we recommend adding a field for the common `yarn`, `npm`, and `pnpm` to maximize compatibility.

## Usage

```typescript
import { AzureCosmosDBNoSQLVectorStore } from "@langchain/azure-cosmosdb";

const store = await AzureCosmosDBNoSQLVectorStore.fromDocuments(
  ["Hello, World!"],
  new OpenAIEmbeddings(),
  {
    databaseName: "langchain",
    containerName: "documents",
  }
);

const resultDocuments = await store.similaritySearch("hello");
console.log(resultDocuments[0].pageContent);
```


## Links discovered
- [Azure CosmosDB](https://learn.microsoft.com/azure/cosmos-db/)
- [Azure CosmosDB for NoSQL](https://js.langchain.com/docs/integrations/vector_stores/azure_cosmosdb_nosql)
- [Azure CosmosDB for MongoDB vCore](https://js.langchain.com/docs/integrations/vector_stores/azure_cosmosdb_mongodb)
- [`@langchain/core`](https://npmjs.com/package/@langchain/core/)
