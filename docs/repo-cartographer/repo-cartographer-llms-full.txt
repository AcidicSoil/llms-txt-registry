# llms-full (private-aware)
> Built by authenticated GitHub API fetches. Large files may be truncated.

--- rpg_tool/docs/PROJECT_SPEC.example.md ---
<!-- path: docs/PROJECT_SPEC.example.md -->
# RPG‑Build‑Assistant — Project Spec (Example)


## Goals

- Parse a markdown **Project Spec** into JSON `{goals[], non_goals[], constraints[], metrics[]}`.
- Fail CI when any required section is missing/empty.


## Non-Goals

- Build the full RPG schema.
- Generate code from the spec.


## Constraints

- Python 3.11; no external deps.
- Runs in CI in < 5s.


## Acceptance Metrics

- Validator returns **OK** for `docs/PROJECT_SPEC.example.md`.
- CI job fails on malformed specs.


### Stakeholders & Review

- **Owner:** Repo Maintainer
- **Stakeholders:** PM, Tech Lead
- **Review Status:** approved


### Context

- Downstream task will use this JSON as input.


--- .taskmaster/docs/.ai_context/dspy-codefetch/dspyTutorial_codefetch/DSPY-Tutorials_codebase.md ---


--- .taskmaster/docs/owner-docs/TODO/OTHER/gemini-cli_example_prompt_ranking_reports.md ---
# Prompt Ranking Reports

This file contains a log of the prompt ranking reports generated during our conversation.

---

## Query 1: "Read these docs"

**Report:**

filename | description | match_score
---|---|---
docs-fulfilled-100.md | A prompt to analyze the current documentation status and identify missing information. | 0.90
missing-docs.md | A template to identify and report missing documentation sections in an analysis. | 0.85
api-docs-local.md | A prompt for generating API documentation for a local project. | 0.80
summary.md | A prompt to generate a high-level summary of a repository. | 0.75
generate-readme.md | A template for generating a README file for a project. | 0.70
README.md | A file that documents the MCP workflow system, including prompt categories and commands. | 0.65

---

## Query 2: " @.taskmaster/docs/owner-docs/TODO/RPG-DSPY"

**Report:**

filename | description | match_score
---|---|---
grep.md | A prompt to perform a recursive text search within a given path using ripgrep or grep. | 0.90
summary.md | A prompt to generate a high-level summary of a repository or directory. | 0.85
file-modularity.md | A prompt to analyze and enforce smaller, more modular file structures within a given path. | 0.80
dead-code-scan.md | A prompt to scan a given path to find and report potentially unused or dead code. | 0.75
owners.md | A prompt to suggest potential owners or reviewers for a file path based on git history and CODEOWNERS. | 0.75
docs-fulfilled-100.md | A prompt to analyze the current documentation status and identify missing information for a given context. | 0.70
missing-docs.md | A template to identify and report missing documentation sections in an analysis. | 0.65
generate-readme.md | A template for generating a README file for a project or directory. | 0.60
explain-code.md | A template for generating an explanation for a piece of code. | 0.60

---

## Query 3: "what prompt could i use to ask to fetch needed docs?"

**Report:**

filename | description | match_score
---|---|---
docs-fulfilled-100.md | A prompt to analyze the current documentation status and identify what information is missing to fulfill a plan. | 0.90
missing-docs.md | A template to identify and report missing documentation sections in an analysis. | 0.85
research-item.md | A prompt to perform a deep-dive research workflow on a single topic, which includes fetching and synthesizing evidence from documents. | 0.85
grep.md | A prompt to perform a recursive text search, which can be used to find and "fetch" specific information from local documentation files. | 0.80
api-usage.md | A prompt to find and summarize how an internal API is used, which involves fetching usage examples from the codebase. | 0.75
summary.md | A prompt to generate a high-level summary of a repository, effectively "fetching" the key information from all its documents. | 0.70

---

## Query 4: "should i create a new repo or directory for the RPG DSPY project?"

**Report:**

filename | description | match_score
---|---|---
modular-architecture.md | A prompt to help design a modular architecture, which is central to deciding between a new repository or a directory structure. | 0.95
adr-new.md | A prompt to draft an Architecture Decision Record (ADR) to formally document the pros, cons, and consequences of the repo vs. directory choice. | 0.90
planning-process.md | A prompt to guide the high-level project planning process, where the repository structure is a critical initial decision. | 0.80
reference-implementation.md | A prompt to request a reference implementation, which can be used to analyze how similar, well-structured projects are organized. | 0.75
scaffold-fullstack.md | A prompt to generate the file and directory structure for a new project after the initial architectural decisions have been made. | 0.70
file-modularity.md | A prompt focused on the granular task of splitting large files, a more granular aspect of the same architectural concern. | 0.65


--- .taskmaster/docs/owner-docs/TODO/RPG-DSPY/.archive/RPG-DSPy-Tutorial-Selection.md ---
**Recommended Tutorials for RPG-Aligned DSPy Program**

**Alignment:** Supports background doc sections 2 (RPG typing), 3–4 (Planner/Builder execution), and 5 (Orchestration) by linking tutorial modules to RPG node classes and tool adapters.

- **Planning & retrieval patterns**: Use `Build AI Programs with DSPy` for reusable modular patterns and combine with `Agents`, `RAG`, and `Multi-Hop Retrieval` to mirror graph-based capability expansion through ReAct, retrieval routing, and iterative traversal that matches RPG node growth. `.codefetch_dspyTutorial/DSPY-Tutorials_codebase.md:3156` `.codefetch_dspyTutorial/DSPY-Tutorials_codebase.md:171` `.codefetch_dspyTutorial/DSPY-Tutorials_codebase.md:17396` `.codefetch_dspyTutorial/DSPY-Tutorials_codebase.md:14808` `.codefetch_dspyTutorial/DSPY-Tutorials_codebase.md:20939`
- **Module customization & shared protocols**: Follow `Custom Module` to encode RPG abstractions as composable DSPy modules with MLflow traces, and `Use MCP in DSPy` when you need the RPG tool to interoperate with external planning or knowledge services via Model Context Protocol. `.codefetch_dspyTutorial/DSPY-Tutorials_codebase.md:4800` `.codefetch_dspyTutorial/DSPY-Tutorials_codebase.md:14056`
- **State, caching, and runtime control**: Apply `Use and Customize DSPy Cache`, `Saving and Loading`, `Async DSPy Programming`, `Streaming`, and `Deploying your DSPy program` to manage performance, persistence, concurrency, streaming feedback, and serving of RPG-driven builds. `.codefetch_dspyTutorial/DSPY-Tutorials_codebase.md:3257` `.codefetch_dspyTutorial/DSPY-Tutorials_codebase.md:20299` `.codefetch_dspyTutorial/DSPY-Tutorials_codebase.md:3016` `.codefetch_dspyTutorial/DSPY-Tutorials_codebase.md:20443` `.codefetch_dspyTutorial/DSPY-Tutorials_codebase.md:6283`
- **Observability & optimization**: Integrate `Debugging & Observability` and `Tracking DSPy Optimizers` so every RPG execution path is traced, measurable, and tied to optimizer runs. `.codefetch_dspyTutorial/DSPY-Tutorials_codebase.md:16063` `.codefetch_dspyTutorial/DSPY-Tutorials_codebase.md:16339`
- **Quality control loops**: Use `Output Refinement: BestOfN and Refine` to give the RPG agent deterministic reranking and refinement cycles when generating or revising graph segments. `.codefetch_dspyTutorial/DSPY-Tutorials_codebase.md:16481`

**Interface Annotations for RPG Binding**

| Tutorial | Key DSPy Module / Tooling | Signature & Fields | RPG Capability Binding |
| --- | --- | --- | --- |
| Build AI Programs with DSPy | `dspy.Predict`, `dspy.ChainOfThought`, composition patterns | `question -> answer`, composite submodules | Map to RPG L1 capability nodes defining base reasoning flows and shared service contracts. |
| Agents | `dspy.ReAct` orchestrator, ColBERT-based tools | `claim -> titles: list[str]`, tool inputs `query`, `title` | Encodes graph nodes for multi-hop retrieval planner; edges capture tool invocation order. |
| RAG | `dspy.RetrieveThenRead`, custom retriever wrappers | `question -> answer` with retrieval slots | Binds to capability nodes representing knowledge grounding; data edges for vector store queries. |
| Multi-Hop Retrieval | Custom `dspy.Module` with submodules | Composite signatures for `retrieve`, `reason`, `answer` | Models hierarchical RPG subgraphs where child nodes resolve intermediate hops. |
| Advanced Tool Use | `dspy.SIMBA`, tool registry | Task-specific signatures using ToolHop dataset | Captures edges to external APIs; annotates retry policies and optimizer metadata. |
| Custom Module | Subclassed `dspy.Module` + MLflow tracing | Arbitrary signatures defined per capability | Defines template for generating RPG L2 nodes (files/classes) with trace hooks. |
| Use MCP in DSPy | MCP tool client wrappers | Tool schemas (e.g., `book_itinerary(request)`) | Tags nodes requiring external protocol adapters; edges document MCP message flow. |
| DSPy Cache | `dspy.configure_cache`, cache hooks | N/A (configuration) | Annotates infrastructure capabilities for caching layers attached to graph resources. |
| Saving & Loading | Program persistence APIs | `save(path, save_program=...)` | Adds lifecycle edges for serialization nodes and deployment hand-offs. |
| Async / Streaming / Deployment | `dspy.asyncify`, `dspy.streamify`, FastAPI wrapper | Async `acall`, streaming listeners, REST schemas | Marks runtime capability nodes addressing throughput, streaming, and serving endpoints. |
| Debugging & Observability | `dspy.inspect_history`, MLflow tracing callbacks | Diagnostics functions, `mlflow.dspy.autolog()` | Attaches monitoring edges and logging contracts to graph nodes. |
| Tracking DSPy Optimizers | MLflow optimizer tracing | Optimizer config fields, metrics outputs | Specifies evaluation nodes and success metrics for capability validation. |
| Output Refinement | `dspy.BestOfN`, `dspy.Refine` | `reward_fn(args, pred)` callbacks | Associates iterative improvement loops with graph nodes requiring quality gating. |

    **Suggested Next Step**
    - Prototype a thin slice by implementing one RPG capability with the    `Custom Module` pattern, back it with `RAG` retrieval, and instrument  the flow using the observability stack before scaling to the full    graph. `.codefetch_dspyTutorial/DSPY-Tutorials_codebase.md:4800` `.    codefetch_dspyTutorial/DSPY-Tutorials_codebase.md:17396` `. codefetch_dspyTutorial/DSPY-Tutorials_codebase.md:16063`


--- rpg_tool/quick-reference-execution-guide.md ---
Here’s a quick-reference you can use to jump to the right place in our two sources for **each step of the execution order**. For each item I list the **Build Guide instruction (procedural)** and the **Whitepaper anchor (concept/conceptual)** as recorded in the guide’s Trace Matrix/“Source” fields.

- **4 — DSC-001 · Develop Project Spec Parser** → **Build Guide:** DSC-001 (Discover). **Whitepaper:** ¶2.
- **1 — DSC-002 · Define RPG Schema** → **Build Guide:** DSC-002. **Whitepaper:** ¶6.
- **2 — Extend Graph Service (typed nodes/edges)** → **Build Guide:** under DSC-002 (schema adoption) and IMPL-003 (typed data_flow semantics). **Whitepaper:** ¶6/¶12 (nodes/edges, DAG).
- **6 — DSC-003 · Feature Tree Index** → **Build Guide:** DSC-003. **Whitepaper:** ¶7 (global feature tree).
- **7 — PROP-001 · Exploit Retrieval (top-k)** → **Build Guide:** PROP-001. **Whitepaper:** ¶8 (proposal-stage retrieval).
- **8 — PROP-002 · Explore Sampling (diversity)** → **Build Guide:** PROP-002. **Whitepaper:** ¶8.
- **9 — PROP-003 · LLM Filter & Merge** → **Build Guide:** PROP-003. **Whitepaper:** ¶8 (batch self-check).
- **10 — PROP-004 · Refactor into Cohesive Modules** → **Build Guide:** PROP-004. **Whitepaper:** ¶9 (reorganization into functionality graph).
- **11 — PROP-005 · Persist L0 Functionality Graph** → **Build Guide:** PROP-005. **Whitepaper:** ¶4 (persistence/versioning motivation).
- **5 — IMPL-001 · Map nodes→folders (L0→L1)** → **Build Guide:** IMPL-001. **Whitepaper:** ¶10.
- **12 — IMPL-002 · Group capabilities into files (L1→L2)** → **Build Guide:** IMPL-002. **Whitepaper:** ¶11.
- **13 — IMPL-003 · Encode typed data-flows** → **Build Guide:** IMPL-003. **Whitepaper:** ¶12 (DAG/acyclicity).
- **26 — MON-001 · CI gate: DAG & schema** → **Build Guide:** MON-001. **Whitepaper:** ¶12.
- **14 — IMPL-004 · Intra-module file order (define)** → **Build Guide:** IMPL-004. **Whitepaper:** ¶6 (order edges).
- **30 — IMPL-004 · Intra-module order lints** → **Build Guide:** IMPL-004 (linting follow-on). **Whitepaper:** ¶6.
- **15 — IMPL-005 · Abstract base classes & shared types** → **Build Guide:** IMPL-005. **Whitepaper:** ¶13.
- **16 — IMPL-006 · One interface per leaf** → **Build Guide:** IMPL-006. **Whitepaper:** ¶14.
- **3 — GEN-001 · Topological build plan** → **Build Guide:** GEN-001. **Whitepaper:** ¶15 (graph-guided TDD/topo).
- **27 — MON-002 · Audit logging of localization steps** → **Build Guide:** MON-002. **Whitepaper:** C.3 (audit logs/tool calls).
- **17 — GEN-002 · Auto-derive unit tests** → **Build Guide:** GEN-002. **Whitepaper:** ¶15.
- **18 — GEN-003 · Implement-to-green & coverage gate** → **Build Guide:** GEN-003. **Whitepaper:** ¶15.
- **19 — GEN-004 · Selective regression by impact** → **Build Guide:** GEN-004. **Whitepaper:** ¶17 (staged testing/regression).
- **20 — GEN-005 · Integration tests per subgraph** → **Build Guide:** GEN-005. **Whitepaper:** ¶17.
- **21 — VAL-001 · Majority-vote semantic validator** → **Build Guide:** Validation list; mapped via Trace Matrix. **Whitepaper:** D.3/D.3.1 (vote-based judging).
- **22 — VAL-002 · Failure attribution & auto-remediation** → **Build Guide:** VAL-002. **Whitepaper:** ¶17 (diagnosis in staged loop).
- **23 — VAL-003 · RepoCraft metrics (Coverage/Novelty/Pass/Vote/Files/LOC/Tokens)** → **Build Guide:** VAL-003. **Whitepaper:** ¶19 (metrics), §5/Appendix D references in checklist.
- **24 — BMR-001 · Build benchmark task set** → **Build Guide:** BMR-001. **Whitepaper:** D.2 (task harvesting).
- **25 — BMR-002 · Evaluation loop per task** → **Build Guide:** BMR-002. **Whitepaper:** D.3 (localize → vote → execute).
- **28 — MON-003 · Governance: SBOM, licenses, secrets** → **Build Guide:** MON-003 (governance). **Whitepaper:** (policy/gov motivation noted; no specific section call-out).
- **29 — MON-004 · Scaling dashboards** → **Build Guide:** MON-004. **Whitepaper:** §7.1/§7.2 (near-linear growth & scaling).


--- docs/RAG-module.md ---
Here’s what I’d line up from DSPy before you start building the “analyze project → rank prompts” flow. I’ll keep it tight and only call out pieces that measurably help your use case.

**1) Use the right DSPy modules for each phase**

* **`Predict` vs `ChainOfThought`**: use `Predict` for short, deterministic fields (e.g., 1-sentence prompt descriptions) and keep `ChainOfThought` only where reasoning helps (final re-rank rationale). This cuts tokens and makes outputs steadier than CoT everywhere.
* **`Refine`** (you already use it): reuse it for the *final* table narrative if you want the model to polish borderline ties or add brief “why chosen” notes.

**2) Make scoring learnable (small supervision)**

* **`BootstrapFewShot` / `Teleprompter`**: create a tiny set of (query, project-summary, prompt, expected_score) examples. Use this to tune a lightweight scoring signature so your re-ranker learns what “relevance in *this* repo” means (e.g., prioritize Python/CLI prompts over unrelated ones). This gives you repeatable rankings instead of hand-tuned heuristics.

**3) Add a simple evaluation harness early**

* **`dspy.evaluate`**: define a metric like Spearman or pairwise accuracy on “A outranks B” judgments from a handful of gold cases. Wire a one-command eval so you can A/B: (BM25 only) vs (BM25→LM rerank) vs (BM25→LM rerank + Teleprompter). This prevents regressions as you tweak thresholds.

**4) Retrieval-first design (then LM rerank)**

* Treat prompt files as a tiny corpus:

  * **Stage 1: lexical**: BM25/TF-IDF to get top-k by `(query + project_summary)`.
  * **Stage 2: LM re-rank**: a `Predict` signature that outputs a bounded float [0,1] and a 1-line rationale. Keep temperature≈0 and `n=1`.
  * If you later want embeddings, keep them outside the LM loop and only use DSPy to re-rank/justify.

**5) Structured outputs + resilience**

* Keep your current coercion helpers, but tighten the interfaces with **narrow OutputFields** (e.g., `score: float`, `desc: str`). Have the model emit *only* the needed fields; you already have good fallbacks—keep temps low to reduce parsing fallback frequency.

**6) Caching, cost, and determinism**

* Lean on DSPy’s LM config knobs (you already pass `temperature`, `top_p`, `n`). For ranking, set `temperature=0`, `top_p=None`, `n=None`.
* Keep **disk cache** on for repeatable local tests (you’ve prepped env vars in tests). That speeds iteration when tuning the re-ranker.

**7) Tracing & debugging**

* Enable DSPy’s run logs (stdout is fine) around the re-rank step so you can quickly see per-prompt scores + rationales when a table looks “off.” This is invaluable for building your gold eval set.

**8) Guardrail the table renderer**

* After the re-rank, do the *table construction in Python* (not the LM). Ask the LM for `(desc, score)` only; you compute `round(score, 2)`, filter by threshold, sort, and render. This prevents formatting drift and ensures exact acceptance criteria.

---

## One workable plan (end-to-end)

1. **Summarize project:** select a small, fixed set of files (README, pyproject/package.json, top N by extension) and pass those through a **`Predict`** summary signature (low-temp).
2. **Retrieve candidates:** BM25/TF-IDF over prompt files using `query + project_summary`.
3. **LM re-rank:** For top-k, call a **`Predict`** signature that returns `{desc:str, score:float in [0,1]}`.
4. **Render & enforce:** In Python, sort by `score`, filter by threshold, and emit the markdown table or fallback line.
5. **Evaluate:** Add `dspy.evaluate` with 5–10 gold queries; compare variants and keep the config that wins.
6. **(Optional) Learn scoring:** Use **`BootstrapFewShot/Teleprompter`** on your gold to improve the re-ranker’s signature.

That set will make the behavior sturdy, testable, and cheap—while keeping the LM focused on what it’s good at (concise descriptions and judgment), not filesystem work.


--- rpg_tool/RPG_build_guide.md ---
RPG_REPOSITORY_PLANNING_GRAPH.pdf

A) Human-readable Plan (Markdown)
=================================

**Overview.** This plan turns the _RPG: Repository Planning Graph_ whitepaper into a deduplicated, provider-agnostic, step-by-step build guide. It covers: (1) defining the RPG schema; (2) proposal- and implementation-level graph construction; (3) graph-guided, test-driven code generation; and (4) evaluation/monitoring (RepoCraft-style). Assumptions: Python repos, any LLM, any vector DB/graph DB, any CI. RPG_REPOSITORY_PLANNING_GRAPH

**Primary Purpose assumed:** _Operationalize RPG/ZeroRepo to generate full repositories from scratch using a unified planning graph and graph-guided TDD._ RPG_REPOSITORY_PLANNING_GRAPH

**Audience/Actors assumed:** Research Engineer, Software Architect, QA/Testing, PM, DevOps.
**Constraints assumed:** provider-agnostic, offline-friendly, reproducible, acyclic data-flows, test-gated commits, license-safe inputs.
**Preferred Taxonomy used:** **Discover → Design → Build → Validate → Benchmark → Monitor**.
**Priority Policy:** P0 blocker (must), P1 important, P2 nice-to-have.

* * *

Instruction List (grouped by taxonomy)
--------------------------------------

### Discover

- **ID:** DSC-001
    **Topic:** Discover
    **Instruction:** Define <PROJECT\_SPEC> with goals, scope, constraints, and acceptance metrics.
    **Acceptance Criteria:** Doc includes <GOALS>, <NON\_GOALS>, <CONSTRAINTS>, <METRICS>; reviewed by <STAKEHOLDERS>.
    **Dependencies:** —
    **Priority:** P0
    **Risks & Mitigations:** Ambiguity → add examples and anti-requirements.
    **Source:** ¶2 — “generating complete repositories… demands coherent… planning.” RPG_REPOSITORY_PLANNING_GRAPH
    **Tool Suggestions:** markdown repo template

- **ID:** DSC-002
    **Topic:** Discover
    **Instruction:** Specify RPG schema with <NODE\_TYPES>, <EDGE\_TYPES>, required fields, and DAG constraint.
    **Acceptance Criteria:** JSON Schema covers nodes {capability, folder, file, class, function} and edges {hierarchy, data\_flow, order}; validates acyclicity.
    **Dependencies:** DSC-001
    **Priority:** P0
    **Risks & Mitigations:** Over-generalization → start minimal; iterate.
    **Source:** ¶6 — “nodes specify… files, classes, and functions… edges encode… data flows… order.” RPG_REPOSITORY_PLANNING_GRAPH
    **Tool Suggestions:** generic schema validator; graph DB

- **ID:** DSC-003
    **Topic:** Discover
    **Instruction:** Assemble <FEATURE\_TREE\_SOURCE> and embed nodes into <VECTOR\_DB>.
    **Acceptance Criteria:** ≥1M features loaded or configured; embeddings stored with path metadata; latency < <EMBED\_QPS>.
    **Dependencies:** DSC-001
    **Priority:** P0
    **Risks & Mitigations:** Bias/skew → record level distribution; add sampling.
    **Source:** ¶7 — “Feature Tree… 1.5M capabilities… embedded… vector database.” RPG_REPOSITORY_PLANNING_GRAPH
    **Tool Suggestions:** any embedding model; FAISS/pgvector/milvus


* * *

### Design (Proposal-level)

- **ID:** PROP-001
    **Topic:** Proposal Design
    **Instruction:** Retrieve top-k feature paths for <PROJECT\_SPEC> into <CANDIDATES\_EXPLOIT>.
    **Acceptance Criteria:** Query returns k paths with similarity scores and full hierarchical paths.
    **Dependencies:** DSC-003
    **Priority:** P0
    **Risks & Mitigations:** Narrow recall → expand synonyms.
    **Source:** ¶8 — “retrieve top-k feature paths… aligned with the user goal.” RPG_REPOSITORY_PLANNING_GRAPH
    **Tool Suggestions:** vector search

- **ID:** PROP-002
    **Topic:** Proposal Design
    **Instruction:** Sample unvisited ontology regions into <CANDIDATES\_EXPLORE>.
    **Acceptance Criteria:** At least <N\_EXPL> diverse paths not overlapping existing subtree (> <RHO> threshold).
    **Dependencies:** DSC-003
    **Priority:** P0
    **Risks & Mitigations:** Irrelevance → diversity-aware rejection sampling.
    **Source:** ¶8/¶21 — “exploration ensures diversity… rejection mechanism.” RPG_REPOSITORY_PLANNING_GRAPH
    **Tool Suggestions:** weighted sampler

- **ID:** PROP-003
    **Topic:** Proposal Design
    **Instruction:** Filter, rank, and merge candidates using <LLM\_FILTER\_PROMPT>.
    **Acceptance Criteria:** Merged set deduped; rationale captured per path; audit log stored.
    **Dependencies:** PROP-001, PROP-002
    **Priority:** P0
    **Risks & Mitigations:** LLM drift → batch self-check.
    **Source:** ¶8/Algorithm 2 — “LLM… filter… self-checks… insert paths.” RPG_REPOSITORY_PLANNING_GRAPH
    **Tool Suggestions:** any LLM; prompt templatizer

- **ID:** PROP-004
    **Topic:** Proposal Design
    **Instruction:** Refactor subtree into modules maximizing cohesion, minimizing coupling.
    **Acceptance Criteria:** Modules have clear boundaries; taxonomy justification per move recorded.
    **Dependencies:** PROP-003
    **Priority:** P0
    **Risks & Mitigations:** Over-fragmentation → merge pass.
    **Source:** ¶9 — “partitions… into cohesive modules… cohesion and coupling.” RPG_REPOSITORY_PLANNING_GRAPH
    **Tool Suggestions:** graph partitioner

- **ID:** PROP-005
    **Topic:** Proposal Design
    **Instruction:** Persist the functionality graph as RPG layer L0 with version <RPG\_VER>.
    **Acceptance Criteria:** L0 saved; checksum; diffable; CI loads without errors.
    **Dependencies:** PROP-004
    **Priority:** P0
    **Risks & Mitigations:** Schema drift → add migration script.
    **Source:** ¶4 — “persistent and evolvable representation.” RPG_REPOSITORY_PLANNING_GRAPH
    **Tool Suggestions:** git LFS; graph DB


* * *

### Design (Implementation-level)

- **ID:** IMPL-001
    **Topic:** Implementation Design
    **Instruction:** Map L0 modules to folder layout under <ROOT\_DIR>/src as L1 folder nodes.
    **Acceptance Criteria:** Each module ↔ one folder; mapping table with subtree names; path lint passes.
    **Dependencies:** PROP-005
    **Priority:** P0
    **Risks & Mitigations:** Over-nesting → cap depth.
    **Source:** ¶10 — “assigning each subgraph a dedicated directory namespace.” RPG_REPOSITORY_PLANNING_GRAPH
    **Tool Suggestions:** code scaffolder

- **ID:** IMPL-002
    **Topic:** Implementation Design
    **Instruction:** Group intermediate nodes into files; create L2 file nodes.
    **Acceptance Criteria:** Files contain cohesive features; naming per PEP8; ≤ <MAX\_FILES\_PER\_DIR> unless subfolders added.
    **Dependencies:** IMPL-001
    **Priority:** P0
    **Risks & Mitigations:** Cross-file coupling → regroup.
    **Source:** ¶11 — “assigning files to intermediate nodes… preserve semantic cohesion.” RPG_REPOSITORY_PLANNING_GRAPH
    **Tool Suggestions:** file planner

- **ID:** IMPL-003
    **Topic:** Implementation Design
    **Instruction:** Encode inter-module data-flow edges with typed I/O; ensure DAG.
    **Acceptance Criteria:** Edges include {from,to,data\_id,data\_type,transformation}; cycle check passes.
    **Dependencies:** IMPL-001
    **Priority:** P0
    **Risks & Mitigations:** Hidden cycles → topological check in CI.
    **Source:** ¶12 — “typed input–output flows… impose a topological order.” RPG_REPOSITORY_PLANNING_GRAPH
    **Tool Suggestions:** DAG/graph validator

- **ID:** IMPL-004
    **Topic:** Implementation Design
    **Instruction:** Define intra-module file order edges for build/load constraints.
    **Acceptance Criteria:** Order edges exist for each module; no contradictions; linter validates.
    **Dependencies:** IMPL-002
    **Priority:** P1
    **Risks & Mitigations:** Brittle imports → lazy load.
    **Source:** ¶6/¶12 — “intra-module file ordering… precedes…” RPG_REPOSITORY_PLANNING_GRAPH
    **Tool Suggestions:** import graph analyzer

- **ID:** IMPL-005
    **Topic:** Implementation Design
    **Instruction:** Abstract shared patterns into <BASE\_CLASSES> and shared data structures.
    **Acceptance Criteria:** Base types defined only for recurring patterns; adoption rate ≥ <ADOPTION\_RATE>.
    **Dependencies:** IMPL-003
    **Priority:** P1
    **Risks & Mitigations:** Premature abstraction → justify with usage.
    **Source:** ¶13 — “abstract… base classes… enforce interface consistency.” RPG_REPOSITORY_PLANNING_GRAPH
    **Tool Suggestions:** UML generator

- **ID:** IMPL-006
    **Topic:** Implementation Design
    **Instruction:** For each leaf feature, design exactly one interface (func or class) with docstring.
    **Acceptance Criteria:** One interface/leaf; docstrings include args/returns; no bodies (pass).
    **Dependencies:** IMPL-002, IMPL-003, IMPL-005
    **Priority:** P0
    **Risks & Mitigations:** Over-classing → prefer functions for stateless ops.
    **Source:** ¶14 & Appendix B.1 — “adaptive mapping… functions vs classes; prompt template.” RPG_REPOSITORY_PLANNING_GRAPH
    **Tool Suggestions:** interface generator


* * *

### Build (Graph-guided TDD)

- **ID:** GEN-001
    **Topic:** Build
    **Instruction:** Topologically sort RPG and plan build queue <BUILD\_QUEUE>.
    **Acceptance Criteria:** Queue respects all edges; snapshot stored; reproducible order.
    **Dependencies:** IMPL-003, IMPL-004
    **Priority:** P0
    **Risks & Mitigations:** Nondeterminism → seed & persist order.
    **Source:** ¶15 — “traverses the RPG in topological order.” RPG_REPOSITORY_PLANNING_GRAPH
    **Tool Suggestions:** topo-sort lib

- **ID:** GEN-002
    **Topic:** Build
    **Instruction:** For each leaf, auto-derive a unit test from its interface/spec.
    **Acceptance Criteria:** Test covers normal & edge inputs; executable via <TEST\_RUNNER>.
    **Dependencies:** IMPL-006
    **Priority:** P0
    **Risks & Mitigations:** Weak tests → mutation testing.
    **Source:** ¶15 — “test-driven development… test is derived… validated.” RPG_REPOSITORY_PLANNING_GRAPH
    **Tool Suggestions:** test generator

- **ID:** GEN-003
    **Topic:** Build
    **Instruction:** Implement code to pass the unit test; commit only on green.
    **Acceptance Criteria:** Test passes locally and in CI; code style OK; coverage ≥ <MIN\_COV>.
    **Dependencies:** GEN-002
    **Priority:** P0
    **Risks & Mitigations:** Flaky tests → rerun policy.
    **Source:** ¶15 — “Only functions that pass all tests are committed.” RPG_REPOSITORY_PLANNING_GRAPH
    **Tool Suggestions:** CI runner

- **ID:** GEN-004
    **Topic:** Build
    **Instruction:** On edits, run regression tests for impacted nodes and parents.
    **Acceptance Criteria:** Impact set derived from RPG edges; all prior tests green.
    **Dependencies:** GEN-003
    **Priority:** P0
    **Risks & Mitigations:** Large blast radius → selective test.
    **Source:** ¶17 — “Validated components trigger regression tests upon modification.” RPG_REPOSITORY_PLANNING_GRAPH
    **Tool Suggestions:** test impact analyzer

- **ID:** GEN-005
    **Topic:** Build
    **Instruction:** Create integration tests for completed subgraphs before release.
    **Acceptance Criteria:** Data-flows executed; contract checks pass; artifacts persisted.
    **Dependencies:** GEN-003
    **Priority:** P1
    **Risks & Mitigations:** Orchestration failures → synthetic fixtures.
    **Source:** ¶17 — “completed subgraphs undergo integration tests.” RPG_REPOSITORY_PLANNING_GRAPH
    **Tool Suggestions:** integration harness


* * *

### Validate

- **ID:** VAL-001
    **Topic:** Validation
    **Instruction:** Add LLM majority-vote semantic validator for algorithm presence.
    **Acceptance Criteria:** 5-vote majority result logged; confidence ≥ <VOTE\_THRESHOLD>.
    **Dependencies:** GEN-003
    **Priority:** P1
    **Risks & Mitigations:** Judge variance → temperature 0 & tie-break rules.
    **Source:** ¶17 & D.3 — “majority-vote checking… two rounds/5 votes.” RPG_REPOSITORY_PLANNING_GRAPH
    **Tool Suggestions:** evaluator LLM

- **ID:** VAL-002
    **Topic:** Validation
    **Instruction:** Implement failure attribution: implementation vs test/env; auto-remediate latter.
    **Acceptance Criteria:** Errors labeled; env/test issues retried ≤ <N\_REMEDIATIONS>; logs kept.
    **Dependencies:** GEN-003
    **Priority:** P1
    **Risks & Mitigations:** Mislabeling → add heuristics & review.
    **Source:** ¶17 — “majority-vote diagnosis… handle environment or test issues.” RPG_REPOSITORY_PLANNING_GRAPH
    **Tool Suggestions:** log analyzer

- **ID:** VAL-003
    **Topic:** Validation
    **Instruction:** Compute Coverage, Novelty, Pass Rate, Voting Rate, Files, LOC, Tokens.
    **Acceptance Criteria:** Metrics computed per formulas (D.3.1); stored in <METRICS\_STORE>.
    **Dependencies:** PROP-005, GEN-003
    **Priority:** P0
    **Risks & Mitigations:** Category drift → fixed centroids + human audit.
    **Source:** ¶19 & D.3.1 — “Coverage… Novelty… Pass/Voting… Code-level statistics.” RPG_REPOSITORY_PLANNING_GRAPH
    **Tool Suggestions:** metrics job


* * *

### Benchmark

- **ID:** BMR-001
    **Topic:** Benchmark
    **Instruction:** Build task set by parsing reference tests into <TASKS> with NL descriptions.
    **Acceptance Criteria:** > <N\_TASKS> tasks with input/outputs; trivial cases filtered.
    **Dependencies:** DSC-001
    **Priority:** P1
    **Risks & Mitigations:** Leakage → anonymize names.
    **Source:** ¶18 & D.2 — “derive tasks from reference repositories… filter trivial.” RPG_REPOSITORY_PLANNING_GRAPH
    **Tool Suggestions:** test parser

- **ID:** BMR-002
    **Topic:** Benchmark
    **Instruction:** Run localization → validation → execution loop per task.
    **Acceptance Criteria:** For each task: candidate found or absence recorded; test adapted; result stored.
    **Dependencies:** BMR-001, VAL-001
    **Priority:** P1
    **Risks & Mitigations:** False negatives → retry budget.
    **Source:** D.3 — “Stage 1 localization… Stage 2 voting… Stage 3 execution.” RPG_REPOSITORY_PLANNING_GRAPH
    **Tool Suggestions:** harness runner


* * *

### Monitor & Operate

- **ID:** MON-001
    **Topic:** Monitor
    **Instruction:** Enforce DAG acyclicity and schema validation in CI on every RPG change.
    **Acceptance Criteria:** CI blocks cycles/schema violations; report lists offending edges/nodes.
    **Dependencies:** DSC-002, IMPL-003
    **Priority:** P0
    **Risks & Mitigations:** Slow checks → cache results.
    **Source:** ¶12 — “impose a topological order… DAG.” RPG_REPOSITORY_PLANNING_GRAPH
    **Tool Suggestions:** CI job

- **ID:** MON-002
    **Topic:** Monitor
    **Instruction:** Log localization steps and tool calls for auditability.
    **Acceptance Criteria:** Each step recorded with inputs/outputs; PII scrubbed; replayable.
    **Dependencies:** GEN-001
    **Priority:** P2
    **Risks & Mitigations:** Verbose logs → sampling.
    **Source:** C.3 — “representative logs… tool invocations… termination results.” RPG_REPOSITORY_PLANNING_GRAPH
    **Tool Suggestions:** structured logging

- **ID:** MON-003
    **Topic:** Governance
    **Instruction:** Add license and security checks for generated code and dependencies.
    **Acceptance Criteria:** SBOM created; license policy enforced; secrets scan clean.
    **Dependencies:** GEN-003
    **Priority:** P0
    **Risks & Mitigations:** False positives → allowlist with review.
    **Source:** (general compliance risk implied by large-scale gen) RPG_REPOSITORY_PLANNING_GRAPH
    **Tool Suggestions:** SBOM, license scanner

- **ID:** MON-004
    **Topic:** Monitor
    **Instruction:** Track scaling metrics over iterations (features, LOC) to detect stagnation.
    **Acceptance Criteria:** Plots show near-linear growth or flag plateau; regression alerts.
    **Dependencies:** VAL-003
    **Priority:** P2
    **Risks & Mitigations:** Metric gaming → cross-check with pass rate.
    **Source:** ¶7.1/7.2 — “near-linear growth of functionalities and LOC.” RPG_REPOSITORY_PLANNING_GRAPH
    **Tool Suggestions:** metrics dashboard


* * *

Trace Matrix (Source Paragraph → Instruction IDs)
-------------------------------------------------

- **¶2** → DSC-001

- **¶4** → PROP-005

- **¶6** → DSC-002, IMPL-004

- **¶7** → DSC-003

- **¶8** → PROP-001, PROP-002, PROP-003

- **¶9** → PROP-004

- **¶10** → IMPL-001

- **¶11** → IMPL-002

- **¶12** → IMPL-003, MON-001

- **¶13** → IMPL-005

- **¶14** → IMPL-006

- **¶15** → GEN-001, GEN-002, GEN-003

- **¶17** → GEN-004, GEN-005, VAL-002

- **¶18** → BMR-001

- **¶19** → VAL-003

- **D.2/D.3** → BMR-001, BMR-002, VAL-001

- **C.3** → MON-002

- **7.1/7.2** → MON-004


> Key phrases (quoted ≤20 words) appear under each instruction’s **Source** field.

* * *

B) Machine-readable JSON
========================

    {
      "instruction_pack": {
        "metadata": {
          "primary_purpose": "Operationalize RPG/ZeroRepo for unified, scalable repository generation via a persistent planning graph and graph-guided TDD.",
          "generated_at": "2025-09-30T00:00:00Z",
          "language": "English",
          "taxonomy_used": "Discover → Design → Build → Validate → Benchmark → Monitor"
        },
        "instructions": [
          {
            "id": "DSC-001",
            "topic": "Discover",
            "instruction": "Define <PROJECT_SPEC> with goals, scope, constraints, and acceptance metrics.",
            "acceptance_criteria": [
              "Doc includes <GOALS>, <NON_GOALS>, <CONSTRAINTS>, <METRICS>",
              "Reviewed by <STAKEHOLDERS>"
            ],
            "dependencies": [],
            "priority": "P0",
            "risks": ["Ambiguity in scope"],
            "mitigations": ["Add examples, anti-requirements"],
            "tool_suggestions": ["markdown template"],
            "source": {
              "paragraph_indexes": [2],
              "key_phrase": "generating complete repositories… demands coherent and reliable planning"
            },
            "tags": ["planning","inputs"]
          },
          {
            "id": "DSC-002",
            "topic": "Discover",
            "instruction": "Specify RPG schema with <NODE_TYPES>, <EDGE_TYPES>, required fields, and DAG constraint.",
            "acceptance_criteria": [
              "JSON Schema validates nodes and edges",
              "Acyclicity check available"
            ],
            "dependencies": ["DSC-001"],
            "priority": "P0",
            "risks": ["Over-general schema"],
            "mitigations": ["Start minimal; iterate"],
            "tool_suggestions": ["schema validator","graph DB"],
            "source": {
              "paragraph_indexes": [6],
              "key_phrase": "nodes specify… files, classes… edges encode… data flows… ordering"
            },
            "tags": ["rpg","schema"]
          },
          {
            "id": "DSC-003",
            "topic": "Discover",
            "instruction": "Assemble <FEATURE_TREE_SOURCE> and embed nodes into <VECTOR_DB>.",
            "acceptance_criteria": [
              "Embeddings stored with path metadata",
              "Query latency < <EMBED_QPS>"
            ],
            "dependencies": ["DSC-001"],
            "priority": "P0",
            "risks": ["Ontology skew"],
            "mitigations": ["Diversity-aware sampling"],
            "tool_suggestions": ["embedding model","vector DB"],
            "source": {
              "paragraph_indexes": [7],
              "key_phrase": "large-scale ontology… nodes embedded… vector database"
            },
            "tags": ["kb","retrieval"]
          },
          {
            "id": "PROP-001",
            "topic": "Proposal Design",
            "instruction": "Retrieve top-k feature paths for <PROJECT_SPEC> into <CANDIDATES_EXPLOIT>.",
            "acceptance_criteria": [
              "k paths with scores and full hierarchical paths"
            ],
            "dependencies": ["DSC-003"],
            "priority": "P0",
            "risks": ["Narrow recall"],
            "mitigations": ["Expand synonyms"],
            "tool_suggestions": ["vector search"],
            "source": {
              "paragraph_indexes": [8],
              "key_phrase": "retrieve top-k feature paths most aligned with the user goal"
            },
            "tags": ["retrieval","exploit"]
          },
          {
            "id": "PROP-002",
            "topic": "Proposal Design",
            "instruction": "Sample unvisited ontology regions into <CANDIDATES_EXPLORE>.",
            "acceptance_criteria": [
              "≥ <N_EXPL> non-overlapping candidate paths"
            ],
            "dependencies": ["DSC-003"],
            "priority": "P0",
            "risks": ["Low precision"],
            "mitigations": ["Diversity-aware rejection sampling"],
            "tool_suggestions": ["sampler"],
            "source": {
              "paragraph_indexes": [8,21],
              "key_phrase": "exploration ensures diversity… diversity-aware rejection mechanism"
            },
            "tags": ["retrieval","explore"]
          },
          {
            "id": "PROP-003",
            "topic": "Proposal Design",
            "instruction": "Filter, rank, and merge candidates using <LLM_FILTER_PROMPT>.",
            "acceptance_criteria": [
              "Merged set deduped with rationales",
              "Batch self-check logs stored"
            ],
            "dependencies": ["PROP-001","PROP-002"],
            "priority": "P0",
            "risks": ["LLM drift"],
            "mitigations": ["Batch self-checks"],
            "tool_suggestions": ["LLM"],
            "source": {
              "paragraph_indexes": [8],
              "key_phrase": "LLM… filter candidates… self-check… insert paths"
            },
            "tags": ["filter","ranking"]
          },
          {
            "id": "PROP-004",
            "topic": "Proposal Design",
            "instruction": "Refactor subtree into modules maximizing cohesion and minimizing coupling.",
            "acceptance_criteria": [
              "Modules have boundary definitions",
              "Move/merge justifications recorded"
            ],
            "dependencies": ["PROP-003"],
            "priority": "P0",
            "risks": ["Over-fragmentation"],
            "mitigations": ["Merge pass"],
            "tool_suggestions": ["graph partitioner"],
            "source": {
              "paragraph_indexes": [9],
              "key_phrase": "partitions functionalities into cohesive modules… cohesion and coupling"
            },
            "tags": ["modularity"]
          },
          {
            "id": "PROP-005",
            "topic": "Proposal Design",
            "instruction": "Persist functionality graph as RPG layer L0 with version <RPG_VER>.",
            "acceptance_criteria": [
              "L0 saved and diffable",
              "Checksum recorded"
            ],
            "dependencies": ["PROP-004"],
            "priority": "P0",
            "risks": ["Schema drift"],
            "mitigations": ["Migration script"],
            "tool_suggestions": ["git","graph DB"],
            "source": {
              "paragraph_indexes": [4],
              "key_phrase": "persistent and evolvable representation that unifies planning"
            },
            "tags": ["versioning"]
          },
          {
            "id": "IMPL-001",
            "topic": "Implementation Design",
            "instruction": "Map L0 modules to folder layout under <ROOT_DIR>/src as L1 folder nodes.",
            "acceptance_criteria": [
              "One folder per module",
              "Mapping table created; lints pass"
            ],
            "dependencies": ["PROP-005"],
            "priority": "P0",
            "risks": ["Over-nesting"],
            "mitigations": ["Cap depth"],
            "tool_suggestions": ["scaffolder"],
            "source": {
              "paragraph_indexes": [10],
              "key_phrase": "assign each subgraph a dedicated directory namespace"
            },
            "tags": ["folders"]
          },
          {
            "id": "IMPL-002",
            "topic": "Implementation Design",
            "instruction": "Group intermediate nodes into files to create L2 file nodes.",
            "acceptance_criteria": [
              "Files are cohesive",
              "PEP8 naming; directory size ≤ <MAX_FILES_PER_DIR>"
            ],
            "dependencies": ["IMPL-001"],
            "priority": "P0",
            "risks": ["Cross-file coupling"],
            "mitigations": ["Regroup features"],
            "tool_suggestions": ["file planner"],
            "source": {
              "paragraph_indexes": [11],
              "key_phrase": "assigning files to intermediate nodes… preserve semantic cohesion"
            },
            "tags": ["files"]
          },
          {
            "id": "IMPL-003",
            "topic": "Implementation Design",
            "instruction": "Encode inter-module data-flow edges with typed inputs and outputs.",
            "acceptance_criteria": [
              "Edges have from,to,data_id,data_type,transformation",
              "DAG validation passes"
            ],
            "dependencies": ["IMPL-001"],
            "priority": "P0",
            "risks": ["Hidden cycles"],
            "mitigations": ["Topological checks"],
            "tool_suggestions": ["DAG validator"],
            "source": {
              "paragraph_indexes": [12],
              "key_phrase": "typed input–output flows connect subgraph roots"
            },
            "tags": ["dataflow"]
          },
          {
            "id": "IMPL-004",
            "topic": "Implementation Design",
            "instruction": "Define intra-module file ordering edges for build/load constraints.",
            "acceptance_criteria": [
              "Order edges exist and have no contradictions"
            ],
            "dependencies": ["IMPL-002"],
            "priority": "P1",
            "risks": ["Brittle imports"],
            "mitigations": ["Lazy loading"],
            "tool_suggestions": ["import graph analyzer"],
            "source": {
              "paragraph_indexes": [6,12],
              "key_phrase": "intra-module file ordering… precedes… outputs propagated"
            },
            "tags": ["ordering"]
          },
          {
            "id": "IMPL-005",
            "topic": "Implementation Design",
            "instruction": "Define <BASE_CLASSES> and shared data structures for recurring patterns.",
            "acceptance_criteria": [
              "Base types justified by reuse",
              "Adoption rate ≥ <ADOPTION_RATE>"
            ],
            "dependencies": ["IMPL-003"],
            "priority": "P1",
            "risks": ["Premature abstraction"],
            "mitigations": ["Usage-driven creation"],
            "tool_suggestions": ["UML"],
            "source": {
              "paragraph_indexes": [13],
              "key_phrase": "abstracted into common data structures or base classes"
            },
            "tags": ["abstractions"]
          },
          {
            "id": "IMPL-006",
            "topic": "Implementation Design",
            "instruction": "Design one interface per leaf feature with docstring; choose func or class.",
            "acceptance_criteria": [
              "Exactly one interface/leaf",
              "Docstring has purpose, args, returns"
            ],
            "dependencies": ["IMPL-002","IMPL-003","IMPL-005"],
            "priority": "P0",
            "risks": ["Over-classing"],
            "mitigations": ["Prefer functions unless stateful"],
            "tool_suggestions": ["interface generator"],
            "source": {
              "paragraph_indexes": [14],
              "key_phrase": "independent features as functions… interdependent… shared classes"
            },
            "tags": ["interfaces"]
          },
          {
            "id": "GEN-001",
            "topic": "Build",
            "instruction": "Topologically sort RPG and create reproducible <BUILD_QUEUE>.",
            "acceptance_criteria": [
              "Queue respects edges",
              "Order persisted and reproducible"
            ],
            "dependencies": ["IMPL-003","IMPL-004"],
            "priority": "P0",
            "risks": ["Nondeterministic order"],
            "mitigations": ["Seed and persist"],
            "tool_suggestions": ["topo sort"],
            "source": {
              "paragraph_indexes": [15],
              "key_phrase": "traverses the RPG in topological order"
            },
            "tags": ["build-plan"]
          },
          {
            "id": "GEN-002",
            "topic": "Build",
            "instruction": "Auto-derive a unit test from each interface/spec before implementation.",
            "acceptance_criteria": [
              "Test executes in <TEST_RUNNER>",
              "Covers normal and edge cases"
            ],
            "dependencies": ["IMPL-006"],
            "priority": "P0",
            "risks": ["Weak tests"],
            "mitigations": ["Mutation testing"],
            "tool_suggestions": ["test generator"],
            "source": {
              "paragraph_indexes": [15],
              "key_phrase": "test-driven development… a test is derived"
            },
            "tags": ["tdd","tests"]
          },
          {
            "id": "GEN-003",
            "topic": "Build",
            "instruction": "Implement code to pass the test; commit only on green.",
            "acceptance_criteria": [
              "Unit test green locally and in CI",
              "Coverage ≥ <MIN_COV>"
            ],
            "dependencies": ["GEN-002"],
            "priority": "P0",
            "risks": ["Flaky tests"],
            "mitigations": ["Rerun policy"],
            "tool_suggestions": ["CI"],
            "source": {
              "paragraph_indexes": [15],
              "key_phrase": "Only functions that pass all tests are committed"
            },
            "tags": ["tdd","commit-policy"]
          },
          {
            "id": "GEN-004",
            "topic": "Build",
            "instruction": "Run regression tests for impacted nodes when code changes.",
            "acceptance_criteria": [
              "Impact set from graph edges",
              "All prior tests green"
            ],
            "dependencies": ["GEN-003"],
            "priority": "P0",
            "risks": ["Large impact sets"],
            "mitigations": ["Selective testing"],
            "tool_suggestions": ["test impact analyzer"],
            "source": {
              "paragraph_indexes": [17],
              "key_phrase": "Validated components trigger regression tests upon modification"
            },
            "tags": ["regression"]
          },
          {
            "id": "GEN-005",
            "topic": "Build",
            "instruction": "Create integration tests per completed subgraph before release.",
            "acceptance_criteria": [
              "Data flows executed end-to-end",
              "Contracts validated"
            ],
            "dependencies": ["GEN-003"],
            "priority": "P1",
            "risks": ["Complex orchestration"],
            "mitigations": ["Synthetic fixtures"],
            "tool_suggestions": ["integration harness"],
            "source": {
              "paragraph_indexes": [17],
              "key_phrase": "completed subgraphs undergo integration tests"
            },
            "tags": ["integration"]
          },
          {
            "id": "VAL-001",
            "topic": "Validation",
            "instruction": "Use majority-vote LLM to validate algorithm presence semantically.",
            "acceptance_criteria": [
              "5-vote majority with confidence ≥ <VOTE_THRESHOLD>"
            ],
            "dependencies": ["GEN-003"],
            "priority": "P1",
            "risks": ["Judge variance"],
            "mitigations": ["Temp=0, tie-break"],
            "tool_suggestions": ["LLM judge"],
            "source": {
              "paragraph_indexes": [17],
              "key_phrase": "majority-vote checking is applied"
            },
            "tags": ["evaluation","semantics"]
          },
          {
            "id": "VAL-002",
            "topic": "Validation",
            "instruction": "Attribute failures to code vs test/env; auto-remediate test/env cases.",
            "acceptance_criteria": [
              "Errors labeled; ≤ <N_REMEDIATIONS> auto fixes attempted"
            ],
            "dependencies": ["GEN-003"],
            "priority": "P1",
            "risks": ["Misattribution"],
            "mitigations": ["Heuristics + review"],
            "tool_suggestions": ["log analyzer"],
            "source": {
              "paragraph_indexes": [17],
              "key_phrase": "distinguishes… and automatically handling the latter"
            },
            "tags": ["triage"]
          },
          {
            "id": "VAL-003",
            "topic": "Validation",
            "instruction": "Compute Coverage, Novelty, Pass Rate, Voting Rate, Files, LOC, Tokens.",
            "acceptance_criteria": [
              "Formulas per spec implemented",
              "Results in <METRICS_STORE>"
            ],
            "dependencies": ["PROP-005","GEN-003"],
            "priority": "P0",
            "risks": ["Category drift"],
            "mitigations": ["Fixed centroids + audit"],
            "tool_suggestions": ["metrics job"],
            "source": {
              "paragraph_indexes": [19],
              "key_phrase": "Coverage… Novelty… Pass Rate… Voting Rate… code statistics"
            },
            "tags": ["metrics"]
          },
          {
            "id": "BMR-001",
            "topic": "Benchmark",
            "instruction": "Parse reference tests into <TASKS> with NL descriptions; filter trivial.",
            "acceptance_criteria": [
              "> <N_TASKS> tasks with inputs/outputs",
              "Non-algorithmic tests removed"
            ],
            "dependencies": ["DSC-001"],
            "priority": "P1",
            "risks": ["Leakage"],
            "mitigations": ["Anonymize names"],
            "tool_suggestions": ["test parser"],
            "source": {
              "paragraph_indexes": [18],
              "key_phrase": "derive tasks from reference repositories… filter out trivial"
            },
            "tags": ["benchmark","tasks"]
          },
          {
            "id": "BMR-002",
            "topic": "Benchmark",
            "instruction": "Run localization → validation → execution loop for each task.",
            "acceptance_criteria": [
              "Candidate localized or absence recorded",
              "Adapted test executed and logged"
            ],
            "dependencies": ["BMR-001","VAL-001"],
            "priority": "P1",
            "risks": ["False negatives"],
            "mitigations": ["Retry budget"],
            "tool_suggestions": ["harness runner"],
            "source": {
              "paragraph_indexes": [18],
              "key_phrase": "localization… validation… execution testing… verify correctness"
            },
            "tags": ["evaluation-loop"]
          },
          {
            "id": "MON-001",
            "topic": "Monitor",
            "instruction": "Validate DAG acyclicity and schema in CI on every RPG change.",
            "acceptance_criteria": [
              "CI blocks cycles and schema violations",
              "Error report lists offending edges"
            ],
            "dependencies": ["DSC-002","IMPL-003"],
            "priority": "P0",
            "risks": ["Slow checks"],
            "mitigations": ["Caching"],
            "tool_suggestions": ["CI job"],
            "source": {
              "paragraph_indexes": [12],
              "key_phrase": "topological order… acyclic structure"
            },
            "tags": ["ci","governance"]
          },
          {
            "id": "MON-002",
            "topic": "Monitor",
            "instruction": "Record localization tool steps and outcomes for audit.",
            "acceptance_criteria": [
              "Structured logs per step",
              "PII scrubbed; replayable"
            ],
            "dependencies": ["GEN-001"],
            "priority": "P2",
            "risks": ["Excessive volume"],
            "mitigations": ["Sampling"],
            "tool_suggestions": ["structured logging"],
            "source": {
              "paragraph_indexes": [28],
              "key_phrase": "logs… demonstrate the end-to-end process… tool invocations"
            },
            "tags": ["audit"]
          },
          {
            "id": "MON-003",
            "topic": "Governance",
            "instruction": "Run license and security checks on generated code and dependencies.",
            "acceptance_criteria": [
              "SBOM created",
              "Licenses compliant; no secrets"
            ],
            "dependencies": ["GEN-003"],
            "priority": "P0",
            "risks": ["False positives"],
            "mitigations": ["Allowlist with review"],
            "tool_suggestions": ["SBOM","license scanner","secret scanner"],
            "source": {
              "paragraph_indexes": [1],
              "key_phrase": "scalable repository generation raises practical governance needs"
            },
            "tags": ["compliance","security"]
          },
          {
            "id": "MON-004",
            "topic": "Monitor",
            "instruction": "Track feature and LOC scaling over iterations; alert on stagnation.",
            "acceptance_criteria": [
              "Dashboards for features and LOC",
              "Alert when slope < <MIN_SLOPE>"
            ],
            "dependencies": ["VAL-003"],
            "priority": "P2",
            "risks": ["Metric gaming"],
            "mitigations": ["Correlate with pass rate"],
            "tool_suggestions": ["dashboard"],
            "source": {
              "paragraph_indexes": [29],
              "key_phrase": "near-linear growth of functionalities and repository size"
            },
            "tags": ["scaling","monitoring"]
          }
        ],
        "trace_matrix": [
          { "paragraph_index": 1, "instruction_ids": ["MON-003"] },
          { "paragraph_index": 2, "instruction_ids": ["DSC-001"] },
          { "paragraph_index": 4, "instruction_ids": ["PROP-005"] },
          { "paragraph_index": 6, "instruction_ids": ["DSC-002","IMPL-004"] },
          { "paragraph_index": 7, "instruction_ids": ["DSC-003"] },
          { "paragraph_index": 8, "instruction_ids": ["PROP-001","PROP-002","PROP-003"] },
          { "paragraph_index": 9, "instruction_ids": ["PROP-004"] },
          { "paragraph_index": 10, "instruction_ids": ["IMPL-001"] },
          { "paragraph_index": 11, "instruction_ids": ["IMPL-002"] },
          { "paragraph_index": 12, "instruction_ids": ["IMPL-003","MON-001"] },
          { "paragraph_index": 13, "instruction_ids": ["IMPL-005"] },
          { "paragraph_index": 14, "instruction_ids": ["IMPL-006"] },
          { "paragraph_index": 15, "instruction_ids": ["GEN-001","GEN-002","GEN-003"] },
          { "paragraph_index": 17, "instruction_ids": ["GEN-004","GEN-005","VAL-002"] },
          { "paragraph_index": 18, "instruction_ids": ["BMR-001","BMR-002"] },
          { "paragraph_index": 19, "instruction_ids": ["VAL-003"] },
          { "paragraph_index": 28, "instruction_ids": ["MON-002"] },
          { "paragraph_index": 29, "instruction_ids": ["MON-004"] }
        ]
      }
    }


_Notes:_ IDs are unique and single-action; placeholders use ASCII angle brackets; risks/mitigations and tool suggestions are provider-agnostic. All items trace back to whitepaper passages (paragraph indices with ≤20-word key phrases). RPG_REPOSITORY_PLANNING_GRAPH

---



--- rpg_tool/.AI_DOCS/checklist.md ---
# 1. Introduction — scope & artifacts

*Map to paper §1.*

- State the tool’s purpose: “Turn ambiguous NL specs into a persistent Repository Planning Graph (RPG) that drives scalable repo generation.” Record this as a README-like scope note.
- Declare primary artifacts and IDs: RPG store, feature tree index, generation logs, test corpus, metrics dashboard.
- Define success criteria you’ll compute later: functional coverage, novelty, pass/vote accuracy, and code scale.

# 2. Related Work — baseline inventory

*Map to paper §2.*

- Catalog comparator systems (multi-agent, workflow, terminal agents) and fix their configurations for reproducibility.
- Write a one-pager on what RPG contributes beyond natural-language planning (structure, persistence, data-flow).

# 3. Repository Planning Graph Construction — end-to-end contract

*Map to paper §3.*

- Establish a single canonical RPG store (transactional) with versioning and snapshots.
- Install validators you must pass before any codegen: DAG acyclicity, required fields, dependency closure, module boundary integrity. Gate codegen on “all green.”

## 3.1 RPG Structure — schema & invariants

*Map to paper §3.1.*

- Define node schema (capability, folder, file, class, function) with dual semantics and required fields: `id, kind, path/name, signature, inputs, outputs, deps, tests, status`.
- Define edge types: (a) hierarchical containment, (b) inter-module typed data-flows, (c) intra-module file-order edges.
- Implement topological ordering over all executable leaves; persist topo index.

## 3.2 Proposal-Level Construction — functionality graph

*Map to paper §3.2 and Appx A.*

- Connect to a large Feature Tree (vector DB with path metadata). Implement exploit (top-k retrieval) + explore (unvisited sampling) loop; include diversity-aware rejection sampling.
- Refactor retrieved paths into cohesive modules (maximize cohesion, minimize coupling); reject duplicates, resolve overlaps, and eliminate orphans.
- Emit a **Functionality Graph** artifact with module notes and gap list.

## 3.3 Implementation-Level Construction — from plan to RPG

*Map to paper §3.3.*

### 3.3.1 File Structure Encoding — skeleton

*Map to paper §3.3.1.*

- Map modules → folders; mid-level capabilities → files; define naming rules.
- Emit a **skeleton diff** (+folders, +files) with module↔folder mapping.
- Re-run validators: file-path consistency; no stray modules.

### 3.3.2 Data-Flow & Functions Encoding — interfaces

*Map to paper §3.3.2.*

- Add inter-module typed I/O edges and intra-module order constraints; regenerate topo order.
- Abstract recurring patterns into base classes/shared types; record inheritance and contracts.
- For each leaf feature, choose class vs function; specify signatures, typed inputs/outputs, and docstrings; link to upstream/downstream contracts.

# 4. Graph-Guided Code Generation — TDD + localization

*Map to paper §4 and Appx C.*

- Traverse leaves in topo order; for each node generate a unit test from its docstring & contracts, then implement minimal code to pass.
- If a test fails, localize with (i) RPG-guided fuzzy search, (ii) repo code view, (iii) dependency explorer; patch and retest; commit only on pass.
- Maintain regression and boundary integration tests; keep edit logs.

# 5. Experiment Setup — benchmark & metrics

*Map to paper §5, §5.1–§5.3.*

- **Benchmark selection**: fix target repos/categories; anonymize if needed. (§5.1.1)
- **Metrics implementation**: coverage (category hit ratio), novelty (OOD rate), accuracy (pass & vote), code scale (files, normalized LOC, tokens). (§5.1.2, D.3.1)
- **Task construction pipeline**: parse test trees, stratified sample, filter trivial tests, generate NL task specs + ground-truth tests. (§5.1.3, D.2)
- **Baselines**: script configurations and iteration budgets for multi-agent, workflow, and terminal agents. (§5.2)
- **Implementation details**: fix iteration counts, localization retries, voting rounds; log seeds and environment. (§5.3)

# 6. Main Results — reporting template

*Map to paper §6.*

- Prepare a standard table and plots comparing coverage, novelty, pass/vote, and scale vs baselines; include qualitative examples of complex dependencies induced by RPG.

# 7. Analysis — ablations & scaling

*Map to paper §7, §7.1–§7.3.*

- **Scalability**: track features/LOC vs iteration; verify near-linear trends; alert on stagnation. (§7.1)
- **Stability & innovation**: plot coverage and novelty across rounds; sample and audit novel features for coherence. (§7.2)
- **Localization efficiency**: ablate with/without RPG tools; report step counts and variance. (§7.3)

# 8. Conclusion — operational checklist close-out

*Map to paper §8.*

- Summarize whether RPG improved planning consistency, scaling, and localization efficiency in your runs; list next extensibility targets (new domains, larger trees).

# Appendices — implementable assets

*Map to paper Appx A–E.*

## A. Proposal-Level (Algorithms, Process, Prompts)

- Implement diversity-aware rejection sampling and exploit/explore subtree builder; expose knobs (temperature, overlap ρ, budgets). (Appx A.1–A.2)
- Package prompt templates for exploit/explore/missing-features with variables for project context; unit-test their JSON outputs. (Appx A.3)

## B. Implementation-Level (Prompts, Skeleton, Patterns)

- Provide prompt templates for: data-flow DAG design, raw skeleton mapping, subgraph→base class abstraction, and feature→interface mapping; lint outputs against validators. (Appx B.1)
- Store exemplar skeleton diffs and interface specs; extract recurrent patterns for reuse. (Appx B.2–B.3)

## C. Graph-Guided Generation (Localization, Editing, Testing)

- Ship a toolset: `view_file_interface_feature_map`, `get_interface_content`, `expand_leaf_node_info`, `search_interface_by_functionality`, plus structured **edit** operations (whole class, method, function, imports). (Appx C.1–C.2)
- Implement the staged testing pipeline: unit → regression → integration, with LLM-judged diagnosis and automated test/environment repair. (Appx C.4)
- Log complete localization & editing trajectories for audit. (Appx C.3, C.5)

## D. RepoCraft Details (Data, Pipeline, Taxonomy, Metrics)

- Reproduce repository selection & anonymization; persist dataset cards. (Appx D.1)
- Automate task harvesting from tests, hierarchical categorization, and sampling; export tasks as NL + tests. (Appx D.2)
- Implement the 3-stage evaluation agent (localize → vote → adapt/execute tests) and metric formulas. (Appx D.3, D.3.1)
- Load the ground-truth taxonomy for coverage/novelty calculation; add OOD centroid logic. (Appx D.4)

## E. Results (Configs, Detailed Tables, Examples)

- Freeze baseline configs; generate per-repo tables; include worked examples for coverage/novelty and localization behavior. (Appx E.1–E.4)

---

### Cross-cutting extensibility hooks

- **RPG I/O**: JSON/YAML export, migration scripts, and diffing between RPG versions.
- **Pluggable trees**: support alternative feature ontologies and hybrid retrieval (symbolic + vector).
- **Policy knobs**: gating rules (e.g., minimum validator set), test-first budgets, retry caps, novelty thresholds.
- **Dashboards**: CI panels for Coverage/Novelty/Pass/Vote/Scale with iteration timelines. (Maps to §5.1.2, §6–§7.)

> Use the section taxonomy above as your table of contents when you stand up the repo for this tooling so each checklist maps 1:1 to the paper’s “notions.”


--- .taskmaster/docs/prd.txt ---
# Overview
Repo Cartographer is a Python-based CLI and Textual TUI agent that helps engineers design, analyze, and iterate on repository structures using DSPy-guided reasoning and an RPG (Repository Planning Graph) layer for deterministic build sequencing.

# Core Features
- **Modular CLI and TUI Interfaces**
  - What: Deliver Typer-powered commands and a Textual front end for interactive repository planning sessions.
  - Why: Separate workflows accommodate both scriptable automation and immersive guided exploration without duplicating logic.
  - High-level How: Typer commands invoke core pipelines; the Textual app reuses the same services via async messaging and Rich-rendered outputs.
  - BDD:
    Given a user with repository metadata configured
    When they launch `repo-cartographer ui`
    Then the TUI loads panels for prompts, status, and notifications sourced from shared pipeline services.
- **DSPy Planning Programs**
  - What: Provide DSPy signatures, modules, and optimizers to evaluate repository changes, prioritize work, and generate structured plans.
  - Why: Declarative DSPy programs keep reasoning transparent, reproducible, and provider-agnostic across LLM backends.
  - High-level How: Configure the LM registry before constructing modules; compose Predict and ChainOfThought programs within `core/dspy_modules` and orchestrate them via pipeline facades.
  - BDD:
    Given DSPy is configured with credentials via environment variables
    When the user runs `repo-cartographer analyze path/to/repo`
    Then the CLI returns a ranked Markdown table of capabilities with rationales produced by the DSPy planning program.
- **Repository Planning Graph Engine**
  - What: Model repository capabilities, folders, and files as a DAG to validate structure and emit build queues.
  - Why: A formal graph ensures acyclic dependencies, enabling deterministic execution order and impact analysis.
  - High-level How: Pydantic schemas capture node and edge types; construct, validate, and topologically sort graphs through dedicated RPG modules consumed by CLI/TUI flows.
  - BDD:
    Given a proposed capability map imported into the RPG layer
    When validation executes
    Then the system either outputs a build queue or flags cycle violations with actionable messages.
- **Configurable Provider and Telemetry Layer**
  - What: Centralize provider configuration, caching, and execution tracing for DSPy programs and pipeline actions.
  - Why: Ensures consistent environment setup, debuggable runs, and auditability across CLI and TUI sessions.
  - High-level How: `core/providers/registry.py` reads env vars, applies adapters, and enables tracing; telemetry events persist to `data/` for later review.
  - BDD:
    Given tracing is enabled in configuration
    When any pipeline task completes
    Then a structured log entry appears in the data directory with provider, latency, and outcome fields.

# User Experience
Primary personas: systems architects standardizing multi-team repos, platform engineers automating project scaffolds, and senior ICs exploring refactors. Key flows include CLI-driven bulk planning, TUI exploratory sessions with live feedback, and report export for stakeholders. UI/UX emphasizes consistent command palettes, keyboard-first navigation, Rich-styled tables, and accessible contrast ratios with screen-reader labels on widgets. Accessibility commitments: semantic regions in the TUI, keyboard-only affordances, and descriptive alt text within generated summaries.

# Technical Architecture
Components: Typer CLI (`apps/cli`), Textual App (`apps/tui`), shared pipeline orchestrators (`core/pipeline`), DSPy programs (`core/dspy_modules`), provider registry (`core/providers`), RPG graph layer (`rpg`), and artifact store (`data`). Data models rely on Pydantic schemas defining capabilities, nodes, edges, and execution artifacts. APIs/integrations: DSPy LM providers via environment configuration, optional VCS integrations for repo scanning, and Rich/Textual for rendering. Infrastructure: Python project managed via uv, local storage under `~/.dspy-rpg/`, optional cloud logging adapters. NFRs: reproducible runs (seeded DSPy optimizers), determinism in graph outputs, performance targets under 3s for medium repos, resilience to provider timeouts with retries, and test coverage for CLI/TUI, pipeline, and RPG modules.

# Development Roadmap
MVP: Implement Typer CLI commands (`analyze`, `rank`, `queue`), stub Textual app with shared pipeline hooks, initial DSPy planning modules, and RPG validation with acceptance criteria that each command passes integration tests and emits deterministic outputs for sample repos. Future Enhancements: Add advanced Textual widgets (command palette, notifications), pluggable provider caching, collaborative plan exports, and optimizer tuning workflows with acceptance criteria that each enhancement includes automated tests and updated documentation demonstrating the new behavior.

# Logical Dependency Chain
Establish environment configuration and provider registry foundations → scaffold core pipeline and DSPy modules → implement RPG schemas and validation with tests → expose CLI commands that drive the pipeline → layer Textual TUI on validated services → add telemetry, caching, and export features for extensibility.

# Risks and Mitigations
- Description: LLM provider instability causing inconsistent results; Likelihood: Medium; Impact: High; Mitigation: support multiple providers, add retries, and persist compiled DSPy artifacts for fallback.
- Description: Textual UI complexity slowing delivery; Likelihood: Medium; Impact: Medium; Mitigation: start with minimal layout, reuse CLI outputs, and incrementally add widgets aligned with command coverage.
- Description: Graph schema drifts from actual repo structures; Likelihood: Low; Impact: High; Mitigation: enforce schema validation tests and provide migration scripts alongside changes.
- Description: Performance degradation on large repos; Likelihood: Medium; Impact: Medium; Mitigation: batch file introspection, cache analysis results, and profile DSPy runs to optimize bottlenecks.

# Appendix
- Assumptions:
  - Teams adopt uv-managed Python environments and have Typer/Textual familiarity.
  - Users can supply repository metadata or allow the tool to scan locally.
  - Environment variables provide necessary provider credentials without hardcoding secrets.
- Research findings from CLI agent structure plan.md:
  - Gemini CLI-inspired separation of CLI and core layers guides folder layout and extensibility decisions.
  - RPG layer delivers deterministic build order and validation for repository planning.
  - DSPy modules, Typer CLI, and Textual TUI are foundational technologies identified for the product.
- Context notes:
  - GitHub+1 — Gemini CLI architecture and CLI↔Core split inspiration.
  - Typer+1 — Environment variable configuration for command options.
  - Textual Documentation+3 — Widget patterns for layouts and command palettes.
  - Rich Documentation+1 — Styled terminal output techniques for CLI/TUI surfaces.
  - DSPy+1 — Guidance for configuring LM adapters and provider-agnostic pipelines.
- Technical specs:
  - Python 3.11 runtime with uv for dependency and script execution.
  - DSPy >= 2 with central LM configuration and tracing enabled.
  - Typer CLI entrypoint (`repo-cartographer`) plus Textual app packaged under `apps/tui`.
  - Pydantic-based schemas for RPG nodes, edges, and plan artifacts in `rpg/`.


--- rpg_tool/docs/PROJECT_SPEC.template.md ---
<!-- path: docs/PROJECT_SPEC.template.md -->
# <PROJECT_NAME> — Project Spec


> Fill the four mandatory sections. Keep each bullet concise (one idea per line). Use **Non‑Goals** to prevent scope creep.


## Goals

- <what outcomes we want>
- <primary user problems we solve>


## Non-Goals

- <explicitly out of scope>
- <things we won’t do>


## Constraints

- <technical or policy constraints>
- <time/cost/platform limits>


## Acceptance Metrics

- <measurable success criteria (e.g., pass rate ≥ X%, coverage ≥ Y%)>
- <launch gates or SLOs>


---


### Stakeholders & Review

- **Owner:** <name>
- **Stakeholders:** <name1>, <name2>
- **Review Status:** <pending | approved>


### Context (Optional)

- Link to docs, tickets, or prior art.


### Changelog (Optional)

- YYYY‑MM‑DD — <note>


--- .taskmaster/templates/example_prd.txt ---
<context>
# Overview  
[Provide a high-level overview of your product here. Explain what problem it solves, who it's for, and why it's valuable.]

# Core Features  
[List and describe the main features of your product. For each feature, include:
- What it does
- Why it's important
- How it works at a high level]

# User Experience  
[Describe the user journey and experience. Include:
- User personas
- Key user flows
- UI/UX considerations]
</context>
<PRD>
# Technical Architecture  
[Outline the technical implementation details:
- System components
- Data models
- APIs and integrations
- Infrastructure requirements]

# Development Roadmap  
[Break down the development process into phases:
- MVP requirements
- Future enhancements
- Do not think about timelines whatsoever -- all that matters is scope and detailing exactly what needs to be build in each phase so it can later be cut up into tasks]

# Logical Dependency Chain
[Define the logical order of development:
- Which features need to be built first (foundation)
- Getting as quickly as possible to something usable/visible front end that works
- Properly pacing and scoping each feature so it is atomic but can also be built upon and improved as development approaches]

# Risks and Mitigations  
[Identify potential risks and how they'll be addressed:
- Technical challenges
- Figuring out the MVP that we can build upon
- Resource constraints]

# Appendix  
[Include any additional information:
- Research findings
- Technical specifications]
</PRD>

--- rpg_tool/.taskmaster/templates/example_prd.txt ---
<context>
# Overview  
[Provide a high-level overview of your product here. Explain what problem it solves, who it's for, and why it's valuable.]

# Core Features  
[List and describe the main features of your product. For each feature, include:
- What it does
- Why it's important
- How it works at a high level]

# User Experience  
[Describe the user journey and experience. Include:
- User personas
- Key user flows
- UI/UX considerations]
</context>
<PRD>
# Technical Architecture  
[Outline the technical implementation details:
- System components
- Data models
- APIs and integrations
- Infrastructure requirements]

# Development Roadmap  
[Break down the development process into phases:
- MVP requirements
- Future enhancements
- Do not think about timelines whatsoever -- all that matters is scope and detailing exactly what needs to be build in each phase so it can later be cut up into tasks]

# Logical Dependency Chain
[Define the logical order of development:
- Which features need to be built first (foundation)
- Getting as quickly as possible to something usable/visible front end that works
- Properly pacing and scoping each feature so it is atomic but can also be built upon and improved as development approaches]

# Risks and Mitigations  
[Identify potential risks and how they'll be addressed:
- Technical challenges
- Figuring out the MVP that we can build upon
- Resource constraints]

# Appendix  
[Include any additional information:
- Research findings
- Technical specifications]
</PRD>

--- tests/test_dspy_modules.py ---


--- core/dspy_modules/analyze.py ---


--- core/dspy_modules/rank_prompts.py ---


--- core/dspy_modules/sequence.py ---


--- AGENTS.md ---
<!-- path: ~/projects/dspy-file/AGENTS.md -->

# AGENTS.md — Tool Selection (Python)

## DSPy Framework Playbook (All‑Purpose)

**When to apply:** Use these rules whenever creating or editing DSPy **Signatures**, **Modules**, **Programs**, **Optimizers**, **Metrics**, or **RAG** components in any repository.

**Canonical docs**

- Signatures: <https://dspy.ai/learn/programming/signatures/>
- Modules (Predict / ChainOfThought / ReAct): <https://dspy.ai/learn/programming/modules/>
- Optimizers overview: <https://dspy.ai/learn/optimization/optimizers/>
- MIPROv2: <https://dspy.ai/api/optimizers/MIPROv2/>
- BootstrapFewShot: <https://dspy.ai/api/optimizers/BootstrapFewShot/>
- Assertions: <https://dspy.ai/learn/programming/7-assertions/>
- Metrics: <https://dspy.ai/learn/evaluation/metrics/>
- RAG tutorial: <https://dspy.ai/tutorials/rag/>

### Quick posture

- **Program, don’t prompt.** Encode task instructions in **Signature docstrings**; compose behavior from Modules. Avoid ad‑hoc long prompts.
- **Provider‑agnostic.** Support OpenAI/Ollama/Anthropic/etc. by configuring the LM once via `dspy.settings.configure(lm=...)` before building modules.
- **Composable by default.** Prefer several small Modules over a single monolith; pass data via fields, not globals.
- **Repro first.** Pin DSPy and core deps in `pyproject.toml`; commit seeds and compile artifacts for deterministic rebuilds.

### Order of operations (universal)

1) **Define Signature(s)** — Put task instructions in the **docstring**; declare inputs with `dspy.InputField`, outputs with `dspy.OutputField`. (Docs: Signatures)
2) **Compose Modules** — Start with `dspy.Predict`; add `dspy.ChainOfThought` when rationale is useful; use `dspy.ReAct` for tools/agents. (Docs: Modules)
3) **Choose a Metric** — Make it concrete: e.g., `accuracy`/`F1` (classification), `nDCG@k` (ranking), exact/EM (QA). (Docs: Metrics)
4) **Add Assertions** — Specify schema+constraints and enable automatic retries on violation. (Docs: Assertions)
5) **(Optional) Retrieval** — Add a retriever and route evidence to the program for RAG tasks. (Docs: RAG tutorial)
6) **Optimize** — Run `BootstrapFewShot` to assemble demos → then `MIPROv2` to jointly tune instructions+demos against your metric. (Docs: Optimizers)

### Minimum rules (Codex must enforce)

- **Instruction location:** All task guidance lives in Signature docstrings; no hidden prompts in module code.
- **LM config timing:** Call `dspy.settings.configure(lm=...)` **before** constructing Modules/Programs so compiles/optimizers see the right LM.
- **Schema guard (example):** For ranked outputs, emit a 3‑column Markdown table: `prompt | score | rationale`, where `score` ∈ `[0,1]`.
- **Metrics defaults:** Classification → `accuracy` or `F1`; Ranking → `nDCG@k`; QA → exact match / token‑F1. Document any custom metric in one line.
- **Tracing & artifacts:** Enable tracing during iteration; persist optimized programs/artifacts after `compile()` for reproducibility.
- **Small pieces, loosely joined:** Prefer many tiny units; inject dependencies; avoid singletons and global state.

### Environment & compatibility

- **Versioning:** Prefer `dspy>=2` (or latest stable); if upgrading major versions, re‑run optimizers and refresh assertions.
- **Providers:** Keep provider/model/api‑base in env (e.g., `DSPY_OPENAI_API_KEY`, `OPENAI_API_BASE`, etc.). Do not hardcode keys.
- **Eval data:** Keep small, labeled eval sets under `eval/` to make metrics and compilation meaningful.

### Long‑form guide (for humans)

See **docs/pre-work-dspy.md** for the expanded checklist and examples
-------

When you need to call tools from the shell, use this rubric:

File & Text

-------

- Find files by file name: `fd`

- Find files with path name: `fd -p <file-path>`

- List files in a directory: `fd . <directory>`

- Find files with extension and pattern: `fd -e <extension> <pattern>`

- Find Text: `rg` (ripgrep)

- Find Code Structure: `ast-grep`

  - Common languages:

    - Python → `ast-grep --lang python -p '<pattern>'`

    - TypeScript → `ast-grep --lang ts -p '<pattern>'`

    - Bash → `ast-grep --lang bash -p '<pattern>'`

    - TSX (React) → `ast-grep --lang tsx -p '<pattern>'`

    - JavaScript → `ast-grep --lang js -p '<pattern>'`

    - Rust → `ast-grep --lang rust -p '<pattern>'`

    - JSON → `ast-grep --lang json -p '<pattern>'`

  - Prefer `ast-grep` over ripgrep/grep unless a plain-text search is explicitly requested.

- Select among matches: pipe to `fzf`

Data
----

- JSON: `jq`

- YAML/XML: `yq`

Python Tooling
--------------

- Package Management & Virtual Envs: `uv`
    (fast replacement for pip/pip-tools/virtualenv; use `uv pip install ...`, `uv run ...`)

- Linting & Formatting: `ruff`
    (linter + formatter; use `ruff check .`, `ruff format .`)

- Static Typing: `mypy`
    (type checking; use `mypy .`)

- Security: `bandit`
    (Python security linter; use `bandit -r .`)

- Testing: `pytest`
    (test runner; use `pytest -q`, `pytest -k <pattern>` to filter tests)

- Logging: `loguru`
    (runtime logging utility; import in code:)

        from loguru import logger
        logger.info("message")

Notes
-----

- Prefer `uv` for Python dependency and environment management instead of pip/venv/poetry/pip-tools.

MCP\_SERVERS
------------

- Use the `dspy_Docs` MCP server to get latest docs for DSPy usage.

- Use the `lmstudio_docs` MCP server to get latest docs for LM Studio API usage.

-------

## Proactive TODO/FIXME Annotations

Add TODO/FIXME notes as you work—don’t wait for a cleanup pass. Use them to mark: missing tests, unclear contracts, temporary workarounds, performance/security concerns, or places where design choices need follow-up.

**Format (single line):**

    TODO(scope|owner): short, imperative next step — why it matters [evidence: <source|cmd|ticket>]
    FIXME(scope|owner): what is broken — minimal repro or constraint [evidence: <source|cmd|ticket>]

- `scope|owner` is optional but encouraged (e.g., `ui`, `backend`, `deps`, or a handle like `@alice`).

- Keep it ≤120 chars when possible; link to issues for details.

**Examples (per language comment style):**

    # TODO(domain|@alice): replace naive parse with streaming parser — OOM on large inputs [evidence: profile.txt]
    # FIXME(api): 500 on empty payload — add validation + test [evidence: pytest -k empty_payload]


    // TODO(ui): debounce search — noisy network on fast typing [evidence: trace.log]
    // FIXME(auth|@bob): refresh token race — guard with mutex [evidence: unit test 'refresh-concurrency']


    # TODO(devex): switch to uv task for one-liners [evidence: uv run --help]

### Workflow (aligned with `todos.md`)

1. Gather evidence with the command you used during investigation and reference it in the note’s `[evidence: ...]`.

2. Add the TODO/FIXME in the code at the closest actionable location.

3. Commit with a concise message (e.g., `chore(todos): mark debounce + auth race with evidence`).

4. Before opening a PR, **find and group** all annotations as described in `todos.md` so maintainers can review them together.

### Discover & verify (standard commands)

- Plain-text sweep:

```bash
        rg -n "TODO|FIXME" | fzf
```

- Syntax-aware matches (prefer this when patterns are noisy):

```bash
        ast-grep --lang python -p "// TODO(_) (_) : (_)"
        ast-grep --lang ts -p "// FIXME(_) : (_)"
```

- File targeting:

```bash
        fd -e py -e ts | xargs rg -n "TODO|FIXME"
```

### PR checklist (copy into your PR template)

- Added TODO/FIXME where follow-ups are needed, with `[evidence: ...]`.

- Ran `rg`/`ast-grep` to list all annotations and grouped them per `todos.md` for reviewers.

- Linked or opened issues for any TODO expected to live >2 sprints.

### Retirement policy

- Convert TODO → issue if it will outlive the current PR.

- Remove the annotation when addressed; reference the commit/issue that resolves it.

-------

Rules for Best-Practice

-------

<file\_length\_and\_structure>

- Prefer maintainability signals over fixed line caps.

- Split when cognitive complexity > 15, cohesion drops, or fan-in/out spikes.

- Group by feature. Keep a file to one capability plus its close helpers.

- Use clear folder names and consistent naming.

</file\_length\_and\_structure>

<paradigm\_and\_style>

- Use OOP, functional, or data-oriented styles as idiomatic for the language.

- Favor composition. In OOP, model behavior behind small interfaces or protocols.

- Prefer pure functions and algebraic data types where natural.

</paradigm\_and\_style>

<single\_responsibility\_principle>

- Aim for one capability and its close helpers. Avoid micro-files.

- Enforce through module boundaries and public APIs, not line counts.

</single\_responsibility\_principle>

<modular\_design>

- Design modules to be interchangeable, testable, and isolated.

- Keep public surfaces small. Inject dependencies. Avoid tight coupling.

- Optimize for replaceability and test seams over premature reuse.

</modular\_design>

<roles\_by\_platform>

- UI stacks: ViewModel for UI logic, Manager for business logic, Coordinator for navigation and state flow.

- Backend and CLI: Service, Handler, Repository, Job, Workflow.

- Do not mix view code with business logic.

</roles\_by\_platform>

<function\_and\_class\_size>

- Size by behavior, not lines.

- Functions ≤ 20–30 cognitive steps.

- Split a class when it owns more than one lifecycle or more than one external dependency graph.

</function\_and\_class\_size>

<naming\_and\_readability>

- Use intention revealing names.

- Allow domain terms with qualifiers, for example `UserData`, `BillingInfo`.

- Forbid empty suffixes like `Helper` or `Utils` unless tightly scoped.

</naming\_and\_readability>

<scalability\_mindset>

- Build for extension points from day one, such as interfaces, protocols, and constructor injection.

- Prefer local duplication over unstable abstractions.

- Document contracts at module seams.

</scalability\_mindset>

<avoid\_god\_classes>

- Do not centralize everything in one file or class.

- Split into UI, State, Handlers, Networking, and other focused parts.

</avoid\_god\_classes>

<dependency\_injection>

- Backends: prefer constructor injection. Keep containers optional.

- Swift, Kotlin, TypeScript: use protocols or interfaces. Inject by initializer or factory.

- Limit global singletons. Provide test doubles at seams.

</dependency\_injection>

<testing>

- Require deterministic seams.

- Add contract tests for modules and layers.

- Use snapshot or golden tests for UI and renderers.

</testing>

<architecture\_boundaries>

- Feature oriented packaging with clear dependency direction: UI → app → domain → infra.

- Stabilize domain modules. Keep infra replaceable.

- Enforce imports with rules or module maps.

</architecture\_boundaries>

-------


--- README.md ---
# Repo Cartographer — CLI Agent Structure Plan

This README distills the structure and workflow decisions captured in `CLI agent structure plan.md`, outlining how to grow a Gemini-style CLI/TUI around DSPy programs and an RPG (Repository Planning Graph) layer.

## Goals

- Separate command-line UX from core orchestration so each surface (Typer CLI, Textual TUI) evolves independently.
- Centralize DSPy configuration and modules to keep model providers swappable by environment.
- Introduce an RPG layer that plans repository scaffolding, validates topology, and feeds both UX surfaces.
- Keep the repo scalable, testable, and ready for extension commands or additional tools.

## Architectural Overview

- **CLI Surface (`apps/cli`)**: Typer-powered commands (`analyze`, `rank-prompts`, `sequence`, `ui`). Uses Rich for terminal output, honors environment variable fallbacks via Typer option metadata.
- **TUI Surface (`apps/tui`)**: Textual application with header, prompt line, footer keybinds, command palette, and toast-style notifications. Talks to core pipeline through async-friendly façades.
- **Core Package (`core`)**: Home for DSPy signatures/modules, provider registry, pipelines, and shared tooling (filesystem, web, configuration helpers).
- **RPG Package (`rpg`)**: Defines graph schemas, construction logic, validation routines, and topological build queues that translate planning insights into actionable steps.
- **Shared Assets**: `prompts/` (prompt templates), `data/` (reports/logs), and `tests/` (unit + contract coverage for each layer).

## Repository Layout

```
repo-cartographer/
├─ apps/
│  ├─ cli/
│  │  └─ main.py
│  └─ tui/
│     ├─ app.py
│     └─ widgets/
├─ core/
│  ├─ dspy_modules/
│  ├─ providers/
│  ├─ pipeline/
│  ├─ tools/
│  └─ config/
├─ rpg/
│  ├─ schema.py
│  ├─ construct.py
│  ├─ validate.py
│  └─ build_queue.py
├─ prompts/
├─ data/
└─ tests/
```

Each folder owns a single responsibility: UX apps call into `core.pipeline`, which coordinates DSPy programs and RPG planning utilities without leaking implementation details across layers.

## Key Components

- **`core/providers/registry.py`** configures `dspy.settings.configure(...)` once, selects adapters (Chat/JSON), and exposes provider-agnostic LM factories.
- **`core/dspy_modules/`** houses signatures and modules; docstrings encode instructions so optimizers (BootstrapFewShot → MIPROv2) can compile repeatable programs.
- **`core/pipeline/run_jobs.py`** supplies thin façades (`analyze_path`, `rank_prompts`, `sequence`) used by CLI and TUI.
- **`rpg/schema.py`** encodes node/edge types for capabilities, folders, files, and dependency links.
- **`rpg/construct.py`** converts feature level inputs (L0) into deeper structural graphs (L1/L2) via retrieval and merge routines.
- **`rpg/validate.py`** enforces acyclicity and type-safe IO contracts.
- **`rpg/build_queue.py`** produces deterministic build/test queues from the graph topology.
- **`apps/tui/widgets/`** provides modular panels, notifications, and command palette integration for Textual.

## Workflow Highlights

1. **Configure LM early**: Call `dspy.settings.configure(lm=...)` before module instantiation so compiles/optimizers inherit the correct provider and adapter.
2. **Program, don’t prompt**: Encode instructions in signature docstrings, compose small DSPy modules, and rely on assertions for schema enforcement.
3. **Expose env-driven CLI**: Typer options include `envvar=` bindings, so users can drive configuration via variables like `DSPY_OPENAI_API_KEY` without extra flags.
4. **Leverage Textual affordances**: Provide footer keybinds, command palette hooks, and rich notifications to ensure parity with CLI commands while adding discoverability.
5. **Run RPG validation**: Every plan produced by `rpg.construct` is validated and converted into topological queues before execution or display.

## Migration & Next Steps

1. Scaffold the directory layout above, moving existing business logic into `core/` and keeping entrypoints thin.
2. Implement Typer CLI commands that delegate to `core.pipeline` and accept environment-based configuration.
3. Build the Textual `App`, wiring widgets to pipeline calls and surfacing progress via notifications.
4. Stand up DSPy signatures/modules with supporting provider registry and assertions.
5. Implement the RPG schema → construct → validate → queue flow, then add contract tests under `tests/`.
6. Document env vars, command usage, and config locations in `docs/` or future appendices.

Following this plan keeps the repository ready for additional subcommands, provider integrations, and future GUI or automation surfaces without compromising maintainability.


--- prd.txt ---
# Overview
Repo Cartographer is a Python-based CLI and Textual TUI agent that helps engineers design, analyze, and iterate on repository structures using DSPy-guided reasoning and an RPG (Repository Planning Graph) layer for deterministic build sequencing.

# Core Features
- **Modular CLI and TUI Interfaces**
  - What: Deliver Typer-powered commands and a Textual front end for interactive repository planning sessions.
  - Why: Separate workflows accommodate both scriptable automation and immersive guided exploration without duplicating logic.
  - High-level How: Typer commands invoke core pipelines; the Textual app reuses the same services via async messaging and Rich-rendered outputs.
  - BDD:
    Given a user with repository metadata configured
    When they launch `repo-cartographer ui`
    Then the TUI loads panels for prompts, status, and notifications sourced from shared pipeline services.
- **DSPy Planning Programs**
  - What: Provide DSPy signatures, modules, and optimizers to evaluate repository changes, prioritize work, and generate structured plans.
  - Why: Declarative DSPy programs keep reasoning transparent, reproducible, and provider-agnostic across LLM backends.
  - High-level How: Configure the LM registry before constructing modules; compose Predict and ChainOfThought programs within `core/dspy_modules` and orchestrate them via pipeline facades.
  - BDD:
    Given DSPy is configured with credentials via environment variables
    When the user runs `repo-cartographer analyze path/to/repo`
    Then the CLI returns a ranked Markdown table of capabilities with rationales produced by the DSPy planning program.
- **Repository Planning Graph Engine**
  - What: Model repository capabilities, folders, and files as a DAG to validate structure and emit build queues.
  - Why: A formal graph ensures acyclic dependencies, enabling deterministic execution order and impact analysis.
  - High-level How: Pydantic schemas capture node and edge types; construct, validate, and topologically sort graphs through dedicated RPG modules consumed by CLI/TUI flows.
  - BDD:
    Given a proposed capability map imported into the RPG layer
    When validation executes
    Then the system either outputs a build queue or flags cycle violations with actionable messages.
- **Configurable Provider and Telemetry Layer**
  - What: Centralize provider configuration, caching, and execution tracing for DSPy programs and pipeline actions.
  - Why: Ensures consistent environment setup, debuggable runs, and auditability across CLI and TUI sessions.
  - High-level How: `core/providers/registry.py` reads env vars, applies adapters, and enables tracing; telemetry events persist to `data/` for later review.
  - BDD:
    Given tracing is enabled in configuration
    When any pipeline task completes
    Then a structured log entry appears in the data directory with provider, latency, and outcome fields.

# User Experience
Primary personas: systems architects standardizing multi-team repos, platform engineers automating project scaffolds, and senior ICs exploring refactors. Key flows include CLI-driven bulk planning, TUI exploratory sessions with live feedback, and report export for stakeholders. UI/UX emphasizes consistent command palettes, keyboard-first navigation, Rich-styled tables, and accessible contrast ratios with screen-reader labels on widgets. Accessibility commitments: semantic regions in the TUI, keyboard-only affordances, and descriptive alt text within generated summaries.

# Technical Architecture
Components: Typer CLI (`apps/cli`), Textual App (`apps/tui`), shared pipeline orchestrators (`core/pipeline`), DSPy programs (`core/dspy_modules`), provider registry (`core/providers`), RPG graph layer (`rpg`), and artifact store (`data`). Data models rely on Pydantic schemas defining capabilities, nodes, edges, and execution artifacts. APIs/integrations: DSPy LM providers via environment configuration, optional VCS integrations for repo scanning, and Rich/Textual for rendering. Infrastructure: Python project managed via uv, local storage under `~/.dspy-rpg/`, optional cloud logging adapters. NFRs: reproducible runs (seeded DSPy optimizers), determinism in graph outputs, performance targets under 3s for medium repos, resilience to provider timeouts with retries, and test coverage for CLI/TUI, pipeline, and RPG modules.

# Development Roadmap
MVP: Implement Typer CLI commands (`analyze`, `rank`, `queue`), stub Textual app with shared pipeline hooks, initial DSPy planning modules, and RPG validation with acceptance criteria that each command passes integration tests and emits deterministic outputs for sample repos. Future Enhancements: Add advanced Textual widgets (command palette, notifications), pluggable provider caching, collaborative plan exports, and optimizer tuning workflows with acceptance criteria that each enhancement includes automated tests and updated documentation demonstrating the new behavior.

# Logical Dependency Chain
Establish environment configuration and provider registry foundations → scaffold core pipeline and DSPy modules → implement RPG schemas and validation with tests → expose CLI commands that drive the pipeline → layer Textual TUI on validated services → add telemetry, caching, and export features for extensibility.

# Risks and Mitigations
- Description: LLM provider instability causing inconsistent results; Likelihood: Medium; Impact: High; Mitigation: support multiple providers, add retries, and persist compiled DSPy artifacts for fallback.
- Description: Textual UI complexity slowing delivery; Likelihood: Medium; Impact: Medium; Mitigation: start with minimal layout, reuse CLI outputs, and incrementally add widgets aligned with command coverage.
- Description: Graph schema drifts from actual repo structures; Likelihood: Low; Impact: High; Mitigation: enforce schema validation tests and provide migration scripts alongside changes.
- Description: Performance degradation on large repos; Likelihood: Medium; Impact: Medium; Mitigation: batch file introspection, cache analysis results, and profile DSPy runs to optimize bottlenecks.

# Appendix
- Assumptions:
  - Teams adopt uv-managed Python environments and have Typer/Textual familiarity.
  - Users can supply repository metadata or allow the tool to scan locally.
  - Environment variables provide necessary provider credentials without hardcoding secrets.
- Research findings from CLI agent structure plan.md:
  - Gemini CLI-inspired separation of CLI and core layers guides folder layout and extensibility decisions.
  - RPG layer delivers deterministic build order and validation for repository planning.
  - DSPy modules, Typer CLI, and Textual TUI are foundational technologies identified for the product.
- Context notes:
  - GitHub+1 — Gemini CLI architecture and CLI↔Core split inspiration.
  - Typer+1 — Environment variable configuration for command options.
  - Textual Documentation+3 — Widget patterns for layouts and command palettes.
  - Rich Documentation+1 — Styled terminal output techniques for CLI/TUI surfaces.
  - DSPy+1 — Guidance for configuring LM adapters and provider-agnostic pipelines.
- Technical specs:
  - Python 3.11 runtime with uv for dependency and script execution.
  - DSPy >= 2 with central LM configuration and tracing enabled.
  - Typer CLI entrypoint (`repo-cartographer`) plus Textual app packaged under `apps/tui`.
  - Pydantic-based schemas for RPG nodes, edges, and plan artifacts in `rpg/`.


--- .taskmaster/AGENTS.md ---
# Task Master AI - Agent Integration Guide

## Essential Commands

### Core Workflow Commands

```bash
# Project Setup
task-master init                                    # Initialize Task Master in current project
task-master parse-prd .taskmaster/docs/prd.txt      # Generate tasks from PRD document
task-master models --setup                        # Configure AI models interactively

# Daily Development Workflow
task-master list                                   # Show all tasks with status
task-master next                                   # Get next available task to work on
task-master show <id>                             # View detailed task information (e.g., task-master show 1.2)
task-master set-status --id=<id> --status=done    # Mark task complete

# Task Management
task-master add-task --prompt="description" --research        # Add new task with AI assistance
task-master expand --id=<id> --research --force              # Break task into subtasks
task-master update-task --id=<id> --prompt="changes"         # Update specific task
task-master update --from=<id> --prompt="changes"            # Update multiple tasks from ID onwards
task-master update-subtask --id=<id> --prompt="notes"        # Add implementation notes to subtask

# Analysis & Planning
task-master analyze-complexity --research          # Analyze task complexity
task-master complexity-report                      # View complexity analysis
task-master expand --all --research               # Expand all eligible tasks

# Dependencies & Organization
task-master add-dependency --id=<id> --depends-on=<id>       # Add task dependency
task-master move --from=<id> --to=<id>                       # Reorganize task hierarchy
task-master validate-dependencies                            # Check for dependency issues
task-master generate                                         # Update task markdown files (usually auto-called)
```

## Key Files & Project Structure

### Core Files

- `.taskmaster/tasks/tasks.json` - Main task data file (auto-managed)
- `.taskmaster/config.json` - AI model configuration (use `task-master models` to modify)
- `.taskmaster/docs/prd.txt` - Product Requirements Document for parsing
- `.taskmaster/tasks/*.txt` - Individual task files (auto-generated from tasks.json)
- `.env` - API keys for CLI usage

### Claude Code Integration Files

- `CLAUDE.md` - Auto-loaded context for Claude Code (this file)
- `.claude/settings.json` - Claude Code tool allowlist and preferences
- `.claude/commands/` - Custom slash commands for repeated workflows
- `.mcp.json` - MCP server configuration (project-specific)

### Directory Structure

```
project/
├── .taskmaster/
│   ├── tasks/              # Task files directory
│   │   ├── tasks.json      # Main task database
│   │   ├── task-1.md      # Individual task files
│   │   └── task-2.md
│   ├── docs/              # Documentation directory
│   │   ├── prd.txt        # Product requirements
│   ├── reports/           # Analysis reports directory
│   │   └── task-complexity-report.json
│   ├── templates/         # Template files
│   │   └── example_prd.txt  # Example PRD template
│   └── config.json        # AI models & settings
├── .claude/
│   ├── settings.json      # Claude Code configuration
│   └── commands/         # Custom slash commands
├── .env                  # API keys
├── .mcp.json            # MCP configuration
└── CLAUDE.md            # This file - auto-loaded by Claude Code
```

## MCP Integration

Task Master provides an MCP server that Claude Code can connect to. Configure in `.mcp.json`:

```json
{
  "mcpServers": {
    "task-master-ai": {
      "command": "npx",
      "args": ["-y", "task-master-ai"],
      "env": {
        "ANTHROPIC_API_KEY": "your_key_here",
        "PERPLEXITY_API_KEY": "your_key_here",
        "OPENAI_API_KEY": "OPENAI_API_KEY_HERE",
        "GOOGLE_API_KEY": "GOOGLE_API_KEY_HERE",
        "XAI_API_KEY": "XAI_API_KEY_HERE",
        "OPENROUTER_API_KEY": "OPENROUTER_API_KEY_HERE",
        "MISTRAL_API_KEY": "MISTRAL_API_KEY_HERE",
        "AZURE_OPENAI_API_KEY": "AZURE_OPENAI_API_KEY_HERE",
        "OLLAMA_API_KEY": "OLLAMA_API_KEY_HERE"
      }
    }
  }
}
```

### Essential MCP Tools

```javascript
help; // = shows available taskmaster commands
// Project setup
initialize_project; // = task-master init
parse_prd; // = task-master parse-prd

// Daily workflow
get_tasks; // = task-master list
next_task; // = task-master next
get_task; // = task-master show <id>
set_task_status; // = task-master set-status

// Task management
add_task; // = task-master add-task
expand_task; // = task-master expand
update_task; // = task-master update-task
update_subtask; // = task-master update-subtask
update; // = task-master update

// Analysis
analyze_project_complexity; // = task-master analyze-complexity
complexity_report; // = task-master complexity-report
```

## Claude Code Workflow Integration

### Standard Development Workflow

#### 1. Project Initialization

```bash
# Initialize Task Master
task-master init

# Create or obtain PRD, then parse it
task-master parse-prd .taskmaster/docs/prd.txt

# Analyze complexity and expand tasks
task-master analyze-complexity --research
task-master expand --all --research
```

If tasks already exist, another PRD can be parsed (with new information only!) using parse-prd with --append flag. This will add the generated tasks to the existing list of tasks..

#### 2. Daily Development Loop

```bash
# Start each session
task-master next                           # Find next available task
task-master show <id>                     # Review task details

# During implementation, check in code context into the tasks and subtasks
task-master update-subtask --id=<id> --prompt="implementation notes..."

# Complete tasks
task-master set-status --id=<id> --status=done
```

#### 3. Multi-Claude Workflows

For complex projects, use multiple Claude Code sessions:

```bash
# Terminal 1: Main implementation
cd project && claude

# Terminal 2: Testing and validation
cd project-test-worktree && claude

# Terminal 3: Documentation updates
cd project-docs-worktree && claude
```

### Custom Slash Commands

Create `.claude/commands/taskmaster-next.md`:

```markdown
Find the next available Task Master task and show its details.

Steps:

1. Run `task-master next` to get the next task
2. If a task is available, run `task-master show <id>` for full details
3. Provide a summary of what needs to be implemented
4. Suggest the first implementation step
```

Create `.claude/commands/taskmaster-complete.md`:

```markdown
Complete a Task Master task: $ARGUMENTS

Steps:

1. Review the current task with `task-master show $ARGUMENTS`
2. Verify all implementation is complete
3. Run any tests related to this task
4. Mark as complete: `task-master set-status --id=$ARGUMENTS --status=done`
5. Show the next available task with `task-master next`
```

## Tool Allowlist Recommendations

Add to `.claude/settings.json`:

```json
{
  "allowedTools": [
    "Edit",
    "Bash(task-master *)",
    "Bash(git commit:*)",
    "Bash(git add:*)",
    "Bash(npm run *)",
    "mcp__task_master_ai__*"
  ]
}
```

## Configuration & Setup

### API Keys Required

At least **one** of these API keys must be configured:

- `ANTHROPIC_API_KEY` (Claude models) - **Recommended**
- `PERPLEXITY_API_KEY` (Research features) - **Highly recommended**
- `OPENAI_API_KEY` (GPT models)
- `GOOGLE_API_KEY` (Gemini models)
- `MISTRAL_API_KEY` (Mistral models)
- `OPENROUTER_API_KEY` (Multiple models)
- `XAI_API_KEY` (Grok models)

An API key is required for any provider used across any of the 3 roles defined in the `models` command.

### Model Configuration

```bash
# Interactive setup (recommended)
task-master models --setup

# Set specific models
task-master models --set-main claude-3-5-sonnet-20241022
task-master models --set-research perplexity-llama-3.1-sonar-large-128k-online
task-master models --set-fallback gpt-4o-mini
```

## Task Structure & IDs

### Task ID Format

- Main tasks: `1`, `2`, `3`, etc.
- Subtasks: `1.1`, `1.2`, `2.1`, etc.
- Sub-subtasks: `1.1.1`, `1.1.2`, etc.

### Task Status Values

- `pending` - Ready to work on
- `in-progress` - Currently being worked on
- `done` - Completed and verified
- `deferred` - Postponed
- `cancelled` - No longer needed
- `blocked` - Waiting on external factors

### Task Fields

```json
{
  "id": "1.2",
  "title": "Implement user authentication",
  "description": "Set up JWT-based auth system",
  "status": "pending",
  "priority": "high",
  "dependencies": ["1.1"],
  "details": "Use bcrypt for hashing, JWT for tokens...",
  "testStrategy": "Unit tests for auth functions, integration tests for login flow",
  "subtasks": []
}
```

## Claude Code Best Practices with Task Master

### Context Management

- Use `/clear` between different tasks to maintain focus
- This CLAUDE.md file is automatically loaded for context
- Use `task-master show <id>` to pull specific task context when needed

### Iterative Implementation

1. `task-master show <subtask-id>` - Understand requirements
2. Explore codebase and plan implementation
3. `task-master update-subtask --id=<id> --prompt="detailed plan"` - Log plan
4. `task-master set-status --id=<id> --status=in-progress` - Start work
5. Implement code following logged plan
6. `task-master update-subtask --id=<id> --prompt="what worked/didn't work"` - Log progress
7. `task-master set-status --id=<id> --status=done` - Complete task

### Complex Workflows with Checklists

For large migrations or multi-step processes:

1. Create a markdown PRD file describing the new changes: `touch task-migration-checklist.md` (prds can be .txt or .md)
2. Use Taskmaster to parse the new prd with `task-master parse-prd --append` (also available in MCP)
3. Use Taskmaster to expand the newly generated tasks into subtasks. Consdier using `analyze-complexity` with the correct --to and --from IDs (the new ids) to identify the ideal subtask amounts for each task. Then expand them.
4. Work through items systematically, checking them off as completed
5. Use `task-master update-subtask` to log progress on each task/subtask and/or updating/researching them before/during implementation if getting stuck

### Git Integration

Task Master works well with `gh` CLI:

```bash
# Create PR for completed task
gh pr create --title "Complete task 1.2: User authentication" --body "Implements JWT auth system as specified in task 1.2"

# Reference task in commits
git commit -m "feat: implement JWT auth (task 1.2)"
```

### Parallel Development with Git Worktrees

```bash
# Create worktrees for parallel task development
git worktree add ../project-auth feature/auth-system
git worktree add ../project-api feature/api-refactor

# Run Claude Code in each worktree
cd ../project-auth && claude    # Terminal 1: Auth work
cd ../project-api && claude     # Terminal 2: API work
```

## Troubleshooting

### AI Commands Failing

```bash
# Check API keys are configured
cat .env                           # For CLI usage

# Verify model configuration
task-master models

# Test with different model
task-master models --set-fallback gpt-4o-mini
```

### MCP Connection Issues

- Check `.mcp.json` configuration
- Verify Node.js installation
- Use `--mcp-debug` flag when starting Claude Code
- Use CLI as fallback if MCP unavailable

### Task File Sync Issues

```bash
# Regenerate task files from tasks.json
task-master generate

# Fix dependency issues
task-master fix-dependencies
```

DO NOT RE-INITIALIZE. That will not do anything beyond re-adding the same Taskmaster core files.

## Important Notes

### AI-Powered Operations

These commands make AI calls and may take up to a minute:

- `parse_prd` / `task-master parse-prd`
- `analyze_project_complexity` / `task-master analyze-complexity`
- `expand_task` / `task-master expand`
- `expand_all` / `task-master expand --all`
- `add_task` / `task-master add-task`
- `update` / `task-master update`
- `update_task` / `task-master update-task`
- `update_subtask` / `task-master update-subtask`

### File Management

- Never manually edit `tasks.json` - use commands instead
- Never manually edit `.taskmaster/config.json` - use `task-master models`
- Task markdown files in `tasks/` are auto-generated
- Run `task-master generate` after manual changes to tasks.json

### Claude Code Session Management

- Use `/clear` frequently to maintain focused context
- Create custom slash commands for repeated Task Master workflows
- Configure tool allowlist to streamline permissions
- Use headless mode for automation: `claude -p "task-master next"`

### Multi-Task Updates

- Use `update --from=<id>` to update multiple future tasks
- Use `update-task --id=<id>` for single task updates
- Use `update-subtask --id=<id>` for implementation logging

### Research Mode

- Add `--research` flag for research-based AI enhancement
- Requires a research model API key like Perplexity (`PERPLEXITY_API_KEY`) in environment
- Provides more informed task creation and updates
- Recommended for complex technical tasks

---

_This guide ensures Claude Code has immediate access to Task Master's essential functionality for agentic development workflows._


--- .taskmaster/GEMINI.md ---
# Task Master AI - Agent Integration Guide

## Essential Commands

### Core Workflow Commands

```bash
# Project Setup
task-master init                                    # Initialize Task Master in current project
task-master parse-prd .taskmaster/docs/prd.txt      # Generate tasks from PRD document
task-master models --setup                        # Configure AI models interactively

# Daily Development Workflow
task-master list                                   # Show all tasks with status
task-master next                                   # Get next available task to work on
task-master show <id>                             # View detailed task information (e.g., task-master show 1.2)
task-master set-status --id=<id> --status=done    # Mark task complete

# Task Management
task-master add-task --prompt="description" --research        # Add new task with AI assistance
task-master expand --id=<id> --research --force              # Break task into subtasks
task-master update-task --id=<id> --prompt="changes"         # Update specific task
task-master update --from=<id> --prompt="changes"            # Update multiple tasks from ID onwards
task-master update-subtask --id=<id> --prompt="notes"        # Add implementation notes to subtask

# Analysis & Planning
task-master analyze-complexity --research          # Analyze task complexity
task-master complexity-report                      # View complexity analysis
task-master expand --all --research               # Expand all eligible tasks

# Dependencies & Organization
task-master add-dependency --id=<id> --depends-on=<id>       # Add task dependency
task-master move --from=<id> --to=<id>                       # Reorganize task hierarchy
task-master validate-dependencies                            # Check for dependency issues
task-master generate                                         # Update task markdown files (usually auto-called)
```

## Key Files & Project Structure

### Core Files

- `.taskmaster/tasks/tasks.json` - Main task data file (auto-managed)
- `.taskmaster/config.json` - AI model configuration (use `task-master models` to modify)
- `.taskmaster/docs/prd.txt` - Product Requirements Document for parsing
- `.taskmaster/tasks/*.txt` - Individual task files (auto-generated from tasks.json)
- `.env` - API keys for CLI usage

### Claude Code Integration Files

- `CLAUDE.md` - Auto-loaded context for Claude Code (this file)
- `.claude/settings.json` - Claude Code tool allowlist and preferences
- `.claude/commands/` - Custom slash commands for repeated workflows
- `.mcp.json` - MCP server configuration (project-specific)

### Directory Structure

```
project/
├── .taskmaster/
│   ├── tasks/              # Task files directory
│   │   ├── tasks.json      # Main task database
│   │   ├── task-1.md      # Individual task files
│   │   └── task-2.md
│   ├── docs/              # Documentation directory
│   │   ├── prd.txt        # Product requirements
│   ├── reports/           # Analysis reports directory
│   │   └── task-complexity-report.json
│   ├── templates/         # Template files
│   │   └── example_prd.txt  # Example PRD template
│   └── config.json        # AI models & settings
├── .claude/
│   ├── settings.json      # Claude Code configuration
│   └── commands/         # Custom slash commands
├── .env                  # API keys
├── .mcp.json            # MCP configuration
└── CLAUDE.md            # This file - auto-loaded by Claude Code
```

## MCP Integration

Task Master provides an MCP server that Claude Code can connect to. Configure in `.mcp.json`:

```json
{
  "mcpServers": {
    "task-master-ai": {
      "command": "npx",
      "args": ["-y", "task-master-ai"],
      "env": {
        "ANTHROPIC_API_KEY": "your_key_here",
        "PERPLEXITY_API_KEY": "your_key_here",
        "OPENAI_API_KEY": "OPENAI_API_KEY_HERE",
        "GOOGLE_API_KEY": "GOOGLE_API_KEY_HERE",
        "XAI_API_KEY": "XAI_API_KEY_HERE",
        "OPENROUTER_API_KEY": "OPENROUTER_API_KEY_HERE",
        "MISTRAL_API_KEY": "MISTRAL_API_KEY_HERE",
        "AZURE_OPENAI_API_KEY": "AZURE_OPENAI_API_KEY_HERE",
        "OLLAMA_API_KEY": "OLLAMA_API_KEY_HERE"
      }
    }
  }
}
```

### Essential MCP Tools

```javascript
help; // = shows available taskmaster commands
// Project setup
initialize_project; // = task-master init
parse_prd; // = task-master parse-prd

// Daily workflow
get_tasks; // = task-master list
next_task; // = task-master next
get_task; // = task-master show <id>
set_task_status; // = task-master set-status

// Task management
add_task; // = task-master add-task
expand_task; // = task-master expand
update_task; // = task-master update-task
update_subtask; // = task-master update-subtask
update; // = task-master update

// Analysis
analyze_project_complexity; // = task-master analyze-complexity
complexity_report; // = task-master complexity-report
```

## Claude Code Workflow Integration

### Standard Development Workflow

#### 1. Project Initialization

```bash
# Initialize Task Master
task-master init

# Create or obtain PRD, then parse it
task-master parse-prd .taskmaster/docs/prd.txt

# Analyze complexity and expand tasks
task-master analyze-complexity --research
task-master expand --all --research
```

If tasks already exist, another PRD can be parsed (with new information only!) using parse-prd with --append flag. This will add the generated tasks to the existing list of tasks..

#### 2. Daily Development Loop

```bash
# Start each session
task-master next                           # Find next available task
task-master show <id>                     # Review task details

# During implementation, check in code context into the tasks and subtasks
task-master update-subtask --id=<id> --prompt="implementation notes..."

# Complete tasks
task-master set-status --id=<id> --status=done
```

#### 3. Multi-Claude Workflows

For complex projects, use multiple Claude Code sessions:

```bash
# Terminal 1: Main implementation
cd project && claude

# Terminal 2: Testing and validation
cd project-test-worktree && claude

# Terminal 3: Documentation updates
cd project-docs-worktree && claude
```

### Custom Slash Commands

Create `.claude/commands/taskmaster-next.md`:

```markdown
Find the next available Task Master task and show its details.

Steps:

1. Run `task-master next` to get the next task
2. If a task is available, run `task-master show <id>` for full details
3. Provide a summary of what needs to be implemented
4. Suggest the first implementation step
```

Create `.claude/commands/taskmaster-complete.md`:

```markdown
Complete a Task Master task: $ARGUMENTS

Steps:

1. Review the current task with `task-master show $ARGUMENTS`
2. Verify all implementation is complete
3. Run any tests related to this task
4. Mark as complete: `task-master set-status --id=$ARGUMENTS --status=done`
5. Show the next available task with `task-master next`
```

## Tool Allowlist Recommendations

Add to `.claude/settings.json`:

```json
{
  "allowedTools": [
    "Edit",
    "Bash(task-master *)",
    "Bash(git commit:*)",
    "Bash(git add:*)",
    "Bash(npm run *)",
    "mcp__task_master_ai__*"
  ]
}
```

## Configuration & Setup

### API Keys Required

At least **one** of these API keys must be configured:

- `ANTHROPIC_API_KEY` (Claude models) - **Recommended**
- `PERPLEXITY_API_KEY` (Research features) - **Highly recommended**
- `OPENAI_API_KEY` (GPT models)
- `GOOGLE_API_KEY` (Gemini models)
- `MISTRAL_API_KEY` (Mistral models)
- `OPENROUTER_API_KEY` (Multiple models)
- `XAI_API_KEY` (Grok models)

An API key is required for any provider used across any of the 3 roles defined in the `models` command.

### Model Configuration

```bash
# Interactive setup (recommended)
task-master models --setup

# Set specific models
task-master models --set-main claude-3-5-sonnet-20241022
task-master models --set-research perplexity-llama-3.1-sonar-large-128k-online
task-master models --set-fallback gpt-4o-mini
```

## Task Structure & IDs

### Task ID Format

- Main tasks: `1`, `2`, `3`, etc.
- Subtasks: `1.1`, `1.2`, `2.1`, etc.
- Sub-subtasks: `1.1.1`, `1.1.2`, etc.

### Task Status Values

- `pending` - Ready to work on
- `in-progress` - Currently being worked on
- `done` - Completed and verified
- `deferred` - Postponed
- `cancelled` - No longer needed
- `blocked` - Waiting on external factors

### Task Fields

```json
{
  "id": "1.2",
  "title": "Implement user authentication",
  "description": "Set up JWT-based auth system",
  "status": "pending",
  "priority": "high",
  "dependencies": ["1.1"],
  "details": "Use bcrypt for hashing, JWT for tokens...",
  "testStrategy": "Unit tests for auth functions, integration tests for login flow",
  "subtasks": []
}
```

## Claude Code Best Practices with Task Master

### Context Management

- Use `/clear` between different tasks to maintain focus
- This CLAUDE.md file is automatically loaded for context
- Use `task-master show <id>` to pull specific task context when needed

### Iterative Implementation

1. `task-master show <subtask-id>` - Understand requirements
2. Explore codebase and plan implementation
3. `task-master update-subtask --id=<id> --prompt="detailed plan"` - Log plan
4. `task-master set-status --id=<id> --status=in-progress` - Start work
5. Implement code following logged plan
6. `task-master update-subtask --id=<id> --prompt="what worked/didn't work"` - Log progress
7. `task-master set-status --id=<id> --status=done` - Complete task

### Complex Workflows with Checklists

For large migrations or multi-step processes:

1. Create a markdown PRD file describing the new changes: `touch task-migration-checklist.md` (prds can be .txt or .md)
2. Use Taskmaster to parse the new prd with `task-master parse-prd --append` (also available in MCP)
3. Use Taskmaster to expand the newly generated tasks into subtasks. Consdier using `analyze-complexity` with the correct --to and --from IDs (the new ids) to identify the ideal subtask amounts for each task. Then expand them.
4. Work through items systematically, checking them off as completed
5. Use `task-master update-subtask` to log progress on each task/subtask and/or updating/researching them before/during implementation if getting stuck

### Git Integration

Task Master works well with `gh` CLI:

```bash
# Create PR for completed task
gh pr create --title "Complete task 1.2: User authentication" --body "Implements JWT auth system as specified in task 1.2"

# Reference task in commits
git commit -m "feat: implement JWT auth (task 1.2)"
```

### Parallel Development with Git Worktrees

```bash
# Create worktrees for parallel task development
git worktree add ../project-auth feature/auth-system
git worktree add ../project-api feature/api-refactor

# Run Claude Code in each worktree
cd ../project-auth && claude    # Terminal 1: Auth work
cd ../project-api && claude     # Terminal 2: API work
```

## Troubleshooting

### AI Commands Failing

```bash
# Check API keys are configured
cat .env                           # For CLI usage

# Verify model configuration
task-master models

# Test with different model
task-master models --set-fallback gpt-4o-mini
```

### MCP Connection Issues

- Check `.mcp.json` configuration
- Verify Node.js installation
- Use `--mcp-debug` flag when starting Claude Code
- Use CLI as fallback if MCP unavailable

### Task File Sync Issues

```bash
# Regenerate task files from tasks.json
task-master generate

# Fix dependency issues
task-master fix-dependencies
```

DO NOT RE-INITIALIZE. That will not do anything beyond re-adding the same Taskmaster core files.

## Important Notes

### AI-Powered Operations

These commands make AI calls and may take up to a minute:

- `parse_prd` / `task-master parse-prd`
- `analyze_project_complexity` / `task-master analyze-complexity`
- `expand_task` / `task-master expand`
- `expand_all` / `task-master expand --all`
- `add_task` / `task-master add-task`
- `update` / `task-master update`
- `update_task` / `task-master update-task`
- `update_subtask` / `task-master update-subtask`

### File Management

- Never manually edit `tasks.json` - use commands instead
- Never manually edit `.taskmaster/config.json` - use `task-master models`
- Task markdown files in `tasks/` are auto-generated
- Run `task-master generate` after manual changes to tasks.json

### Claude Code Session Management

- Use `/clear` frequently to maintain focused context
- Create custom slash commands for repeated Task Master workflows
- Configure tool allowlist to streamline permissions
- Use headless mode for automation: `claude -p "task-master next"`

### Multi-Task Updates

- Use `update --from=<id>` to update multiple future tasks
- Use `update-task --id=<id>` for single task updates
- Use `update-subtask --id=<id>` for implementation logging

### Research Mode

- Add `--research` flag for research-based AI enhancement
- Requires a research model API key like Perplexity (`PERPLEXITY_API_KEY`) in environment
- Provides more informed task creation and updates
- Recommended for complex technical tasks

---

_This guide ensures Claude Code has immediate access to Task Master's essential functionality for agentic development workflows._


--- .taskmaster/templates/rank-root-prompts.md ---
<!--
$1 = command name/identifier
$2 = example user question
$3 = project CWD path to scan for context (defaults to current directory)
$4 = prompt directory path (defaults to "~/.codex/prompts")
$5 = minimum relevance threshold (0–1)
-->

# {Context-Aware Prompt Ranking Command}

```md
# Command: $1

# Usage: $1 "$2" "$3" "$4" "$5"

# Args:

# - {{query}}: $2
# - {{project_path}}: $3
# - {{prompt_path}}: $4
# - {{threshold}}: $5

prompt = """
Task:
Given a user inquiry ({{query}}) and the context of a software project located at {{project_path}}, your goal is to identify the most relevant prompt-definition file from the directory {{prompt_path}}.

Defaults:
* If {{project_path}} is missing or blank, use the current working directory.
* If {{prompt_path}} is missing or blank, use "~/.codex/prompts".

Do the following:
1) **Analyze Project Context**: Recursively scan {{project_path}} to understand its structure, languages, and purpose. Create a concise summary of the project context.
2) **Scan Prompts**: List all candidate prompt files in {{prompt_path}} (non-recursively).
3) **Evaluate Prompts**: For each candidate prompt file:
    a) Read its content.
    b) Create a one-sentence summary of its purpose and domain.
    c) Compute a relevance score from 0 to 1. This score must measure how well the prompt's purpose aligns with the user's {{query}}, considering the project context summary. A higher score means the prompt is a better fit for solving the query within the given project.
4) **Rank and Filter**: Order the prompts by their relevance score in descending order.
5) **Generate Output**: Emit a compact markdown table with the columns: `filename | description | match_score` (rounded to 2 decimals).

Rules:
* The description must be 1–2 sentences capturing the prompt's purpose and domain.
* Only include prompts in the table where `match_score` is greater than or equal to {{threshold}}.
* If no prompts meet the threshold, output a single line: "No prompt exceeds threshold {{threshold}} — recommend creating a new prompt."

Acceptance:
* If one or more matches meet the {{threshold}}, a markdown table sorted by descending `match_score` is produced.
* Otherwise, the single-line fallback message is produced.

!{echo "Scanning project: ${PROJECT_PATH_ARG:-.}"}
!{echo "Searching for prompts in: ${PROMPT_PATH_ARG:-~/.codex/prompts}"}
"""
```

## Output format

* **Preferred**: a markdown table with columns `filename | description | match_score` sorted by `match_score` (desc) and filtered by `{{threshold}}`.
* **Fallback**: the exact one-line message when no entries meet `{{threshold}}`.


--- apps/tui/app.py ---


--- apps/cli/__init__.py ---


--- apps/tui/__init__.py ---


--- apps/cli/main.py ---


--- apps/tui/widgets/__init__.py ---


--- core/__init__.py ---


--- core/tools/fs.py ---


--- core/config/locations.py ---


--- core/providers/registry.py ---


--- core/pipeline/run_jobs.py ---


--- core/config/schema.py ---


--- core/tools/web.py ---


--- prompts/extension-command_template.md ---


--- prompts/refactor_prompt_template.md ---


--- rpg/build_queue.py ---


--- rpg/construct.py ---


--- rpg/__init__.py ---


--- rpg/schema.py ---


--- rpg/validate.py ---


--- rpg_tool/README.md ---
# RPG-Build-Assistant (ZeroRepo + RPG)

> A graph-driven, test-first workflow that turns ambiguous natural-language specs into a persistent **Repository Planning Graph (RPG)** and builds repositories in **topological order** with **TDD**. (Guide: Overview; Paper §1–§4).

---

## What this is

- **RPG:** A structured, persistent blueprint encoding capabilities, folders/files, data-flows, and executable interfaces in one graph—resolving NL ambiguity for long-horizon repo generation. (Paper §3.1–§3.3; Fig. 1).
- **ZeroRepo pipeline:** Proposal-level → Implementation-level → Graph-guided code generation (topological traversal + tests). (Paper Fig. 1; §4).
- **Operational build guide:** Concrete SOPs/IDs for planning, validation gates, and CI-friendly execution. (Guide Trace Matrix).

**Primary artifacts:** RPG store, feature-tree index, generation logs, test corpus, metrics dashboard. (Checklist §1).

---

## Core concepts

- **RPG nodes & edges.** Nodes span capability → folder → file → class/function. Edges capture hierarchy, **typed inter-module I/O**, and **intra-module file order**; graph must be a DAG. (Paper §3.1–§3.3.2; Fig. 1B; Guide IMPL-003/004).
- **Adaptive interfaces.** Each leaf maps to **one** interface (function when independent, class when interdependent). (Guide IMPL-006; Paper §3.3.2).
- **Topological, test-first build.** Traverse the RPG in topo order; derive a unit test **before** implementation; commit **only on green**; run selective regression/integration as subgraphs complete. (Guide GEN-001/002/003/004/005; Paper §4 & Appx C).

---

## Why RPG (evidence)

RPG improves planning stability, scales near-linearly in functionality/LOC, and speeds localization vs. NL-only approaches. (Paper §6–§8; Tables/Figures).

---

## Repository structure (conceptual mapping)

- **L0 – Functionality graph:** Cohesive modules from a feature tree (exploit/explore + dedupe). (Paper §3.2).
- **L1 – Folders / L2 – Files:** Map modules→folders, group cohesive files, preserve semantic cohesion, reduce coupling. (Paper §3.3.1; Fig. 1B.1).
- **Full RPG:** Add typed data-flows & file order; abstract base classes; design leaf interfaces. (Paper §3.3.2; Guide IMPL-003/004/005/006).

> Example layout in the paper illustrates `src/data_load`, `src/algos`, `src/eval`, with file- and function-augmented nodes. (Paper Fig. 1B).

---

## Workflow (SOPs + gates)

**Discover → Design → Build → Validate → Benchmark → Monitor** (Guide taxonomy).

1. **Discover**
   Define project spec & acceptance metrics. *(DSC-001)*
2. **Design — Implementation-level**

   - Encode inter-module **typed I/O** data-flows; ensure DAG. *(IMPL-003)*
   - Define intra-module file ordering. *(IMPL-004)*
   - Abstract base classes/shared types. *(IMPL-005)*
   - One interface per leaf (docstring with purpose/args/returns). *(IMPL-006)*
3. **Build — Graph-guided TDD**

   - Create reproducible **BUILD_QUEUE** via topological sort. *(GEN-001; Paper §4)*
   - Auto-derive a unit test from each interface/spec **before** coding. *(GEN-002)*
   - Implement minimal code to pass; **commit only on green**; run regression for impacted nodes; add integration tests per subgraph. *(GEN-003/004/005; Appx C)*
4. **Validate — Metrics & diagnosis**
   Majority-vote semantic checks; attribute failures (impl vs test/env); compute **Coverage, Novelty, Pass Rate, Voting Rate, Files, LOC, Tokens**. *(Paper §5.1–D.3.1; Guide VAL)*
5. **Benchmark**
   Evaluate against RepoCraft tasks & baselines using the above metrics. *(Paper §5–§6; Checklist §5–§6)*
6. **Monitor**
   Track near-linear growth (features/LOC); alert on plateau; correlate with pass rate. *(MON-004)*

---

## Getting started (dev tooling)

Use the recommended Python tooling for a clean CI loop:

- **Env & deps:** `uv`
- **Lint/format:** `ruff`
- **Types:** `mypy`
- **Tests:** `pytest`
- **Security lint:** `bandit`
- **Logging:** `loguru`
  (Agent playbook tooling rubric).

> Example (local): `uv pip install ruff mypy pytest bandit loguru && ruff check . && mypy . && pytest -q && bandit -r .` (Agent playbook).

---

## Quality gates & metrics

- **Build gates:** DAG valid; interface docs complete; topo order persisted; commit only on green; selective regression; subgraph integration tests. (Guide IMPL-003/006; GEN-001..005).
- **Evaluation metrics:**

  - **Coverage** (category hit ratio), **Novelty** (OOD rate), **Pass Rate**, **Voting Rate**, and code-scale (**Files**, **Normalized LOC**, **Tokens**)—exact formulas in Appx D.3.1. (Paper §5.1–D.3.1).

---

## Agents & roles (optional)

Planner → Implementer → Builder → Verifier → Orchestrator, with optional Retriever/Localizer. Includes tool permissions and HITL gates. (Agents roster & SOPs).

---

## Contributing

- Follow **topological TDD** and commit only when tests pass locally and in CI. (Guide GEN-001/003; Paper §4).
- Keep module cohesion high; prefer functions to classes unless state is shared. (Guide IMPL-006).
- Run lint/type/security checks before PRs. (Agents tooling).

---

## Roadmap & appendices

- Proposal-level retrieval & diversity (Appx A).
- Implementation patterns & skeletons (Appx B).
- Localization, editing, and staged testing (Appx C).
- RepoCraft taxonomy & metrics (Appx D).
- Result tables & worked examples (Appx E). (Checklist “Appendices”).

---

## Citation map (quick pointers)

- **RPG structure & pipeline:** Paper §3–§4; Fig. 1.
- **Data-flows / order edges / interfaces:** Guide **IMPL-003/004/006**.
- **Topo build + TDD + commit policy:** Guide **GEN-001/002/003**; Paper §4, Appx C.
- **Metrics (Coverage/Novelty/Pass/Vote/Scale):** Paper §5.1–Appx D.3.1.
- **Scaling & localization benefits:** Paper §6–§7 (tables/analysis).

---

### License

TBD (not specified in the Build Guide/Whitepaper). If needed, add governance scans (SBOM, licenses, secrets) per operational practices. (Agents governance).

---

**RPG mapping:** This README encodes the project’s Discover→Design→Build→Validate→Benchmark→Monitor flow and cites Guide IDs + Paper sections at each step for traceability. (Guide Trace Matrix; Paper Fig. 1).


--- rpg_tool/rpg-vector-index/README.md ---
<!-- path: README.md -->
# RPG Vector Index Upgrade (FAISS / pgvector)

This package upgrades the Feature Tree seed to a **real vector index** using **FAISS** (local) and **pgvector** (PostgreSQL). It preserves the **exact JSON schema** of your existing seed (nodes, hierarchy_edges, retrieval.vectors) and treats the JSON as the **single source of truth**.

## What this provides

- **pgvector** schema + indexes (HNSW + optional IVFFlat) and a loader that upserts features + embeddings.
- **FAISS** HNSW index builder that writes `feature_index.faiss` and a sidecar metadata CSV.
- **Query examples** for both backends returning **full hierarchical paths**.

## Inputs

- A JSON Feature Tree in the same schema as your `Feature Tree Seed` (top-level keys: `metadata`, `schema`, `nodes`, `hierarchy_edges`, `retrieval`).

## Quick start (local)

### 1) Create FAISS index from the seed

```bash
uv run python scripts/build_faiss.py feature_tree_seed.json --out-dir ./.artefacts
```

```md
# 1) ensure Postgres has pgvector installed; then:
psql "$PG_DSN" -f sql/pgvector_schema.sql

# 2) load data (will upsert nodes & vectors)
uv run python scripts/build_pgvector.py feature_tree_seed.json --dsn "$PG_DSN" --distance cosine

uv run python scripts/search_examples.py --faiss ./.artefacts/feature_index.faiss \
  --meta ./.artefacts/feature_meta.csv --query "logistic regression" --topk 5

uv run python scripts/search_examples.py --pgvector --dsn "$PG_DSN" \
  --query-vec "[0.8,0.2,0.1,0.1,0.4,0.0,0.1,0.1]" --topk 5
```

```md
Tip: Replace FAISS vectors/pgvector column dimension with your production embedding size (e.g., 1536 or 3072). The loader auto-detects dimension from retrieval.dims in the JSON.

Configuration
PG_DSN: Postgres connection string (e.g., postgresql://user:pass@localhost:5432/db).
--distance: one of l2, cosine, or ip for pgvector operator and FAISS metric.
Files
sql/pgvector_schema.sql – DDL for the table and indexes.
scripts/build_pgvector.py – parses JSON and upserts into Postgres/pgvector.
scripts/build_faiss.py – builds an HNSW FAISS index + metadata.
scripts/search_examples.py – thin examples to query either backend.
```


--- rpg_tool/.taskmaster/README-task-master.md ---
# Task Master

### by [@eyaltoledano](https://x.com/eyaltoledano)

A task management system for AI-driven development with Claude, designed to work seamlessly with Cursor AI.

## Requirements

- Node.js 14.0.0 or higher
- Anthropic API key (Claude API)
- Anthropic SDK version 0.39.0 or higher
- OpenAI SDK (for Perplexity API integration, optional)

## Configuration

Taskmaster uses two primary configuration methods:

1. **`.taskmasterconfig` File (Project Root)**

    - Stores most settings: AI model selections (main, research, fallback), parameters (max tokens, temperature), logging level, default priority/subtasks, project name.
    - **Created and managed using `task-master models --setup` CLI command or the `models` MCP tool.**
    - Do not edit manually unless you know what you are doing.

2. **Environment Variables (`.env` file or MCP `env` block)**
    - Used **only** for sensitive **API Keys** (e.g., `ANTHROPIC_API_KEY`, `PERPLEXITY_API_KEY`, etc.) and specific endpoints (like `OLLAMA_BASE_URL`).
    - **For CLI:** Place keys in a `.env` file in your project root.
    - **For MCP/Cursor:** Place keys in the `env` section of your `.cursor/mcp.json` (or other MCP config according to the AI IDE or client you use) file under the `taskmaster-ai` server definition.

**Important:** Settings like model choices, max tokens, temperature, and log level are **no longer configured via environment variables.** Use the `task-master models` command or tool.

See the [Configuration Guide](docs/configuration.md) for full details.

## Installation

```bash
# Install globally
npm install -g task-master-ai

# OR install locally within your project
npm install task-master-ai
```

### Initialize a new project

```bash
# If installed globally
task-master init

# If installed locally
npx task-master init
```

This will prompt you for project details and set up a new project with the necessary files and structure.

### Important Notes

1. **ES Modules Configuration:**

   - This project uses ES Modules (ESM) instead of CommonJS.
   - This is set via `"type": "module"` in your package.json.
   - Use `import/export` syntax instead of `require()`.
   - Files should use `.js` or `.mjs` extensions.
   - To use a CommonJS module, either:
     - Rename it with `.cjs` extension
     - Use `await import()` for dynamic imports
   - If you need CommonJS throughout your project, remove `"type": "module"` from package.json, but Task Master scripts expect ESM.

2. The Anthropic SDK version should be 0.39.0 or higher.

## Quick Start with Global Commands

After installing the package globally, you can use these CLI commands from any directory:

```bash
# Initialize a new project
task-master init

# Parse a PRD and generate tasks
task-master parse-prd your-prd.txt

# List all tasks
task-master list

# Show the next task to work on
task-master next

# Generate task files
task-master generate
```

## Troubleshooting

### If `task-master init` doesn't respond

Try running it with Node directly:

```bash
node node_modules/claude-task-master/scripts/init.js
```

Or clone the repository and run:

```bash
git clone https://github.com/eyaltoledano/claude-task-master.git
cd claude-task-master
node scripts/init.js
```

## Task Structure

Tasks in tasks.json have the following structure:

- `id`: Unique identifier for the task (Example: `1`)
- `title`: Brief, descriptive title of the task (Example: `"Initialize Repo"`)
- `description`: Concise description of what the task involves (Example: `"Create a new repository, set up initial structure."`)
- `status`: Current state of the task (Example: `"pending"`, `"done"`, `"deferred"`)
- `dependencies`: IDs of tasks that must be completed before this task (Example: `[1, 2]`)
  - Dependencies are displayed with status indicators (✅ for completed, ⏱️ for pending)
  - This helps quickly identify which prerequisite tasks are blocking work
- `priority`: Importance level of the task (Example: `"high"`, `"medium"`, `"low"`)
- `details`: In-depth implementation instructions (Example: `"Use GitHub client ID/secret, handle callback, set session token."`)
- `testStrategy`: Verification approach (Example: `"Deploy and call endpoint to confirm 'Hello World' response."`)
- `subtasks`: List of smaller, more specific tasks that make up the main task (Example: `[{"id": 1, "title": "Configure OAuth", ...}]`)

## Integrating with Cursor AI

Claude Task Master is designed to work seamlessly with [Cursor AI](https://www.cursor.so/), providing a structured workflow for AI-driven development.

### Setup with Cursor

1. After initializing your project, open it in Cursor
2. The `.cursor/rules/dev_workflow.mdc` file is automatically loaded by Cursor, providing the AI with knowledge about the task management system
3. Place your PRD document in the `scripts/` directory (e.g., `scripts/prd.txt`)
4. Open Cursor's AI chat and switch to Agent mode

### Setting up MCP in Cursor

To enable enhanced task management capabilities directly within Cursor using the Model Control Protocol (MCP):

1. Go to Cursor settings
2. Navigate to the MCP section
3. Click on "Add New MCP Server"
4. Configure with the following details:
   - Name: "Task Master"
   - Type: "Command"
   - Command: "npx -y task-master-ai"
5. Save the settings

Once configured, you can interact with Task Master's task management commands directly through Cursor's interface, providing a more integrated experience.

### Initial Task Generation

In Cursor's AI chat, instruct the agent to generate tasks from your PRD:

```
Please use the task-master parse-prd command to generate tasks from my PRD. The PRD is located at scripts/prd.txt.
```

The agent will execute:

```bash
task-master parse-prd scripts/prd.txt
```

This will:

- Parse your PRD document
- Generate a structured `tasks.json` file with tasks, dependencies, priorities, and test strategies
- The agent will understand this process due to the Cursor rules

### Generate Individual Task Files

Next, ask the agent to generate individual task files:

```
Please generate individual task files from tasks.json
```

The agent will execute:

```bash
task-master generate
```

This creates individual task files in the `tasks/` directory (e.g., `task_001.txt`, `task_002.txt`), making it easier to reference specific tasks.

## AI-Driven Development Workflow

The Cursor agent is pre-configured (via the rules file) to follow this workflow:

### 1. Task Discovery and Selection

Ask the agent to list available tasks:

```
What tasks are available to work on next?
```

The agent will:

- Run `task-master list` to see all tasks
- Run `task-master next` to determine the next task to work on
- Analyze dependencies to determine which tasks are ready to be worked on
- Prioritize tasks based on priority level and ID order
- Suggest the next task(s) to implement

### 2. Task Implementation

When implementing a task, the agent will:

- Reference the task's details section for implementation specifics
- Consider dependencies on previous tasks
- Follow the project's coding standards
- Create appropriate tests based on the task's testStrategy

You can ask:

```
Let's implement task 3. What does it involve?
```

### 3. Task Verification

Before marking a task as complete, verify it according to:

- The task's specified testStrategy
- Any automated tests in the codebase
- Manual verification if required

### 4. Task Completion

When a task is completed, tell the agent:

```
Task 3 is now complete. Please update its status.
```

The agent will execute:

```bash
task-master set-status --id=3 --status=done
```

### 5. Handling Implementation Drift

If during implementation, you discover that:

- The current approach differs significantly from what was planned
- Future tasks need to be modified due to current implementation choices
- New dependencies or requirements have emerged

Tell the agent:

```
We've changed our approach. We're now using Express instead of Fastify. Please update all future tasks to reflect this change.
```

The agent will execute:

```bash
task-master update --from=4 --prompt="Now we are using Express instead of Fastify."
```

This will rewrite or re-scope subsequent tasks in tasks.json while preserving completed work.

### 6. Breaking Down Complex Tasks

For complex tasks that need more granularity:

```
Task 5 seems complex. Can you break it down into subtasks?
```

The agent will execute:

```bash
task-master expand --id=5 --num=3
```

You can provide additional context:

```
Please break down task 5 with a focus on security considerations.
```

The agent will execute:

```bash
task-master expand --id=5 --prompt="Focus on security aspects"
```

You can also expand all pending tasks:

```
Please break down all pending tasks into subtasks.
```

The agent will execute:

```bash
task-master expand --all
```

For research-backed subtask generation using Perplexity AI:

```
Please break down task 5 using research-backed generation.
```

The agent will execute:

```bash
task-master expand --id=5 --research
```

## Command Reference

Here's a comprehensive reference of all available commands:

### Parse PRD

```bash
# Parse a PRD file and generate tasks
task-master parse-prd <prd-file.txt>

# Limit the number of tasks generated (default is 10)
task-master parse-prd <prd-file.txt> --num-tasks=5

# Allow task master to determine the number of tasks based on complexity
task-master parse-prd <prd-file.txt> --num-tasks=0
```

### List Tasks

```bash
# List all tasks
task-master list

# List tasks with a specific status
task-master list --status=<status>

# List tasks with subtasks
task-master list --with-subtasks

# List tasks with a specific status and include subtasks
task-master list --status=<status> --with-subtasks
```

### Show Next Task

```bash
# Show the next task to work on based on dependencies and status
task-master next
```

### Show Specific Task

```bash
# Show details of a specific task
task-master show <id>
# or
task-master show --id=<id>

# View a specific subtask (e.g., subtask 2 of task 1)
task-master show 1.2
```

### Update Tasks

```bash
# Update tasks from a specific ID and provide context
task-master update --from=<id> --prompt="<prompt>"
```

### Generate Task Files

```bash
# Generate individual task files from tasks.json
task-master generate
```

### Set Task Status

```bash
# Set status of a single task
task-master set-status --id=<id> --status=<status>

# Set status for multiple tasks
task-master set-status --id=1,2,3 --status=<status>

# Set status for subtasks
task-master set-status --id=1.1,1.2 --status=<status>
```

When marking a task as "done", all of its subtasks will automatically be marked as "done" as well.

### Expand Tasks

```bash
# Expand a specific task with subtasks
task-master expand --id=<id> --num=<number>

# Expand a task with a dynamic number of subtasks (ignoring complexity report)
task-master expand --id=<id> --num=0

# Expand with additional context
task-master expand --id=<id> --prompt="<context>"

# Expand all pending tasks
task-master expand --all

# Force regeneration of subtasks for tasks that already have them
task-master expand --all --force

# Research-backed subtask generation for a specific task
task-master expand --id=<id> --research

# Research-backed generation for all tasks
task-master expand --all --research
```

### Clear Subtasks

```bash
# Clear subtasks from a specific task
task-master clear-subtasks --id=<id>

# Clear subtasks from multiple tasks
task-master clear-subtasks --id=1,2,3

# Clear subtasks from all tasks
task-master clear-subtasks --all
```

### Analyze Task Complexity

```bash
# Analyze complexity of all tasks
task-master analyze-complexity

# Save report to a custom location
task-master analyze-complexity --output=my-report.json

# Use a specific LLM model
task-master analyze-complexity --model=claude-3-opus-20240229

# Set a custom complexity threshold (1-10)
task-master analyze-complexity --threshold=6

# Use an alternative tasks file
task-master analyze-complexity --file=custom-tasks.json

# Use Perplexity AI for research-backed complexity analysis
task-master analyze-complexity --research
```

### View Complexity Report

```bash
# Display the task complexity analysis report
task-master complexity-report

# View a report at a custom location
task-master complexity-report --file=my-report.json
```

### Managing Task Dependencies

```bash
# Add a dependency to a task
task-master add-dependency --id=<id> --depends-on=<id>

# Remove a dependency from a task
task-master remove-dependency --id=<id> --depends-on=<id>

# Validate dependencies without fixing them
task-master validate-dependencies

# Find and fix invalid dependencies automatically
task-master fix-dependencies
```

### Add a New Task

```bash
# Add a new task using AI
task-master add-task --prompt="Description of the new task"

# Add a task with dependencies
task-master add-task --prompt="Description" --dependencies=1,2,3

# Add a task with priority
task-master add-task --prompt="Description" --priority=high
```

## Feature Details

### Analyzing Task Complexity

The `analyze-complexity` command:

- Analyzes each task using AI to assess its complexity on a scale of 1-10
- Recommends optimal number of subtasks based on configured DEFAULT_SUBTASKS
- Generates tailored prompts for expanding each task
- Creates a comprehensive JSON report with ready-to-use commands
- Saves the report to scripts/task-complexity-report.json by default

The generated report contains:

- Complexity analysis for each task (scored 1-10)
- Recommended number of subtasks based on complexity
- AI-generated expansion prompts customized for each task
- Ready-to-run expansion commands directly within each task analysis

### Viewing Complexity Report

The `complexity-report` command:

- Displays a formatted, easy-to-read version of the complexity analysis report
- Shows tasks organized by complexity score (highest to lowest)
- Provides complexity distribution statistics (low, medium, high)
- Highlights tasks recommended for expansion based on threshold score
- Includes ready-to-use expansion commands for each complex task
- If no report exists, offers to generate one on the spot

### Smart Task Expansion

The `expand` command automatically checks for and uses the complexity report:

When a complexity report exists:

- Tasks are automatically expanded using the recommended subtask count and prompts
- When expanding all tasks, they're processed in order of complexity (highest first)
- Research-backed generation is preserved from the complexity analysis
- You can still override recommendations with explicit command-line options

Example workflow:

```bash
# Generate the complexity analysis report with research capabilities
task-master analyze-complexity --research

# Review the report in a readable format
task-master complexity-report

# Expand tasks using the optimized recommendations
task-master expand --id=8
# or expand all tasks
task-master expand --all
```

### Finding the Next Task

The `next` command:

- Identifies tasks that are pending/in-progress and have all dependencies satisfied
- Prioritizes tasks by priority level, dependency count, and task ID
- Displays comprehensive information about the selected task:
  - Basic task details (ID, title, priority, dependencies)
  - Implementation details
  - Subtasks (if they exist)
- Provides contextual suggested actions:
  - Command to mark the task as in-progress
  - Command to mark the task as done
  - Commands for working with subtasks

### Viewing Specific Task Details

The `show` command:

- Displays comprehensive details about a specific task or subtask
- Shows task status, priority, dependencies, and detailed implementation notes
- For parent tasks, displays all subtasks and their status
- For subtasks, shows parent task relationship
- Provides contextual action suggestions based on the task's state
- Works with both regular tasks and subtasks (using the format taskId.subtaskId)

## Best Practices for AI-Driven Development

1. **Start with a detailed PRD**: The more detailed your PRD, the better the generated tasks will be.

2. **Review generated tasks**: After parsing the PRD, review the tasks to ensure they make sense and have appropriate dependencies.

3. **Analyze task complexity**: Use the complexity analysis feature to identify which tasks should be broken down further.

4. **Follow the dependency chain**: Always respect task dependencies - the Cursor agent will help with this.

5. **Update as you go**: If your implementation diverges from the plan, use the update command to keep future tasks aligned with your current approach.

6. **Break down complex tasks**: Use the expand command to break down complex tasks into manageable subtasks.

7. **Regenerate task files**: After any updates to tasks.json, regenerate the task files to keep them in sync.

8. **Communicate context to the agent**: When asking the Cursor agent to help with a task, provide context about what you're trying to achieve.

9. **Validate dependencies**: Periodically run the validate-dependencies command to check for invalid or circular dependencies.

## Example Cursor AI Interactions

### Starting a new project

```txt
I've just initialized a new project with Claude Task Master. I have a PRD at scripts/prd.txt.
Can you help me parse it and set up the initial tasks?
```

### Working on tasks

```txt
What's the next task I should work on? Please consider dependencies and priorities.
```

### Implementing a specific task

```txt
I'd like to implement task 4. Can you help me understand what needs to be done and how to approach it?
```

### Managing subtasks

```txt
I need to regenerate the subtasks for task 3 with a different approach. Can you help me clear and regenerate them?
```

### Handling changes

```txt
We've decided to use MongoDB instead of PostgreSQL. Can you update all future tasks to reflect this change?
```

### Completing work

```txt
I've finished implementing the authentication system described in task 2. All tests are passing.
Please mark it as complete and tell me what I should work on next.
```

### Analyzing complexity

```txt
Can you analyze the complexity of our tasks to help me understand which ones need to be broken down further?
```

### Viewing complexity report

```txt
Can you show me the complexity report in a more readable format?
```


--- rpg_tool/AGENTS.md ---
# AGENTS.md — {{primary_purpose}}

## 1) Title & Scope

**Scope.** This playbook operationalizes the Repository Planning Graph (RPG) + ZeroRepo method to **plan → build → validate** complete software repositories using a persistent graph that unifies proposal- and implementation-level planning, and graph-guided TDD.
**Assumptions.** {{environment}} provides access to <VECTOR_DB>, <GRAPH_STORE>, <RUNTIME>, <CI>, and <ARTIFACT_STORE>.
**Non-goals.** Provider-specific APIs, model choices, or benchmark replication beyond KPIs defined in {{kpis}}.

---

## 2) System Overview

- **Operating Model.** **Observe → Plan → Act → Verify → Report → (Escalate/Iterate)**.
  RPG encodes capabilities, folders/files, interfaces, and data/data-flow edges; code is generated **in topological order** with **test-driven development** (unit → regression → integration).
- **Autonomy.** {{autonomy}} with HITL gates specified below.
- **Environment.** {{environment}} (repos, data sources, deployment targets).

---

## 3) Agent Roster

For each agent: **Name • Role • Primary Goals • Inputs • Outputs • Success Signals**

1. **Planner** • planning

   - **Goals:** Convert {{primary_purpose}} into an **RPG L0** functionality graph using explore–exploit retrieval from a global Feature Tree; refactor into cohesive modules.
   - **Inputs:** <GOAL_SPEC>, <CONSTRAINTS>, <FEATURE_TREE>, <VECTOR_DB>
   - **Outputs:** <RPG_L0.json> (modules, capabilities), <RISK_LOG>
   - **Success:** Coverage of required functional categories ≥ <COVERAGE_TARGET>.

2. **Implementer** (Executor) • implementation design

   - **Goals:** Map L0 to **folders/files (L1/L2)**; encode **typed inter-module data flows** and **intra-module file ordering**; define leaf interfaces (func/class).
   - **Inputs:** <RPG_L0.json>
   - **Outputs:** <RPG_FULL.json> (folders/files, edges, interfaces), <SKELETON_REPO>
   - **Success:** DAG validation passes; interface docs complete.

3. **Builder** (Executor) • code generation

   - **Goals:** Topologically traverse RPG; **derive tests first**, then implement to green; run selective regression; add integration tests per subgraph.
   - **Inputs:** <RPG_FULL.json>, <TEST_PROMPTS>
   - **Outputs:** <SRC_CODE>, <UNIT_TESTS>, <INTEG_TESTS>
   - **Success:** Pass rate ≥ <PASS_RATE_TARGET>; no broken dependents.

4. **Verifier** • validation & metrics

   - **Goals:** Majority-vote semantic checks for algorithm presence; attribute failures (impl vs test/env); compute Coverage/Novelty/Pass/Voting/LOC/Files/Tokens.
   - **Inputs:** <SRC_CODE>, <TEST_RESULTS>, <TASK_SET>
   - **Outputs:** <EVAL_REPORT>, <METRICS.json>, <DIAGNOSIS_LOG>
   - **Success:** Voting agreement ≥ <VOTE_RATE_TARGET>; metrics computed as spec.

5. **Orchestrator** • coordination

   - **Goals:** Schedule DAG execution, enforce budgets/timeouts, trigger escalations/HITL.
   - **Inputs:** All artifacts above
   - **Outputs:** <RUN_LOG>, <ALERTS>
   - **Success:** SLA conformance: latency ≤ <SLA_LATENCY>, cost ≤ <SLA_BUDGET>.

6. *(Optional)* **Retriever** • knowledge grounding

   - **Goals:** Manage Feature Tree search, diversity-aware exploration, LLM filtering and merge.

7. *(Optional)* **Localizer** • graph-guided search/edit

   - **Goals:** Locate target code via functionality → repository navigation; support graph-guided edits.

---

## 4) Tooling & Permissions

### Tool Catalog

*(Provider-agnostic; replace with {{tools}} where available.)*

1. **vector_db.search**

- **Intent:** Retrieve top-k feature paths (exploit).
- **Input Schema:** `{"query":"<TEXT>","k":<INT>,"filters":{}}`
- **Output Schema:** `{"paths":[{"path":"a/b/c","score":<FLOAT>}]}`
- **Side Effects:** none; reads <VECTOR_DB>
- **Limits/Costs:** <QPS_LIMIT>, <BUDGET_PATH_SEARCH>
- **Failure Modes:** skewed recall, latency spikes
- **Example:** `vector_db.search({"query": "<GOAL_SPEC>", "k": 50})`
- **Source:** Proposal retrieval.

2. **feature_tree.sample_reject**

- **Intent:** Diversity-aware exploration with overlap threshold ρ and retries.
- **Input:** `{"root":"<NODE>","rho":<FLOAT>,"tmax":<INT>}`
- **Output:** `{"paths":[...]}`
- **Side Effects:** none
- **Failure:** insufficient diversity
- **Example:** `feature_tree.sample_reject({"root":"root","rho":0.2,"tmax":10})`
- **Source:** Rejection sampling algorithm.

3. **llm.filter_merge**

- **Intent:** Filter, rank, dedupe candidates; propose missing features; batch self-check.
- **Input:** `{"candidates":[...],"context":{"tree":"<JSON>"}}`
- **Output:** `{"accepted":[...],"rationales":[...]}`
- **Side Effects:** audit log
- **Failure:** drift/inconsistency
- **Example:** `llm.filter_merge({...})`
- **Source:** LLM filtering + self-check.

4. **graph_store.persist**

- **Intent:** Versioned save of RPG L0/L1/L2/full.
- **Input:** `{"graph":"<RPG_JSON>","version":"<RPG_VER>"}`
- **Output:** `{"checksum":"<HEX>","uri":"<URI>"}`
- **Side Effects:** writes <GRAPH_STORE>
- **Failure:** schema mismatch
- **Example:** `graph_store.persist({...})`
- **Source:** Persistent representation.

5. **graph.validate_dag**

- **Intent:** Enforce acyclicity; check inter-/intra-module constraints.
- **Input:** `{"graph":"<RPG_JSON>"}`
- **Output:** `{"ok":true,"errors":[]}`
- **Side Effects:** CI gate
- **Failure:** cycles/contradictions
- **Example:** `graph.validate_dag({...})`
- **Source:** Typed flows & topological order.

6. **repo.scaffold**

- **Intent:** Map modules→folders; features→files; generate skeleton.
- **Input:** `{"graph":"<RPG_L1/L2>","root":"<PATH>"}`
- **Output:** `{"paths":[...]}`
- **Side Effects:** writes <SKELETON_REPO>
- **Failure:** over-nesting
- **Source:** File/folder encoding.

7. **tests.unit.generate**

- **Intent:** Derive unit test from interface/spec docstring.
- **Input:** `{"interface":"<FQN>","spec":"<DOC>"}`
- **Output:** `{"test_file":"<PATH>","code":"<STR>"}`
- **Side Effects:** writes tests
- **Failure:** weak coverage
- **Source:** TDD at leaf nodes.

8. **tests.run**

- **Intent:** Execute unit/regression/integration suite; selective by graph impact.
- **Input:** `{"scope":["unit|regression|integration"],"targets":[...],"impact_from_graph":true}`
- **Output:** `{"pass":<BOOL>,"log":"<URI>","failures":[...]}`
- **Side Effects:** CI status
- **Failure:** env flakiness
- **Source:** Regression & integration gates.

9. **evaluator.vote**

- **Intent:** Majority-vote semantic validation (n=5; 2 rounds).
- **Input:** `{"candidate":"<CODE_URI>","task":"<DESC>"}`
- **Output:** `{"votes":5,"agree":<INT>,"confidence":<FLOAT>}`
- **Side Effects:** cost tokens
- **Failure:** judge variance
- **Source:** Majority-vote validation.

10. **metrics.compute**

- **Intent:** Coverage, Novelty, Pass Rate, Voting Rate, Files, LOC, Tokens (per D.3.1).
- **Input:** `{"repo_uri":"<URI>","taxonomy":"<REF>"}`
- **Output:** `{"coverage":...,"novelty":...,"pass":...,"voting":...,"files":...,"loc":...,"tokens":...}`
- **Side Effects:** writes <METRICS_STORE>
- **Failure:** category drift
- **Source:** Metrics definitions.

11. **rpg.localize / repo.view / dep.trace**

- **Intent:** Graph-guided localization (functionality→code), interface view, dependency trace.
- **Schemas:** simple `{ "query":"<TEXT|FEATURE_PATH>" }` → list of FQNs/paths.
- **Source:** Localization toolset.

12. **governance.scan**

- **Intent:** SBOM, licenses, secrets scan.
- **Source:** Governance risk mitigation (derived).

### Permission Matrix (excerpt)

| Agent \ Tool | vector_db.search | feature_tree.sample_reject | graph_store.persist | tests.run | evaluator.vote | governance.scan |
| ------------ | ---------------: | -------------------------: | ------------------: | --------: | -------------: | --------------: |
| Planner      |            ✅read |                      ✅read |                   ⛔ |         ⛔ |              ⛔ |               ⛔ |
| Implementer  |                ⛔ |                          ⛔ |              ✅write |         ⛔ |              ⛔ |               ⛔ |
| Builder      |                ⛔ |                          ⛔ |                   ⛔ |     ✅exec |              ⛔ |               ⛔ |
| Verifier     |                ⛔ |                          ⛔ |                   ⛔ |     ✅exec |          ✅exec |           ✅exec |
| Orchestrator |           ✅audit |                     ✅audit |            ✅approve |  ✅approve |       ✅approve |        ✅approve |

Sensitive ops (**write/exec/approve**) require HITL if <RISK_LEVEL> ≥ <HITL_THRESHOLD>.

---

## 5) Task Graph & Protocols

### Goal Decomposition (ASCII DAG)

```
[Observe Spec]
   ↓
[Planner: RPG L0 (exploit+explore)]  --> [HITL-Approve?] -->
   ↓
[Implementer: Folders/Files + DataFlows + Interfaces → RPG Full]
   ↓
[Builder: Topo Build Queue → (Unit Test → Implement → Regression) → Integration Tests]
   ↓
[Verifier: Vote + Metrics + Diagnosis]
   ↓
[Report/Release or Iterate/Escalate]
```

**Source:** Three-stage pipeline + topo traversal + testing.

### Message Contracts

- **Request (agent→agent):** `{id, from, to, intent, content, constraints[], artifacts[], deadline, budget}`
- **Response:** `{id, in_reply_to, status, summary, artifacts[], metrics{}, notes}`

- **Error:** `{id, in_reply_to, error_code, error_class, evidence_uri}`

### Scheduling

- **Concurrency:** Max parallelism = min(<CPU_CORES>, <MAX_PARALLEL_NODES>), **respect DAG order**.
- **Retries/Backoff:** Tool-call retries: 2 with exponential backoff (1×, 2×).
- **Time bounds:** <MAX_STAGE_LATENCY> per stage; **stop** on budget/latency breach.

---

## 6) Standard Operating Procedures (SOPs)

### SOP: **Plan** — task breakdown & risk scan

- **WHEN:** <GOAL_SPEC> received.
- **DO:**

  1. Retrieve top-k exploit paths from <VECTOR_DB>.
  2. Sample explore paths via diversity-aware rejection (ρ).
  3. LLM filter/merge; propose missing features; batch self-check; log rationales.
  4. Refactor to cohesive modules (maximize cohesion/minimize coupling).
  5. Persist as **RPG L0** (versioned).
- **CHECK:** `<COVERAGE> ≥ <COVERAGE_TARGET>`; modules non-overlapping; rationale log present.
- **IF FAIL:** Increase k/ρ; expand synonyms; induce missing features; re-run 1–4.
- **ARTIFACTS:** <RPG_L0.json>, <PLAN_LOG>
- **SOURCE:** ¶3.2/Alg.2 “exploit–explore + filter/merge”; ¶3.1 dual-semantics note.

### SOP: **Gather** — retrieval, freshness, provenance

- **WHEN:** Planner requests grounding.
- **DO:**

  1. Ensure Feature Tree embeddings loaded with paths/meta.
  2. Run vector_db.search and sample_reject; attach scores/overlap stats.
  3. Record provenance (source tree node IDs, timestamps).
- **CHECK:** Query latency ≤ <EMBED_QPS>; provenance complete.
- **IF FAIL:** Rebuild embeddings; partition index; cache hot queries.
- **ARTIFACTS:** <RETRIEVAL_LOG>, <PROVENANCE_CSV>
- **SOURCE:** Feature Tree embeddings + metadata.

### SOP: **Build/Execute** — implementation & graph-guided TDD

- **WHEN:** RPG L0 approved.
- **DO:**

  1. Map modules→folders; features→files; create skeleton.
  2. Encode typed inter-module data flows and intra-module file ordering; validate DAG.
  3. Design leaf interfaces (func/class) with docstrings; no bodies.
  4. **Topologically** sort RPG into <BUILD_QUEUE>.
  5. For each leaf: **generate unit test → implement → run → commit only on green**.
  6. On edits: run **selective regression** (impacted parents/children).
  7. For completed subgraphs: add **integration tests**.
- **CHECK:** DAG passes; unit pass ≥ <UNIT_PASS_TARGET>; coverage ≥ <MIN_COV>.
- **IF FAIL:** Localize via rpg.localize/repo.view/dep.trace; patch; rerun tests.
- **ARTIFACTS:** <RPG_FULL.json>, <SRC_CODE>, <TESTS>, <CI_REPORT>
- **SOURCE:** §3.3, §4 (topo + TDD + regression/integration).

#### Testing Pitfalls & Guardrails

##### Common pitfalls

- Writing tests that are tied to implementation details (fragile).
- Over-specifying: testing how instead of what.
- Giant steps: skipping the tiny loop, leading to long failing streaks.
- Mocking everything: lose real behavior; mock only true boundaries.

##### Guardrails that make it work

- Keep acceptance tests few and slow but critical; keep unit tests many and fast.
- Contracts are versioned (backward-compatible by default); breaking changes require a contract test update first.
- Make failures actionable: test names read like requirements, outputs diff JSON bodies/schemas, not logs.

### SOP: **Validate** — metrics & policy checks

- **WHEN:** Build completes for milestone/subgraph.
- **DO:**

  1. Majority-vote semantic check (5 votes; 2 rounds) per task.
  2. Attribute failures: **impl vs test/env**; auto-remediate latter.
  3. Compute **Coverage, Novelty, Pass, Voting, Files, LOC, Tokens** as D.3.1.
  4. Run license/secret scan; generate SBOM.
- **CHECK:** `<PASS_RATE> ≥ <PASS_RATE_TARGET>`, `<VOTING_RATE> ≥ <VOTE_RATE_TARGET>`, no license/sec violations.
- **IF FAIL:** Escalate with evidence bundle; increase retries for env/test up to <N_REMEDIATIONS>.
- **ARTIFACTS:** <EVAL_REPORT>, <METRICS.json>, <SBOM>, <GOV_REPORT>
- **SOURCE:** §5/§D.3.1 metrics; validation pipeline.

### SOP: **Deliver/Report** — release & handoff

- **WHEN:** Metrics hit thresholds or HITL approves.
- **DO:**

  1. Emit release notes: scope, build hash, metrics deltas, known issues.
  2. Publish artifacts to <ARTIFACT_STORE>; tag <RPG_VER>.
  3. Schedule next iteration if coverage gap or backlog exists.
- **CHECK:** Artifacts present; checksums match; dashboards updated.
- **IF FAIL:** Rollback to previous green tag; notify stakeholders.
- **ARTIFACTS:** <RELEASE_NOTES>, <ARTIFACT_MANIFEST>
- **SOURCE:** Persistent RPG & staged iteration.

### SOP: **Monitor** — drift & review cadence

- **WHEN:** Post-release or nightly.
- **DO:**

  1. Enforce DAG/schema validation on every RPG change via CI.
  2. Track near-linear scaling of features/LOC; alert on plateau.
  3. Log localization/tool calls for audit; redact PII.
- **CHECK:** CI clean; slope ≥ <MIN_SLOPE>; audit logs ≥ <MIN_LOG_FIELDS>.
- **IF FAIL:** Quarantine change; root-cause; schedule fix.
- **ARTIFACTS:** <CI_LOGS>, <SCALING_DASHBOARD>, <AUDIT_LOG>
- **SOURCE:** Topological CI; scalability analyses; logging examples.

---

## 7) Decision Policies & Guardrails

- **Quality Gates:**

  - `<PASS_RATE> ≥ <PASS_RATE_TARGET>`; `<VOTING_RATE> ≥ <VOTE_RATE_TARGET>`; `<COVERAGE> ≥ <COVERAGE_TARGET>`; `<NOVELTY> ≤ <NOVELTY_MAX>` unless explicitly allowed.
- **Safety Checks:**

  - Enforce license policy; secrets = 0; SBOM emitted; PII redaction in logs.
- **Cost/Time Budgets:**

  - `<MAX_TOOL_CALLS>`, `<MAX_TOKENS>`, `<MAX_WALLTIME>` per stage; **terminate** on breach with evidence bundle.
- **Escalation Matrix:**

  - *What:* DAG cycles, persistent test/env failures, metric shortfall > <DELTA_THRESHOLD>.
  - *To whom:* {{human_roles}} (PM/Eng/QA/Legal/SME).
  - *Bundle:* `{context, diffs, failing tests, votes, logs, metrics_delta}`.
- **Termination Conditions:**

  - All gates green **or** budget exhausted **or** HITL stop; handoff includes <RPG_FULL.json>, code, tests, metrics.

---

## 8) Collaboration Playbooks

- **Multi-Agent Pattern:** `Planner → Implementer → Builder → Verifier` loop until gates pass; max <N_LOOPS>.
- **Consensus/Critique:** Verifier may trigger 1 critique round with <N_CRITICS>; if unresolved → HITL.
- **HITL Interfaces:** Approval prompts include `{summary, changes, risks, budgets, diffs, metrics}`; “request changes” restarts from affected stage only.
- **Communication Etiquette:** Keep messages ≤ <512 tokens>; include `id, intent, constraints, artifacts`.

---

## 9) Observability & Artifacts

- **Logging Fields:** `ts, run_id, agent, tool, input_hash, output_hash, duration_ms, cost, redactions[]`.
- **Redaction Policy:** Strip PII/secrets; hash examples; store raw only in quarantined vault.
- **Artifacts:**

  - Naming: `<proj>-<stage>-<yyyymmdd>-<hash>.<ext>`; store in `<ARTIFACT_STORE>/<stage>/`.
- **Dashboards/Reviews:**

  - Charts: Coverage, Pass/Vote, LOC growth (expect near-linear under RPG), failure classes; weekly review.

---

## 10) Error Handling & Recovery

- **Error Classes:** `validation`, `quota`, `tool`, `network`, `policy`.
- **Playbooks:**

  - `validation`: localize→fix→rerun tests.
  - `quota`: backoff, reduce k/ρ, compress context.
  - `tool`: retry (2), alternate endpoint, stub.
  - `network`: retry with jitter; cache results.
  - `policy`: quarantine, notify Legal, redact & re-run.
- **Rollback:** Revert to last **green** tag (<RPG_VER_prev>); lock write ops until RCA complete.

---

## 11) Configuration Templates

### Agent Spec (YAML)

```yaml
agents:
  - name: Planner
    role: planning
    inputs: [<GOAL_SPEC>, <CONSTRAINTS>]
    outputs: [<RPG_L0.json>, <RISKS>]
    permissions: [read.vector_db, plan, write.plan_log]
  - name: Implementer
    role: implementation
    inputs: [<RPG_L0.json>]
    outputs: [<RPG_FULL.json>, <SKELETON_REPO>]
    permissions: [write.graph, write.repo, validate.dag]
  - name: Builder
    role: build
    inputs: [<RPG_FULL.json>]
    outputs: [<SRC_CODE>, <TESTS>]
    permissions: [exec.tests, write.code]
  - name: Verifier
    role: verify
    inputs: [<SRC_CODE>, <TEST_RESULTS>, <TASK_SET>]
    outputs: [<EVAL_REPORT>, <METRICS.json>]
    permissions: [exec.vote, exec.metrics, exec.governance]
tools:
  - name: vector_db.search
    intent: retrieve exploit paths
    input_schema: {query: <TEXT>, k: <INT>}
    output_schema: {paths: [<PATH_OBJ>]}
    limits: {rate: <N/min>, budget: <$MAX>}
policies:
  quality_gates:
    - metric: pass_rate
      threshold: <PASS_RATE_TARGET>
    - metric: coverage
      threshold: <COVERAGE_TARGET>
```

### Message Contract (JSON)

```json
{
  "id": "<UUID>",
  "from": "<AGENT>",
  "to": "<AGENT|HUMAN>",
  "intent": "<PLAN|EXECUTE|VERIFY|ESCALATE>",
  "content": "<BRIEF>",
  "constraints": ["<RULE1>", "<RULE2>"],
  "artifacts": ["<URI1>", "<URI2>"]
}
```

---

## 12) Examples (Few-Shot)

**Example A — Planner → Builder handoff**

- **Planner output (abridged):**
  `RPG_L0.json` with modules `{algorithms, evaluation, data_loading}` and candidate paths (k=50 exploit + explore set), refactored into cohesive modules. *(Source: proposal-level construction)*
- **Implementer:** maps to `src/algos`, `src/eval`, `src/data_load`; assigns files; encodes flows `data_load → algos → eval`; defines interfaces `DataLoader`, `LinearModels`.
- **Builder:** topo queue then (unit test → implement → commit on green).

**Example B — Verifier blocks & escalates**

- Voting confirms presence (4/5) but **execution fails**; diagnosis labels **test/env** → auto remediation; if still failing, **escalate** with logs, diffs, and metrics to QA.

---

## 13) Traceability

Below are key sources used; each SOP/policy/tool above includes an inline **SOURCE** with paragraph cue.

- **RPG Structure & Dual Semantics; Inter-/Intra- edges; Topological Order.** §3.1 (“nodes specify… files/classes/functions… edges encode data flows/order”).
- **Proposal-Level Construction:** Feature Tree embeddings; exploit–explore; LLM filter/merge; refactor by cohesion/coupling; Algorithm 2.
- **Implementation-Level Construction:** Folder/file encoding; typed flows; adaptive interfaces (func vs class).
- **Graph-Guided Generation:** Topological traversal; **TDD**; regression & integration testing.
- **Localization & Editing Tools:** rpg-guided search, repo view, dependency exploration.
- **Validation Pipeline:** Majority-vote semantic check; failure attribution; remediation loops.
- **Metrics:** Coverage, Novelty, Pass, Voting, Files/LOC/Tokens (Appendix D.3.1).
- **Governance Additions (SBOM, licenses, secrets):** operational risk mitigation derived in build guide.
- **Scalability & Monitoring:** Near-linear growth expectations; CI DAG validation; logging exemplars.

---

## 14) Temporary Priority Dual-Labeling (Taskmaster + RPG)

**Goal:** Use Taskmaster’s priority for scheduling while making the RPG priority explicit—without changing Taskmaster’s schema.

**Authoritative field (unchanged):** `priority` ∈ `{high, medium, low}`
**RPG mapping:** `P0→high`, `P1→medium`, `P2→low`

### What to do (apply to every task and subtask)

1. **Prefix the title (visible)**

   - Format: `[P*] <Title>`
   - Optional ID: `[P*|<RPG_ID>] <Title>`
   - Examples: `[P0] Initialize repo`, `[P1|DSC-001] Define project spec`

2. **Stamp the details (machine-friendly)**

   - Insert as the **first line** of `details`:

     - `RPG_PRIORITY=P*`
     - Optional: `RPG_ID=<RPG_ID>` on the next line
   - Preserve the rest of `details` as-is after a blank line.

### Do / Don’t

- ✅ **Do** keep `priority` exactly `high|medium|low`.
- ✅ **Do** dual-label with `[P*]` in `title` **and** `RPG_PRIORITY=P*` in `details`.
- ✅ **Do** let subtasks inherit the parent’s P* if their own `priority` is missing.
- ❌ **Don’t** add new top-level fields (e.g., `rpg_priority`) or wrappers.
- ❌ **Don’t** rename or re-case Taskmaster’s `priority` values.

### Quick reference

| RPG | Taskmaster `priority` | Title Prefix | Details Stamp     |
| --- | --------------------- | ------------ | ----------------- |
| P0  | high                  | `[P0]`       | `RPG_PRIORITY=P0` |
| P1  | medium                | `[P1]`       | `RPG_PRIORITY=P1` |
| P2  | low                   | `[P2]`       | `RPG_PRIORITY=P2` |

### Example (before → after)

**Before**

```json
{
  "title": "Initialize repository",
  "priority": "high",
  "details": "Create repo, add README.",
  "subtasks": [
    { "title": "Add CI", "priority": "medium", "details": "Set up basic workflow." }
  ]
}
```

**After**

```json
{
  "title": "[P0] Initialize repository",
  "priority": "high",
  "details": "RPG_PRIORITY=P0\n\nCreate repo, add README.",
  "subtasks": [
    {
      "title": "[P1] Add CI",
      "priority": "medium",
      "details": "RPG_PRIORITY=P1\n\nSet up basic workflow."
    }
  ]
}
```

**Validation checklist (agents):**

- Title starts with `[P0]`, `[P1]`, or `[P2]`.
- `details` first line is `RPG_PRIORITY=P*`.
- Taskmaster `priority` untouched and consistent with the P* mapping.
- No extra top-level fields added.

---

**End of AGENTS.md**


--- rpg_tool/execution-order.md ---
Here’s the **execution order** that’s aligned to the RPG pipeline — **Discover → Design → Build → Validate → Benchmark → Monitor** — and consistent with the whitepaper’s sequence (implementation-level refinement before graph-guided topo build). Where a task’s minimal deps would allow earlier execution (e.g., GEN-001), I place it later so the build plan captures intra-module order and full constraints, which is the safer choice for stability. (Build Guide trace matrix; Whitepaper Fig. 1 “B1/B2 before C”. )

1. **4 — [DSC-001] Develop Project Spec Parser**.
2. **1 — [DSC-002] Define Detailed RPG Schema**.
3. **2 — Extend Graph Service for Typed Nodes and Edges** (uses schema).
4. **6 — [DSC-003] Build Feature Tree Index** (spec → feature tree).
5. **7 — [PROP-001] Exploit Retrieval (top-k)**.
6. **8 — [PROP-002] Explore Sampling (diversity)**.
7. **9 — [PROP-003] LLM Filter & Merge**.
8. **10 — [PROP-004] Refactor into Cohesive Modules**.
9. **11 — [PROP-005] Persist L0 Functionality Graph**.
10. **5 — [IMPL-001] Map Nodes → Folders** (L0→L1).
11. **12 — [IMPL-002] Group Capabilities into Files** (L1→L2).
12. **13 — [IMPL-003] Encode Typed Data-Flows**.
13. **26 — [MON-001] CI Gate: DAG & Schema** (turn on guardrails now that data-flows exist).
14. **14 — [IMPL-004] Define Intra-Module File Order**.
15. **30 — [IMPL-004] Intra-Module Order Lints**.
16. **15 — [IMPL-005] Abstract Base Classes & Shared Types** (usage-driven).
17. **16 — [IMPL-006] One Interface per Leaf**.
18. **3 — [GEN-001] Topological Build Plan Generator** (placed **after** IMPL so order edges & data-flows are respected).
19. **27 — [MON-002] Audit Logging of Localization Steps** (depends on planner).
20. **17 — [GEN-002] Auto-Derive Unit Tests**.
21. **18 — [GEN-003] Implement-to-Green & Coverage Gate**.
22. **19 — [GEN-004] Selective Regression by Impact**.
23. **20 — [GEN-005] Integration Tests per Subgraph**.
24. **21 — [VAL-001] Majority-Vote Semantic Validator**.
25. **22 — [VAL-002] Failure Attribution & Auto-Remediation**.
26. **23 — [VAL-003] RepoCraft Metrics (Coverage/Novelty/Pass/Vote/Files/LOC/Tokens)**.
27. **24 — [BMR-001] Build Benchmark Task Set**.
28. **25 — [BMR-002] Evaluation Loop per Task**.
29. **28 — [MON-003] Governance: SBOM, Licenses, Secrets** (after core GEN loop exists).
30. **29 — [MON-004] Scaling Dashboards** (after metrics).

### Machine-readable order

```json
{
  "execution_order": [
    {"id": 4, "title": "[DSC-001] Develop Project Spec Parser"},
    {"id": 1, "title": "[DSC-002] Define Detailed RPG Schema"},
    {"id": 2, "title": "Extend Graph Service for Typed Nodes and Edges"},
    {"id": 6, "title": "[DSC-003] Build Feature Tree Index"},
    {"id": 7, "title": "[PROP-001] Exploit Retrieval"},
    {"id": 8, "title": "[PROP-002] Explore Sampling"},
    {"id": 9, "title": "[PROP-003] LLM Filter & Merge"},
    {"id": 10, "title": "[PROP-004] Refactor into Cohesive Modules"},
    {"id": 11, "title": "[PROP-005] Persist L0 Functionality Graph"},
    {"id": 5, "title": "[IMPL-001] Map Graph Nodes to File System Structure"},
    {"id": 12, "title": "[IMPL-002] Group Capabilities into Files"},
    {"id": 13, "title": "[IMPL-003] Encode Typed Data-Flows"},
    {"id": 26, "title": "[MON-001] CI Gate: DAG & Schema"},
    {"id": 14, "title": "[IMPL-004] Define Intra-Module File Order"},
    {"id": 30, "title": "[IMPL-004] Intra-Module Order Lints"},
    {"id": 15, "title": "[IMPL-005] Abstract Base Classes & Shared Types"},
    {"id": 16, "title": "[IMPL-006] Define One Interface per Leaf"},
    {"id": 3, "title": "[GEN-001] Topological Build Plan Generator"},
    {"id": 27, "title": "[MON-002] Audit Logging of Localization Steps"},
    {"id": 17, "title": "[GEN-002] Auto-Derive Unit Tests"},
    {"id": 18, "title": "[GEN-003] Implement to Green & Gate Commits"},
    {"id": 19, "title": "[GEN-004] Selective Regression by Impact"},
    {"id": 20, "title": "[GEN-005] Add Integration Tests per Subgraph"},
    {"id": 21, "title": "[VAL-001] Majority-Vote Semantic Validator"},
    {"id": 22, "title": "[VAL-002] Failure Attribution & Auto-Remediation"},
    {"id": 23, "title": "[VAL-003] RepoCraft Metrics"},
    {"id": 24, "title": "[BMR-001] Build Benchmark Task Set"},
    {"id": 25, "title": "[BMR-002] Evaluation Loop per Task"},
    {"id": 28, "title": "[MON-003] Governance: SBOM, Licenses, Secrets"},
    {"id": 29, "title": "[MON-004] Scaling Dashboards"}
  ]
}
```

**Why this order:** It respects all declared deps in your task list while enforcing the paper/guide’s staging: proposal & implementation refinement precede topo build and codegen, which then feed validation, benchmarking, and monitoring. This avoids unstable plans and ensures the build queue is constrained by both inter-module data-flows and intra-module order. (Whitepaper Fig. 1; Build Guide §Trace Matrix IMPL→GEN→VAL→BMR→MON).


--- rpg_tool/.codex/AGENTS.md ---
# AGENTS.md — Tool Selection (Python)

When you need to call tools from the shell, use this rubric:

## File & Text

- Find files by file name: `fd`
- Find files with path name: `fd -p <file-path>`
- List files in a directory: `fd . <directory>`
- Find files with extension and pattern: `fd -e <extension> <pattern>`
- Find Text: `rg` (ripgrep)
- Find Code Structure: `ast-grep`
  - Common languages:
    - Python → `ast-grep --lang python -p '<pattern>'`
    - TypeScript → `ast-grep --lang ts -p '<pattern>'`
    - Bash → `ast-grep --lang bash -p '<pattern>'`
    - TSX (React) → `ast-grep --lang tsx -p '<pattern>'`
    - JavaScript → `ast-grep --lang js -p '<pattern>'`
    - Rust → `ast-grep --lang rust -p '<pattern>'`
    - JSON → `ast-grep --lang json -p '<pattern>'`
  - Prefer `ast-grep` over ripgrep/grep unless a plain-text search is explicitly requested.
- Select among matches: pipe to `fzf`

## Data

- JSON: `jq`
- YAML/XML: `yq`

## Python Tooling

- Package Management & Virtual Envs: `uv`
  (fast replacement for pip/pip-tools/virtualenv; use `uv pip install ...`, `uv run ...`)
- Linting & Formatting: `ruff`
  (linter + formatter; use `ruff check .`, `ruff format .`)
- Static Typing: `mypy`
  (type checking; use `mypy .`)
- Security: `bandit`
  (Python security linter; use `bandit -r .`)
- Testing: `pytest`
  (test runner; use `pytest -q`, `pytest -k <pattern>` to filter tests)
- Logging: `loguru`
  (runtime logging utility; import in code:

  ```python
  from loguru import logger
  logger.info("message")
  ```)

## Notes

- Prefer uv for Python dependency and environment management instead of pip/venv/poetry/pip-tools.


You are **RPG Assistant**, an expert grounded in two attached documents that are your **only sources of truth**: (1) the official whitepaper *“RPG: A Repository Planning Graph for Unified and Scalable Codebase Generation”*  and (2) the operational *“RPG build guide.md”* . Answer the user’s questions **strictly by citing these docs**—prefer the whitepaper for concepts/definitions and the build guide for procedures. If they conflict, state both briefly, then adopt the whitepaper’s definition and the build guide’s implementation steps.

### What to do

1. **Identify intent & scope** of the user’s query (conceptual vs. how-to vs. troubleshooting vs. evaluation).
2. **Locate** the relevant sections:

   - Concepts, pipeline, metrics, RepoCraft, RPG structure/topology → **whitepaper** (e.g., RPG nodes/edges, proposal/implementation stages, graph-guided TDD, coverage/novelty metrics).
   - Step-by-step actions, IDs, acceptance criteria, tooling, CI rules → **build guide** (e.g., DSC-001, PROP-001..005, IMPL-001..006, GEN-001..005, VAL/BMR/MON).
3. **Answer with evidence** (short quotes ≤20 words only when necessary) and **inline citations** to the exact doc you used.
4. **If info is missing** from both docs, say so plainly, list the exact missing pieces, and suggest the nearest doc-backed alternative—not external facts.
5. **Never speculate** or introduce outside material unless the user explicitly asks you to broaden beyond these docs.

### How to format every answer

- **Answer:** A crisp, user-level response (bullets or short paragraphs).
- **RPG mapping:** Name the relevant RPG stage(s) or build-guide step IDs (e.g., “IMPL-003 data-flow DAG”).
- **Steps / Checklist (if procedural):** Ordered actions referencing build-guide IDs with one-line acceptance criteria.
- **Citations:** Inline after the sentence(s) they support; cite the specific source (whitepaper vs build guide).
- **Assumptions (if any):** Keep to one short line, and only if unavoidable.

### Style & guardrails

- Be precise and actionable; keep replies focused and brief.
- Do not expose chain-of-thought; use concise rationales instead.
- Use the build-guide taxonomy when proposing work (**Discover → Design → Build → Validate → Benchmark → Monitor**).
- Use whitepaper terminology for RPG semantics (nodes: capability/folder/file/class/function; edges: hierarchy/data-flow/order; DAG/topological traversal).
- For metrics, report Coverage, Novelty, Pass/Vote rates, Files/LOC/Tokens exactly as defined.

### Examples of acceptable references in your prose

- “Encode inter-module typed I/O and ensure a DAG (IMPL-003).”
- “Traverse RPG in topological order with graph-guided TDD.”

**Important:** All factual claims must be backed by one of the two sources above, with inline citations. If the user asks for comparisons, outside tooling, or unrelated frameworks, politely state that such content is outside the current knowledge bank and ask whether they want to broaden scope.


--- rpg_tool/.taskmaster/AGENTS.md ---
# Task Master AI - Agent Integration Guide

## Essential Commands

### Core Workflow Commands

```bash
# Project Setup
task-master init                                    # Initialize Task Master in current project
task-master parse-prd .taskmaster/docs/prd.txt      # Generate tasks from PRD document
task-master models --setup                        # Configure AI models interactively

# Daily Development Workflow
task-master list                                   # Show all tasks with status
task-master next                                   # Get next available task to work on
task-master show <id>                             # View detailed task information (e.g., task-master show 1.2)
task-master set-status --id=<id> --status=done    # Mark task complete

# Task Management
task-master add-task --prompt="description" --research        # Add new task with AI assistance
task-master expand --id=<id> --research --force              # Break task into subtasks
task-master update-task --id=<id> --prompt="changes"         # Update specific task
task-master update --from=<id> --prompt="changes"            # Update multiple tasks from ID onwards
task-master update-subtask --id=<id> --prompt="notes"        # Add implementation notes to subtask

# Analysis & Planning
task-master analyze-complexity --research          # Analyze task complexity
task-master complexity-report                      # View complexity analysis
task-master expand --all --research               # Expand all eligible tasks

# Dependencies & Organization
task-master add-dependency --id=<id> --depends-on=<id>       # Add task dependency
task-master move --from=<id> --to=<id>                       # Reorganize task hierarchy
task-master validate-dependencies                            # Check for dependency issues
task-master generate                                         # Update task markdown files (usually auto-called)
```

## Key Files & Project Structure

### Core Files

- `.taskmaster/tasks/tasks.json` - Main task data file (auto-managed)
- `.taskmaster/config.json` - AI model configuration (use `task-master models` to modify)
- `.taskmaster/docs/prd.txt` - Product Requirements Document for parsing
- `.taskmaster/tasks/*.txt` - Individual task files (auto-generated from tasks.json)
- `.env` - API keys for CLI usage

### Claude Code Integration Files

- `CLAUDE.md` - Auto-loaded context for Claude Code (this file)
- `.claude/settings.json` - Claude Code tool allowlist and preferences
- `.claude/commands/` - Custom slash commands for repeated workflows
- `.mcp.json` - MCP server configuration (project-specific)

### Directory Structure

```
project/
├── .taskmaster/
│   ├── tasks/              # Task files directory
│   │   ├── tasks.json      # Main task database
│   │   ├── task-1.md      # Individual task files
│   │   └── task-2.md
│   ├── docs/              # Documentation directory
│   │   ├── prd.txt        # Product requirements
│   ├── reports/           # Analysis reports directory
│   │   └── task-complexity-report.json
│   ├── templates/         # Template files
│   │   └── example_prd.txt  # Example PRD template
│   └── config.json        # AI models & settings
├── .claude/
│   ├── settings.json      # Claude Code configuration
│   └── commands/         # Custom slash commands
├── .env                  # API keys
├── .mcp.json            # MCP configuration
└── CLAUDE.md            # This file - auto-loaded by Claude Code
```

## MCP Integration

Task Master provides an MCP server that Claude Code can connect to. Configure in `.mcp.json`:

```json
{
  "mcpServers": {
    "task-master-ai": {
      "command": "npx",
      "args": ["-y", "task-master-ai"],
      "env": {
        "ANTHROPIC_API_KEY": "your_key_here",
        "PERPLEXITY_API_KEY": "your_key_here",
        "OPENAI_API_KEY": "OPENAI_API_KEY_HERE",
        "GOOGLE_API_KEY": "GOOGLE_API_KEY_HERE",
        "XAI_API_KEY": "XAI_API_KEY_HERE",
        "OPENROUTER_API_KEY": "OPENROUTER_API_KEY_HERE",
        "MISTRAL_API_KEY": "MISTRAL_API_KEY_HERE",
        "AZURE_OPENAI_API_KEY": "AZURE_OPENAI_API_KEY_HERE",
        "OLLAMA_API_KEY": "OLLAMA_API_KEY_HERE"
      }
    }
  }
}
```

### Essential MCP Tools

```javascript
help; // = shows available taskmaster commands
// Project setup
initialize_project; // = task-master init
parse_prd; // = task-master parse-prd

// Daily workflow
get_tasks; // = task-master list
next_task; // = task-master next
get_task; // = task-master show <id>
set_task_status; // = task-master set-status

// Task management
add_task; // = task-master add-task
expand_task; // = task-master expand
update_task; // = task-master update-task
update_subtask; // = task-master update-subtask
update; // = task-master update

// Analysis
analyze_project_complexity; // = task-master analyze-complexity
complexity_report; // = task-master complexity-report
```

## Claude Code Workflow Integration

### Standard Development Workflow

#### 1. Project Initialization

```bash
# Initialize Task Master
task-master init

# Create or obtain PRD, then parse it
task-master parse-prd .taskmaster/docs/prd.txt

# Analyze complexity and expand tasks
task-master analyze-complexity --research
task-master expand --all --research
```

If tasks already exist, another PRD can be parsed (with new information only!) using parse-prd with --append flag. This will add the generated tasks to the existing list of tasks..

#### 2. Daily Development Loop

```bash
# Start each session
task-master next                           # Find next available task
task-master show <id>                     # Review task details

# During implementation, check in code context into the tasks and subtasks
task-master update-subtask --id=<id> --prompt="implementation notes..."

# Complete tasks
task-master set-status --id=<id> --status=done
```

#### 3. Multi-Claude Workflows

For complex projects, use multiple Claude Code sessions:

```bash
# Terminal 1: Main implementation
cd project && claude

# Terminal 2: Testing and validation
cd project-test-worktree && claude

# Terminal 3: Documentation updates
cd project-docs-worktree && claude
```

### Custom Slash Commands

Create `.claude/commands/taskmaster-next.md`:

```markdown
Find the next available Task Master task and show its details.

Steps:

1. Run `task-master next` to get the next task
2. If a task is available, run `task-master show <id>` for full details
3. Provide a summary of what needs to be implemented
4. Suggest the first implementation step
```

Create `.claude/commands/taskmaster-complete.md`:

```markdown
Complete a Task Master task: $ARGUMENTS

Steps:

1. Review the current task with `task-master show $ARGUMENTS`
2. Verify all implementation is complete
3. Run any tests related to this task
4. Mark as complete: `task-master set-status --id=$ARGUMENTS --status=done`
5. Show the next available task with `task-master next`
```

## Tool Allowlist Recommendations

Add to `.claude/settings.json`:

```json
{
  "allowedTools": [
    "Edit",
    "Bash(task-master *)",
    "Bash(git commit:*)",
    "Bash(git add:*)",
    "Bash(npm run *)",
    "mcp__task_master_ai__*"
  ]
}
```

## Configuration & Setup

### API Keys Required

At least **one** of these API keys must be configured:

- `ANTHROPIC_API_KEY` (Claude models) - **Recommended**
- `PERPLEXITY_API_KEY` (Research features) - **Highly recommended**
- `OPENAI_API_KEY` (GPT models)
- `GOOGLE_API_KEY` (Gemini models)
- `MISTRAL_API_KEY` (Mistral models)
- `OPENROUTER_API_KEY` (Multiple models)
- `XAI_API_KEY` (Grok models)

An API key is required for any provider used across any of the 3 roles defined in the `models` command.

### Model Configuration

```bash
# Interactive setup (recommended)
task-master models --setup

# Set specific models
task-master models --set-main claude-3-5-sonnet-20241022
task-master models --set-research perplexity-llama-3.1-sonar-large-128k-online
task-master models --set-fallback gpt-4o-mini
```

## Task Structure & IDs

### Task ID Format

- Main tasks: `1`, `2`, `3`, etc.
- Subtasks: `1.1`, `1.2`, `2.1`, etc.
- Sub-subtasks: `1.1.1`, `1.1.2`, etc.

### Task Status Values

- `pending` - Ready to work on
- `in-progress` - Currently being worked on
- `done` - Completed and verified
- `deferred` - Postponed
- `cancelled` - No longer needed
- `blocked` - Waiting on external factors

### Task Fields

```json
{
  "id": "1.2",
  "title": "Implement user authentication",
  "description": "Set up JWT-based auth system",
  "status": "pending",
  "priority": "high",
  "dependencies": ["1.1"],
  "details": "Use bcrypt for hashing, JWT for tokens...",
  "testStrategy": "Unit tests for auth functions, integration tests for login flow",
  "subtasks": []
}
```

## Claude Code Best Practices with Task Master

### Context Management

- Use `/clear` between different tasks to maintain focus
- This CLAUDE.md file is automatically loaded for context
- Use `task-master show <id>` to pull specific task context when needed

### Iterative Implementation

1. `task-master show <subtask-id>` - Understand requirements
2. Explore codebase and plan implementation
3. `task-master update-subtask --id=<id> --prompt="detailed plan"` - Log plan
4. `task-master set-status --id=<id> --status=in-progress` - Start work
5. Implement code following logged plan
6. `task-master update-subtask --id=<id> --prompt="what worked/didn't work"` - Log progress
7. `task-master set-status --id=<id> --status=done` - Complete task

### Complex Workflows with Checklists

For large migrations or multi-step processes:

1. Create a markdown PRD file describing the new changes: `touch task-migration-checklist.md` (prds can be .txt or .md)
2. Use Taskmaster to parse the new prd with `task-master parse-prd --append` (also available in MCP)
3. Use Taskmaster to expand the newly generated tasks into subtasks. Consdier using `analyze-complexity` with the correct --to and --from IDs (the new ids) to identify the ideal subtask amounts for each task. Then expand them.
4. Work through items systematically, checking them off as completed
5. Use `task-master update-subtask` to log progress on each task/subtask and/or updating/researching them before/during implementation if getting stuck

### Git Integration

Task Master works well with `gh` CLI:

```bash
# Create PR for completed task
gh pr create --title "Complete task 1.2: User authentication" --body "Implements JWT auth system as specified in task 1.2"

# Reference task in commits
git commit -m "feat: implement JWT auth (task 1.2)"
```

### Parallel Development with Git Worktrees

```bash
# Create worktrees for parallel task development
git worktree add ../project-auth feature/auth-system
git worktree add ../project-api feature/api-refactor

# Run Claude Code in each worktree
cd ../project-auth && claude    # Terminal 1: Auth work
cd ../project-api && claude     # Terminal 2: API work
```

## Troubleshooting

### AI Commands Failing

```bash
# Check API keys are configured
cat .env                           # For CLI usage

# Verify model configuration
task-master models

# Test with different model
task-master models --set-fallback gpt-4o-mini
```

### MCP Connection Issues

- Check `.mcp.json` configuration
- Verify Node.js installation
- Use `--mcp-debug` flag when starting Claude Code
- Use CLI as fallback if MCP unavailable

### Task File Sync Issues

```bash
# Regenerate task files from tasks.json
task-master generate

# Fix dependency issues
task-master fix-dependencies
```

DO NOT RE-INITIALIZE. That will not do anything beyond re-adding the same Taskmaster core files.

## Important Notes

### AI-Powered Operations

These commands make AI calls and may take up to a minute:

- `parse_prd` / `task-master parse-prd`
- `analyze_project_complexity` / `task-master analyze-complexity`
- `expand_task` / `task-master expand`
- `expand_all` / `task-master expand --all`
- `add_task` / `task-master add-task`
- `update` / `task-master update`
- `update_task` / `task-master update-task`
- `update_subtask` / `task-master update-subtask`

### File Management

- Never manually edit `tasks.json` - use commands instead
- Never manually edit `.taskmaster/config.json` - use `task-master models`
- Task markdown files in `tasks/` are auto-generated
- Run `task-master generate` after manual changes to tasks.json

### Claude Code Session Management

- Use `/clear` frequently to maintain focused context
- Create custom slash commands for repeated Task Master workflows
- Configure tool allowlist to streamline permissions
- Use headless mode for automation: `claude -p "task-master next"`

### Multi-Task Updates

- Use `update --from=<id>` to update multiple future tasks
- Use `update-task --id=<id>` for single task updates
- Use `update-subtask --id=<id>` for implementation logging

### Research Mode

- Add `--research` flag for research-based AI enhancement
- Requires a research model API key like Perplexity (`PERPLEXITY_API_KEY`) in environment
- Provides more informed task creation and updates
- Recommended for complex technical tasks

---

_This guide ensures Claude Code has immediate access to Task Master's essential functionality for agentic development workflows._


--- rpg_tool/.github/pull_request_tempate.md ---
<!-- path: .github/pull_request_template.md -->
# PR Checklist — DSC‑001 Gate


**Purpose:** Ensure the project spec meets DSC‑001 before merging.


- [ ] Spec lives at `docs/PROJECT_SPEC.md` (or uses the template name).
- [ ] Sections present & non‑empty: **Goals · Non‑Goals · Constraints · Acceptance Metrics**.
- [ ] Stakeholders listed and marked **approved**.
- [ ] (If changed) `tools/validate_project_spec` run locally and in CI.


> Paste the validator output here (OK / errors):


--- rpg_tool/templates/RPG-prompt-tool_templates.md ---
copy-pasteable **prompt templates** distilled from your RPG materials, grouped by stage and preserved in their original “think/action/solution” styles. I standardized placeholders (ALL\_CAPS) and kept the exact response formats so you can drop them straight into your system.

* * *

## Proposal-Level (Feature Retrieval & Refactor)
=============================================

## 1) Exploitation Paths — Select from High-Relevance Candidates

Use when you have a curated “Exploit Feature Tree” and want precise additions only from it. RPG_REPOSITORY_PLANNING_GRAPH

```md
    SYSTEM
    You are a GitHub project assistant responsible for expanding a repository’s feature tree through path-based modifications.

    You will be given:
    - An Exploit Feature Tree (high-relevance candidate paths) → EXPLOIT_FEATURE_TREE
    - The Current Repository Feature Tree → CURRENT_REPO_TREE

    Objective
    1) Align with the repository’s purpose/scope.
    2) Achieve broad coverage across core areas.
    3) Ensure essential capabilities are represented.
    4) Identify and fill critical gaps.

    Selection Principles
    - Select exclusively from the Exploit Feature Tree.
    - Include all non-duplicated, useful paths.
    - Maintain structural balance by covering underrepresented modules.

    Exclusions
    - Skip generic infra (e.g., logging, configuration).
    - Skip abstract goals (e.g., “optimize CPU usage”).

    Response Format
    Respond only with a <think> and <action> block.

    <think>
    Briefly justify why each selected path improves coverage and cohesion.
    </think>
    <action>
    {
      "all_selected_feature_paths": [
        "path/to/feature", ...
      ]
    }
    </action>
```

## 2) Exploration Paths — Expand Into New, Unvisited Areas

Use when you have a sampled “Exploration Tree” to diversify coverage. RPG_REPOSITORY_PLANNING_GRAPH

```md
    SYSTEM
    You are a GitHub project assistant expanding a repository’s feature tree via path-based modifications.

    Inputs:
    - A Sampled Feature Tree (Exploration Tree) → EXPLORATION_TREE
    - The Current Repository Feature Tree → CURRENT_REPO_TREE

    Objective
    - Improve/expand coverage aligned to real usage scenarios.
    - Capture supporting areas without duplicating existing paths.

    Selection Principles
    - Select only from the Exploration Tree.
    - Include actionable, domain-relevant features.
    - Exclude any path already present in the current tree.
    - Slight over-inclusion is acceptable.

    Exclusions
    - Exclude generic infra and large, unfocused integrations.

    Response Format
    <think>
    Explain how each Exploration Tree path was evaluated and why it was included or excluded.
    </think>
    <action>
    {
      "all_selected_feature_paths": [
        "path/to/feature", ...
      ]
    }
    </action>
```

## 3) Propose Missing Features — Fill Gaps Beyond Current Tree

Use to enumerate concrete, implementable capabilities the repo still needs. RPG_REPOSITORY_PLANNING_GRAPH

```md
    SYSTEM
    You are a GitHub project assistant designing a functionally complete, production-grade repository.
    Focus on intended functionality — not the existing Feature Tree (it may be incomplete).

    Objective
    Identify groups of concrete, implementable features that:
    1) Align with domain and purpose,
    2) Are missing or only superficially represented,
    3) Are specific (functions/classes/modules/algorithms).

    Inclusion
    - Code-level operations only; implementable in this repo’s scope; include standard and advanced algorithms.

    Exclusion
    - No abstract intentions, generic infra, placeholders, or duplicates.

    Naming Rules
    - 3–5 lowercase words, space separated (no snake/camel case).
    - Leaf nodes must describe concrete behavior.

    Structure
    - Organize into logical hierarchies (≤ 4–5 levels) reflecting computation/workflows.

    Response Format
    <think>
    Reason about domains, workflows, algorithms missing from CURRENT_REPO_TREE but expected in real use.
    </think>
    <action>
    {
      "missing_features": {
        "ROOT_NODE": {
          "child node 1": ["leaf feature 1", "leaf feature 2"],
          "child node 2": ["leaf feature 3", "leaf feature 4"]
        }
      }
    }
    </action>
```

* * *

## Implementation-Level (Structure, Flows, Interfaces)
===================================================

## 4) Inter-Subtree Data-Flow (DAG) — Define Typed Edges

Use to produce a repository-level DAG of data exchanges across modules. RPG_REPOSITORY_PLANNING_GRAPH

```md
    SYSTEM
    You are a system architect defining inter-subtree data flow for a Python repository as a structured DAG.

    Inputs
    - Subtree names → TREES_NAMES

    Constraints
    - Full connectivity: every subtree in TREES_NAMES appears in ≥ 1 edge.
    - Acyclic structure (DAG).
    - Field guidelines:
      - data_id: unique, descriptive exchange id
      - data_type: precise, interpretable type/schema
      - transformation: short summary or "none"

    Output Format
    <solution>
    [
      {
        "from": "SOURCE_SUBTREE",
        "to": "TARGET_SUBTREE",
        "data_id": "UNIQUE_DATA_NAME",
        "data_type": "TYPE_OR_SCHEMA",
        "transformation": "SUMMARY_OR_none"
      },
      ...
    ]
    </solution>
```

## 5) Raw Skeleton Mapping — Top-Level Folder Plan

Use to assign subtrees to a clean, Pythonic folder skeleton (no code). RPG_REPOSITORY_PLANNING_GRAPH

```md
    SYSTEM
    You are a repository architect designing a clean, modular file system skeleton.

    Requirements
    1) Separate functional subtrees clearly; reflect logical domain boundaries.
    2) Pythonic names (snake_case), concise & developer-friendly.
    3) Subtree names are labels; rename folders for clarity but include exact subtree mapping.
    4) Flat or nested (e.g., under "src/")—choose what improves clarity/scalability.
    5) Include auxiliary folders if appropriate (e.g., tests, scripts, docs).
    6) Avoid unnecessary nesting/complexity.

    Output Format (single JSON-like object)
    - "folder_name": ["Exact Subtree Name"] → folder assigned to subtree
    - "folder_name": [] → utility/support folder
    - "file_name.ext": null → file placeholder

    <solution>
    {
      "src": [],
      "src/module_a": ["SUBTREE_A"],
      "src/module_b": ["SUBTREE_B"],
      "README.md": null,
      "pyproject.toml": null
    }
    </solution>
```

## 6) Map Feature Paths → Python Skeleton Files (Incremental)

Use iteratively to place unassigned leaf paths into `.py` files under a designated module. RPG_REPOSITORY_PLANNING_GRAPH

```md
    SYSTEM
    You are assigning remaining leaf-level features into Python files.

    Inputs
    - Unassigned leaf features → UNASSIGNED_FEATURE_PATHS (full "a/b/c" style)
    - Designated functional folder → DESIGNATED_FOLDER
    - Partial skeleton (existing assignments hidden) → PARTIAL_SKELETON

    Goals
    - Paths begin with DESIGNATED_FOLDER.
    - Group semantically related features.
    - Reflect real modularization (avoid dumping unrelated features into single files).
    - If a folder hits ≥ 10 files, add semantic subfolders.

    Naming & Organization
    - Production-grade structure; logical modules; meaningful filenames.

    Response Format
    <think>
    Explain grouping choices and how they improve clarity/cohesion.
    </think>
    <solution>
    {
      "path/to/file1.py": ["feature/path/one", "feature/path/two"],
      "path/to/file2.py": ["feature/path/three"]
    }
    </solution>
```

## 7) Convert Subgraphs → Base Classes & Shared Types

Use to design minimal, justified abstractions shared across modules. RPG_REPOSITORY_PLANNING_GRAPH

```md
    SYSTEM
    Design reusable abstractions and shared data structures.

    Design Strategy
    - Shared data structures for nodes with high out-degree (widely consumed outputs).
    - Functional base classes for nodes with high in-degree (common lifecycle/roles).
    - Prefer 1–3 global classes; avoid speculative abstractions.

    Output Format
    Provide grouped code blocks by subtree and file (signatures only; can be skeletons).

    <think>
    Justify each abstraction (why global; how it enforces consistency; where it’s used).
    </think>
    <solution>
    ## SubtreeName
    ### path/to/file.py
    ```python
    from abc import ABC, abstractmethod

    class BaseComponent(ABC):
        """Lifecycle: initialize → process(data) → finalize."""
        def initialize(self) -> None: ...
        @abstractmethod
        def process(self, data): ...
        def finalize(self) -> None: ...
```

</solution> \`\`\`

### 8) Map Features → Interfaces (Functions/Classes; No Impl)

Use to author import lines, signatures, docstrings (with args/returns/assumptions) for each feature. RPG_REPOSITORY_PLANNING_GRAPH

```md
    SYSTEM
    Design modular interfaces for a large-scale Python system.

    Context Provided
    - Overview, tree structure, skeleton, data flow, base classes, upstream interfaces
    - Target subtree → TARGET_SUBTREE
    - Target file → TARGET_FILE

    Objective
    - For each feature, define exactly one interface (function or class).
    - Include imports, signature, and detailed docstring (purpose, args, returns, assumptions).
    - No implementation: use `pass`.
    - One interface per block.

    Design Guidelines
    - Use functions for simple/atomic/stateless steps.
    - Use classes when stateful, multi-method, inherits base class, or designed for extension.
    - Prefer fewer, well-justified abstractions; group only tightly related features.

    Response Format
    <think>
    Explain mapping and interface choices.
    </think>
    <solution>
    design_itfs_for_feature(features=["feature/path/one", "feature/path/two"]):
    ```python
    from typing import Any, Iterable

    def normalize_text(text: str) -> str:
        """Normalize input text.
        Args: text: raw string
        Returns: normalized string
        Assumptions: language-agnostic
        """
        pass
```

</solution> \`\`\`

* * *

## Where these came from
=====================

The templates above are lifted and normalized from the RPG paper’s prompt sections: Proposal-level (Appendix A.3) and Implementation-level (Appendix B.1), plus the data-flow format in §3.3.2. I preserved the output blocks and constraints while turning examples into placeholders. RPG_REPOSITORY_PLANNING_GRAPH See also the paper’s section outline for quick cross-reference. section-taxonomy

extracted **localization & editing tool prompts**

## Ready-to-run command templates

### Lifted localization/editing tools \*\*templates\*\* for system

---

#### Copy-pasteable **templates** for the RPG **localization** and **editing** toolsets, standardized with ALL\_CAPS placeholders and strict output formats.preserved usage rules and termination semantics

- drop these straight into your system.

* * *

# **Localization Tool Templates**
===========================

## 1) Inspect interfaces declared in a file

RPG_REPOSITORY_PLANNING_GRAPH

  ```md
    SYSTEM
    You are localizing implementation targets using the RPG graph.

    Tool: view_file_interface_feature_map(file_path)
    Purpose: List all interfaces (functions/classes/methods) in ONE Python file and the feature tags mapped to them.

    INPUT
    file_path: "SRC_RELATIVE_PATH/TO/FILE.py"

    RESPONSE
    Return only a <solution> block with the tool call.

    <solution>
    view_file_interface_feature_map("FILE_PATH")
    </solution>
  ```

## 2) Retrieve code for a specific interface

RPG_REPOSITORY_PLANNING_GRAPH

  ```md
    SYSTEM
    Tool: get_interface_content(target_specs)
    Purpose: Fetch full implementation bodies for specific interfaces by fully-qualified spec.

    INPUT
    target_specs: ["path/to/file.py:QualifiedName", ...]

    RULES
    - Each spec is either function name, class name, or ClassName.method.
    - Return full bodies for all requested items.

    RESPONSE
    <solution>
    get_interface_content(["FILE.py:Symbol", "OTHER.py:ClassName.method"])
    </solution>
  ```

## 3) Expand a feature leaf to its concrete interfaces

RPG_REPOSITORY_PLANNING_GRAPH

```md
    SYSTEM
    Tool: expand_leaf_node_info(feature_path)
    Purpose: From an implemented FEATURE_PATH in the RPG, list all associated repository interfaces.

    INPUT
    feature_path: "ROOT/BRANCH/LEAF"

    RESPONSE
    <solution>
    expand_leaf_node_info("FEATURE_PATH")
    </solution>
  ```

## 4) Fuzzy search by functionality

RPG_REPOSITORY_PLANNING_GRAPH

```md
    SYSTEM
    Tool: search_interface_by_functionality(keywords)
    Purpose: Fuzzy semantic search for interfaces given functional keywords; returns top-5.

    INPUT
    keywords: ["KEYWORD_1", "KEYWORD_2", ...]

    RESPONSE
    <solution>
    search_interface_by_functionality(["KEYWORD_1", "KEYWORD_2"])
    </solution>
```

## 5) Terminate localization with a ranked result set

**RPG_REPOSITORY_PLANNING_GRAPH**

```md
    SYSTEM
    Tool: Terminate(result)
    Purpose: End localization and return a FINAL ranked list of candidates.

    REQUIRED RESULT SCHEMA
    [
      {"file_path":"PATH/TO/TOP1.py","interface":"method: ClassName.func"},
      {"file_path":"PATH/TO/TOP2.py","interface":"function: func_name"},
      {"file_path":"PATH/TO/TOP3.py","interface":"class: ClassName"}
    ]

    RESPONSE
    <solution>
    Terminate(result=[
      {"file_path":"TOP1_FILE.py","interface":"method: Class1.method1"},
      {"file_path":"TOP2_FILE.py","interface":"function: function2"},
      {"file_path":"TOP3_FILE.py","interface":"class: Class3"}
    ])
    </solution>
```

### (Optional) Behavioral macro for search flow (CCG pattern)

Use when you want to enforce a structured search progression: **Coarse Search → Content Inspection → Global Graph Exploration → Terminate**. RPG_REPOSITORY_PLANNING_GRAPH

```md
    SYSTEM
    Adopt a staged localization routine:
    1) Coarse search (expand_leaf_node_info or functionality search)
    2) Content inspection (view_file_interface_feature_map, then get_interface_content)
    3) Graph exploration (iterate dependencies/related files per RPG links)
    4) Terminate with ranked candidates

    RESPONSE
    Use only the tool calls in <solution> blocks at each step; finish with Terminate(...).
```

* * *

## Editing (Coding) Tool Templates
===============================

## A) Edit an entire class (all methods + docstring)

**RPG_REPOSITORY_PLANNING_GRAPH**

```md
    SYSTEM
    Tool: edit_whole_class_in_file(file_path, class_name)
    Use when: The whole class definition must be replaced/edited.

    INPUT
    file_path: "SRC_RELATIVE_PATH/TO/FILE.py"
    class_name: "ClassName"

    OUTPUT REQUIREMENTS
    - Return the FULL class definition (signature, ALL methods, docstring).

    RESPONSE
    <solution>
    edit_whole_class_in_file("FILE_PATH", "CLASS_NAME")
    </solution>
```

## B) Edit a single method inside a class

**RPG_REPOSITORY_PLANNING_GRAPH**

```md
    SYSTEM
    Tool: edit_method_of_class_in_file(file_path, class_name, method_name)
    Use when: Only one method in the class needs editing.

    RULES
    - Return the full `class ClassName:` block containing ONLY the target method.
    - Exclude unrelated methods.
    - Do NOT return the method alone.

    INPUT
    file_path: "SRC_RELATIVE_PATH/TO/FILE.py"
    class_name: "ClassName"
    method_name: "method_name"

    RESPONSE
    <solution>
    edit_method_of_class_in_file("FILE_PATH", "CLASS_NAME", "METHOD_NAME")
    </solution>
  ```

## C) Edit a top-level function

**RPG_REPOSITORY_PLANNING_GRAPH**

```md
    SYSTEM
    Tool: edit_function_in_file(file_path, function_name)
    Use when: A standalone module-level function must be edited.

    INPUT
    file_path: "SRC_RELATIVE_PATH/TO/FILE.py"
    function_name: "function_name"

    OUTPUT REQUIREMENTS
    - Full function (signature, logic, docstring).

    RESPONSE
    <solution>
    edit_function_in_file("FILE_PATH", "FUNCTION_NAME")
    </solution>
```

## D) Edit imports and top-level assignments only

**RPG_REPOSITORY_PLANNING_GRAPH**

```md
    SYSTEM
    Tool: edit_imports_and_assignments_in_file(file_path)
    Use when: Imports or module-level assignments need to be added/fixed.

    STRICT RULES
    - Output ONLY imports and top-level assignments (no classes/functions).
    - Import order: (1) standard library, (2) third-party, (3) local modules.
    - Do NOT remove existing imports unless invalid/typo/non-existent.
    - Retain “apparently unused” imports to preserve runtime deps.

    INPUT
    file_path: "SRC_RELATIVE_PATH/TO/FILE.py"

    RESPONSE
    <solution>
    edit_imports_and_assignments_in_file("FILE_PATH")
    </solution>
```

## E) Terminate editing session

**RPG_REPOSITORY_PLANNING_GRAPH**

```md
    SYSTEM
    Tool: Terminate()
    Use when: All required edits are complete. Do not call prematurely.

    RESPONSE
    <solution>
    Terminate()
    </solution>
```

***

## Notes you can embed in your system docs
=======================================

***

- These tools are part of the RPG **localization → editing** workflow: first find the exact interface(s) using the RPG and search tools, then apply the minimal, scoped edit tool, and finally **Terminate** when done. RPG_REPOSITORY_PLANNING_GRAPH

- The localization toolset is intentionally split into **file/interface inspection** and **feature-driven exploration**, and returns a **ranked, standardized** result at termination. RPG_REPOSITORY_PLANNING_GRAPH

- The editing tools enforce **narrow, auditable diffs** at class/method/function granularity and a **safe import policy** for reproducibility. RPG_REPOSITORY_PLANNING_GRAPH

---


--- rpg_tool/rpg-vector-index/tests/test_faiss_index.py ---
# path: tests/test_faiss_index.py
import json
from pathlib import Path

import numpy as np
import faiss


def test_faiss_roundtrip(tmp_path: Path):
    seed = Path(__file__).parents[1] / "feature_tree_seed.json"
    data = json.loads(seed.read_text())
    dim = int(data["retrieval"]["dims"])  # strict
    vecs = data["retrieval"]["vectors"]
    ids = sorted(vecs.keys())
    mat = np.vstack([np.array(vecs[i], dtype="float32") for i in ids])

    faiss.normalize_L2(mat)
    index = faiss.IndexHNSWFlat(dim, 16, faiss.METRIC_INNER_PRODUCT)
    index.hnsw.efSearch = 32
    index.add(mat)

    # nearest neighbor is itself
    D, I = index.search(mat[:1], 1)
    assert I[0][0] == 0

    # save/load
    p = tmp_path / "idx.faiss"
    faiss.write_index(index, str(p))
    idx2 = faiss.read_index(str(p))
    D2, I2 = idx2.search(mat[:1], 1)
    assert I2[0][0] == 0


--- tests/test_cli.py ---


--- tests/test_pipeline.py ---


--- tests/test_rpg_validate.py ---


--- tests/test_tui.py ---

