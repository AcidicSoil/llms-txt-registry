# llms-full (private-aware)
> Built by authenticated GitHub API fetches. Large files may be truncated.

--- tm-overview.md ---
# TaskMaster Overview

Trigger: /tm-overview

Purpose: Summarize the current TaskMaster tasks.json by status, priority, dependency health, and critical path to orient work.

Steps:

1. Locate the active tasks.json at repo root or the path supplied in the user message. Do not modify it.
2. Parse fields: id, title, description, status, priority, dependencies, subtasks.
3. Compute counts per status and a table of top pending items by priority.
4. Detect dependency issues: cycles, missing ids, orphans (no deps and not depended on).
5. Approximate a critical path: longest dependency chain among pending→in_progress tasks.

Output format:

- "# Overview" then a bullets summary.
- "## Totals" as a 4-column table: status | count | percent | notes.
- "## Top Pending" table: id | title | priority | unblockers.
- "## Critical Path" as an ordered list of ids with short titles.
- "## Issues" list for cycles, missing references, duplicates.

Examples:

- Input (Codex TUI): /tm-overview
- Output: tables and lists as specified. Keep to <= 200 lines.

Notes:

- Read-only. Assume statuses: pending | in_progress | blocked | done.
- If tasks.json is missing or invalid, output an "## Errors" section with a concise diagnosis.


--- temp-prompts-refactored/tm-overview.md ---
# TaskMaster Overview

Trigger: $1

Purpose: $2

Steps:
1. Locate the active tasks.json at repo root or the path supplied in the user message. Do not modify it.
2. Parse fields: id, title, description, status, priority, dependencies, subtasks.
3. Compute counts per status and a table of top pending items by priority.
4. Detect dependency issues: cycles, missing ids, orphans (no deps and not depended on).
5. Approximate a critical path: longest dependency chain among pending→in_progress tasks.

Output format: $3

Examples:
- Input: $4
- Output: $5

Notes:
- $6: Read-only. Assume statuses: pending | in_progress | blocked | done.
- If tasks.json is missing or invalid, output an "## Errors" section with a concise diagnosis.

<!-- Placeholder mapping -->
$1 = Task description
$2 = Purpose statement
$3 = Output format specification
$4 = Example input format
$5 = Expected output structure
$6 = Critical path details (optional) -->


--- temp-prompts-organized/prompt-front-matter/_shared__tm__overview.tm.refactor.md ---
# TaskMaster Overview

## Metadata

- **identifier**: tm-overview  
- **category**: summarization  
- **lifecycle_stage**: inspection  
- **dependencies**: tasks.json  
- **provided_artifacts**: overview bullets, totals table, top pending list, critical path list, issues list  
- **summary**: Summarize TaskMaster tasks.json by status, priority, and dependency health to orient work.

## Inputs

- `tasks.json` path (optional; defaults to repo root)

## Canonical taxonomy (exact strings)

- summarization
- analysis
- reporting

### Stage hints (for inference)

- inspection → summarizing state, reading data
- analysis → detecting cycles, computing paths
- reporting → outputting tables and lists

## Algorithm

1. Extract signals from $1  
   * Titles/headings, imperative verbs, intent sentences, explicit tags, and dependency phrasing.

2. Determine the primary identifier  
   * Prefer explicit input; otherwise infer from main action + object.  
   * Normalize (lowercase, kebab-case, length-capped, starts with a letter).  
   * De-duplicate.

3. Determine categories  
   * Prefer explicit input; otherwise infer from verbs/headings vs canonical taxonomy.  
   * Validate, sort deterministically, and de-dupe (≤3).

4. Determine lifecycle/stage (optional)  
   * Prefer explicit input; otherwise map categories via stage hints.  
   * Omit if uncertain.

5. Determine dependencies (optional)  
   * Parse phrases implying order or prerequisites; keep id-shaped items (≤5).  

6. Determine provided artifacts (optional)  
   * Short list (≤3) of unlocked outputs.

7. Compose summary  
   * One sentence (≤120 chars): “Do <verb> <object> to achieve <outcome>.”

8. Produce metadata in the requested format  
   * Default to a human-readable serialization; honor any requested alternative.

9. Reconcile if input already contains metadata  
   * Merge: explicit inputs > existing > inferred.  
   * Validate lists; move unknowns to an extension field if needed.  
   * Remove empty keys.

## Assumptions & Constraints

- Emit exactly one document: metadata, a single blank line, then $1.
- Limit distinct placeholders to ≤ 7.
- Body text is not altered.

## Validation

- Identifier matches a normalized id pattern (e.g., kebab-case).
- Categories non-empty and drawn from canonical taxonomy (≤3).
- Stage, if present, is one of the allowed stages implied by stage hints (inspection, analysis, reporting).
- Dependencies, if present, are id-shaped (≤5).
- Summary ≤120 chars; punctuation coherent.
- Body text $1 is not altered.

## Output format examples

- Input: `/tm-overview`  
  Output:  
    # Overview  
    - Bullet summary of status, priority, dependencies  
    ## Totals  
    | status       | count | percent | notes         |  
    |--------------|-------|---------|---------------|  
    | pending      | 5     | 40%     | high volume   |  
    | in_progress  | 3     | 25%     | active        |  
    | blocked      | 1     | 8%      | dependency    |  
    | done         | 6     | 50%     | completed     |  
    ## Top Pending  
    | id   | title               | priority | unblockers          |  
    |------|---------------------|----------|---------------------|  
    | t-12 | Fix login timeout   | high     | resolve API error   |  
    | t-34 | Deploy frontend     | medium   | wait for backend    |  
    ## Critical Path  
    - t-12 → t-34 → t-56  
    ## Issues  
    - Cycle detected: t-78 → t-90 → t-78  
    - Missing reference: t-11 (no dependencies)  
    - Duplicate entry: t-44 appears twice  

---

# TaskMaster Overview

Trigger: /tm-overview

Purpose: Summarize the current TaskMaster tasks.json by status, priority, dependency health, and critical path to orient work.

Steps:

1. Locate the active tasks.json at repo root or the path supplied in the user message. Do not modify it.
2. Parse fields: id, title, description, status, priority, dependencies, subtasks.
3. Compute counts per status and a table of top pending items by priority.
4. Detect dependency issues: cycles, missing ids, orphans (no deps and not depended on).
5. Approximate a critical path: longest dependency chain among pending→in_progress tasks.

Output format:

- "# Overview" then a bullets summary.
- "## Totals" as a 4-column table: status | count | percent | notes.
- "## Top Pending" table: id | title | priority | unblockers.
- "## Critical Path" as an ordered list of ids with short titles.
- "## Issues" list for cycles, missing references, duplicates.

Examples:

- Input (Codex TUI): /tm-overview
- Output: tables and lists as specified. Keep to <= 200 lines.

Notes:

- Read-only. Assume statuses: pending | in_progress | blocked | done.
- If tasks.json is missing or invalid, output an "## Errors" section with a concise diagnosis.

Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## template_markdown ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.


--- temp-prompts-organized/_shared/tm/overview.tm.md ---
# TaskMaster Overview

Trigger: /tm-overview

Purpose: Summarize the current TaskMaster tasks.json by status, priority, dependency health, and critical path to orient work.

Steps:

1. Locate the active tasks.json at repo root or the path supplied in the user message. Do not modify it.
2. Parse fields: id, title, description, status, priority, dependencies, subtasks.
3. Compute counts per status and a table of top pending items by priority.
4. Detect dependency issues: cycles, missing ids, orphans (no deps and not depended on).
5. Approximate a critical path: longest dependency chain among pending→in_progress tasks.

Output format:

- "# Overview" then a bullets summary.
- "## Totals" as a 4-column table: status | count | percent | notes.
- "## Top Pending" table: id | title | priority | unblockers.
- "## Critical Path" as an ordered list of ids with short titles.
- "## Issues" list for cycles, missing references, duplicates.

Examples:

- Input (Codex TUI): /tm-overview
- Output: tables and lists as specified. Keep to <= 200 lines.

Notes:

- Read-only. Assume statuses: pending | in_progress | blocked | done.
- If tasks.json is missing or invalid, output an "## Errors" section with a concise diagnosis.


--- temp-prompts-organized/prompt-front-matter/50-docs__examples__api-usage.examples.refactor.md ---
# API Usage Analysis

## Metadata

- **identifier**: http-client
- **category**: API Usage Analysis
- **lifecycle_stage**: analysis
- **dependencies**: [rg, grep]
- **provided_artifacts**: 
  - Definition: src/network/httpClient.ts line 42
  - Key usages: services/userService.ts, hooks/useRequest.ts
- **summary**: Do analyze how an internal API is used to achieve clear documentation and visibility into its real-world applications.

## Inputs

- Input symbol: HttpClient
- Tool commands: `rg -n {{args}} . || grep -RIn {{args}} .`

## Canonical taxonomy (exact strings)

- API Usage Analysis
- Code Inspection
- Dependency Mapping
- Documentation Generation

### Stage hints (for inference)

- analysis
- inspection
- gathering
- review
- synthesis

## Algorithm

1. Extract signals from $1  
   * Titles/headings, imperative verbs, intent sentences, explicit tags, and dependency phrasing.

2. Determine the primary identifier  
   * Prefer explicit input; otherwise infer from main action + object.  
   * Normalize (lowercase, kebab-case, length-capped, starts with a letter).  
   * De-duplicate.

3. Determine categories  
   * Prefer explicit input; otherwise infer from verbs/headings vs canonical taxonomy.  
   * Validate, sort deterministically, and de-dupe (≤3).

4. Determine lifecycle/stage (optional)  
   * Prefer explicit input; otherwise map categories via stage hints.  
   * Omit if uncertain.

5. Determine dependencies (optional)  
   * Parse phrases implying order or prerequisites; keep id-shaped items (≤5).

6. Determine provided artifacts (optional)  
   * Short list (≤3) of unlocked outputs.

7. Compose summary  
   * One sentence (≤120 chars): “Do <verb> <object> to achieve <outcome>.”

8. Produce metadata in the requested format  
   * Default to a human-readable serialization; honor any requested alternative.

9. Reconcile if input already contains metadata  
   * Merge: explicit inputs > existing > inferred.  
   * Validate lists; move unknowns to an extension field if needed.  
   * Remove empty keys.

## Assumptions & Constraints

- Emit exactly one document: metadata, a single blank line, then $1.
- Limit distinct placeholders to ≤ 7.
- All fields must be derived from content or logical inference.

## Validation

- Identifier matches a normalized id pattern (kebab-case, lowercase).
- Categories non-empty and drawn from canonical taxonomy (≤3).
- Stage, if present, is one of the allowed stages implied by stage hints.
- Dependencies, if present, are id-shaped (≤5).
- Summary ≤120 chars; punctuation coherent.
- Body text $1 is not altered.

## Output format examples

- identifier: http-client  
- category: API Usage Analysis  
- lifecycle_stage: analysis  
- dependencies: [rg, grep]  
- provided_artifacts: 
  - Definition: src/network/httpClient.ts line 42
  - Key usages: services/userService.ts, hooks/useRequest.ts
- summary: Do analyze how an internal API is used to achieve clear documentation and visibility into its real-world applications.


--- temp-prompts-organized/prompt-front-matter/50-docs__examples__reference-implementation.examples.refactor.md ---
# Reference Implementation

## Metadata

- **Identifier**: reference-implementation
- **Categories**: code-generation, api-mapping, diff-generation
- **Lifecycle Stage**: implementation
- **Dependencies**: target-module-path, example-url
- **Provided Artifacts**: side-by-side API table, patch suggestions
- **Summary**: Do map target module's API to reference to achieve consistent structure and naming.

## Steps

1. Accept a path or URL to an example. Extract its public API and patterns.
2. Map target module’s API to the reference.
3. Generate diffs that adopt the same structure and naming.

## Output format

- Side-by-side API table and patch suggestions.


--- temp-prompts-organized/50-docs/examples/api-usage.examples.md ---
You are a CLI assistant focused on helping contributors with the task: Show how an internal API is used across the codebase.

1. Gather context by running `rg -n {{args}} . || grep -RIn {{args}} .`.
2. Summarize common usage patterns and potential misuses for the symbol.
3. Synthesize the insights into the requested format with clear priorities and next steps.

Output:

- Begin with a concise summary that restates the goal: Show how an internal API is used across the codebase.
- Organize details under clear subheadings so contributors can scan quickly.
- Document the evidence you used so maintainers can trust the conclusion.

Example Input:
HttpClient

Expected Output:

- Definition: src/network/httpClient.ts line 42
- Key usages: services/userService.ts, hooks/useRequest.ts


--- temp-prompts-organized/50-docs/examples/reference-implementation.examples.md ---
---
phase: "P2 App Scaffold & Contracts"
gate: "Test Gate lite"
status: "align new modules with proven patterns before deeper work."
previous:
  - "/scaffold-fullstack"
  - "/api-contract"
next:
  - "/modular-architecture"
  - "/openapi-generate"
---

# Reference Implementation

Trigger: /reference-implementation

Purpose: Mimic the style and API of a known working example.

## Steps

1. Accept a path or URL to an example. Extract its public API and patterns.
2. Map target module’s API to the reference.
3. Generate diffs that adopt the same structure and naming.

## Output format

- Side-by-side API table and patch suggestions.



--- api-docs-local.md ---
---
phase: "P2 App Scaffold & Contracts"
gate: "Test Gate lite"
status: "contracts cached locally for repeatable generation."
previous:
  - "/scaffold-fullstack"
next:
  - "/api-contract"
  - "/openapi-generate"
---

# API Docs Local

Trigger: /api-docs-local

Purpose: Fetch API docs and store locally for offline, deterministic reference.

## Steps

1. Create `docs/apis/` directory.
2. For each provided URL or package, write retrieval commands (curl or `npm view` docs links). Do not fetch automatically without confirmation.
3. Add `DOCS.md` index linking local copies.

## Output format

- Command list and file paths to place docs under `docs/apis/`.



--- coverage-guide.md ---
---
phase: "P5 Quality Gates & Tests"
gate: "Test Gate"
status: "coverage targets and regression guard plan recorded."
previous:
  - "/integration-test"
next:
  - "/regression-guard"
  - "/version-control-guide"
---

# Coverage Guide

Trigger: /coverage-guide

Purpose: Propose high-ROI tests to raise coverage using uncovered areas.

You are a CLI assistant focused on helping contributors with the task: Suggest a plan to raise coverage based on uncovered areas.

1. Gather context by running `find . -name 'coverage*' -type f -maxdepth 3 -print -exec head -n 40 {} \; 2>/dev/null` for the coverage hints; running `git ls-files | sed -n '1,400p'` for the repo map.
2. Using coverage artifacts (if available) and repository map, propose the highest‑ROI tests to add.
3. Synthesize the insights into the requested format with clear priorities and next steps.

Output:

- Begin with a concise summary that restates the goal: Suggest a plan to raise coverage based on uncovered areas.
- Offer prioritized, actionable recommendations with rationale.
- Call out test coverage gaps and validation steps.

Example Input:
(none – command runs without arguments)

Expected Output:

- Focus on src/auth/login.ts — 0% branch coverage; add error path test.



--- examples/agent-demo.md ---
# Agent Demo — Daily Flow with MCP Tools

This example shows how a lightweight agent (or script) can wire `@prompts/tools` and the MCP server/CLI logic into a daily loop.

## Why

- Always know what to do next (dependency-aware).
- Close the loop quickly with status updates.
- Keep dashboards and teammates in sync via shared logic.

## Minimal agent pseudo-code

```ts
import { createPromptsTools, NextTaskInput, SetTaskStatusInput } from '@prompts/tools';
import { TaskService } from '../src/mcp/task-service.ts';

const service = new TaskService({ tasksPath: '.taskmaster/tasks/tasks.json', tag: 'master', writeEnabled: true });
await service.load();

const tools = createPromptsTools({
  service: {
    list: () => service.list(),
    next: () => service.next(),
    graph: () => service.graph(),
    setStatus: (id, status) => service.setStatus(id, status),
  },
});

// 1) Plan next step
const { task } = await tools.nextTask.run(NextTaskInput.parse({}));
if (!task) process.exit(0);

// 2) Execute your change (omitted)

// 3) Record completion
await tools.setTaskStatus.run(SetTaskStatusInput.parse({ id: task.id, status: 'done' }));

// 4) Re-plan (loop)
```

## MCP client tips

- Start server read-only for exploration: `node dist/mcp/server.js --write=false`.
- Switch to `--write=true` when you’re ready to persist `set_task_status` or `workflow/advance_state` changes.
- For executing local scripts safely, prefer `workflow_run_task_action`:
  - Map `{script,args}` via task metadata or `actions.json` keyed by task id.
  - Preview with `dryRun: true`.
  - Enable live runs by launching with `--exec-enabled` (or setting `PROMPTS_EXEC_ALLOW=1`).
- Use `graph_export` and `workflow/export_task_list` to power dashboards or PR annotations.


--- api-contract.md ---
---
phase: "P2 App Scaffold & Contracts"
gate: "Test Gate lite"
status: "contract checked into repo with sample generation running cleanly."
previous:
  - "/scaffold-fullstack"
next:
  - "/openapi-generate"
  - "/modular-architecture"
---

# API Contract

Trigger: /api-contract "<feature or domain>"

Purpose: Author an initial OpenAPI 3.1 or GraphQL SDL contract from requirements.

**Steps:**

1. Parse inputs and existing docs. If REST, prefer OpenAPI 3.1 YAML; if GraphQL, produce SDL.
2. Define resources, operations, request/response schemas, error model, auth, and rate limit headers.
3. Add examples for each endpoint or type. Include pagination and filtering conventions.
4. Save to `apis/<domain>/openapi.yaml` or `apis/<domain>/schema.graphql`.
5. Emit changelog entry `docs/api/CHANGELOG.md` with rationale and breaking-change flags.

**Output format:**

- `Contract Path`, `Design Notes`, and a fenced code block with the spec body.

**Examples:**

- `/api-contract "accounts & auth"` → `apis/auth/openapi.yaml` with OAuth 2.1 flows.

**Notes:**

- Follow JSON:API style for REST unless caller specifies otherwise. Include `429` and `5xx` models.



--- api-usage.md ---
You are a CLI assistant focused on helping contributors with the task: Show how an internal API is used across the codebase.

1. Gather context by running `rg -n {{args}} . || grep -RIn {{args}} .`.
2. Summarize common usage patterns and potential misuses for the symbol.
3. Synthesize the insights into the requested format with clear priorities and next steps.

Output:

- Begin with a concise summary that restates the goal: Show how an internal API is used across the codebase.
- Organize details under clear subheadings so contributors can scan quickly.
- Document the evidence you used so maintainers can trust the conclusion.

Example Input:
HttpClient

Expected Output:

- Definition: src/network/httpClient.ts line 42
- Key usages: services/userService.ts, hooks/useRequest.ts


--- openapi-generate.md ---
---
phase: "P2 App Scaffold & Contracts"
gate: "Test Gate lite"
status: "generated code builds and CI checks cover the new scripts."
previous:
  - "/api-contract"
next:
  - "/modular-architecture"
  - "/db-bootstrap"
---

# OpenAPI Generate

Trigger: /openapi-generate <server|client> <lang> <spec-path>

Purpose: Generate server stubs or typed clients from an OpenAPI spec.

**Steps:**

1. Validate `<spec-path>`; fail with actionable errors.
2. For `server`, generate controllers, routers, validation, and error middleware into `apps/api`.
3. For `client`, generate a typed SDK into `packages/sdk` with fetch wrapper and retry/backoff.
4. Add `make generate-api` or `pnpm sdk:gen` scripts and CI step to verify no drift.
5. Produce a diff summary and TODO list for unimplemented handlers.

**Output format:** summary table of generated paths, scripts to add, and next actions.

**Examples:** `/openapi-generate client ts apis/auth/openapi.yaml`.

**Notes:** Prefer openapi-typescript + zod for TS clients when possible.



--- reference-implementation.md ---
---
phase: "P2 App Scaffold & Contracts"
gate: "Test Gate lite"
status: "align new modules with proven patterns before deeper work."
previous:
  - "/scaffold-fullstack"
  - "/api-contract"
next:
  - "/modular-architecture"
  - "/openapi-generate"
---

# Reference Implementation

Trigger: /reference-implementation

Purpose: Mimic the style and API of a known working example.

## Steps

1. Accept a path or URL to an example. Extract its public API and patterns.
2. Map target module’s API to the reference.
3. Generate diffs that adopt the same structure and naming.

## Output format

- Side-by-side API table and patch suggestions.



--- temp-prompts-refactored/api-contract.md ---
<!-- $1 = trigger phrase (e.g., "/api-contract 'accounts & auth'") -->
<!-- $2 = domain (e.g., "auth") -->
<!-- $3 = contract type (e.g., "OpenAPI 3.1", "GraphQL") -->
<!-- $4 = contract file extension (e.g., ".yaml", ".graphql") -->
<!-- $5 = output file path (e.g., "apis/$2/$4") -->
<!-- $6 = changelog entry path (e.g., "docs/api/CHANGELOG.md") -->
<!-- $7 = style convention (e.g., "JSON:API") -->

# API Contract Generator

Trigger: $1

Purpose: Author an initial $3 contract from requirements.

**Steps:**

1. Parse inputs and existing docs. If REST, prefer $3; if GraphQL, produce $3.
2. Define resources, operations, request/response schemas, error model, auth, and rate limit headers.
3. Add examples for each endpoint or type. Include pagination and filtering conventions.
4. Save to $5.
5. Emit changelog entry $6 with rationale and breaking-change flags.

**Affected files:**
- $5
- $6

**Output format:**
- `Contract Path`: $4
- `Design Notes`: $7
- Fenced code block with spec body

**Examples:**
- `$1` → $5

**Notes:**
- Follow $7 style for REST unless specified.


--- temp-prompts-refactored/api-usage.md ---
```

<!-- Placeholder mapping for api-usage.md:
$1 = Example input symbol (e.g., 'HttpClient')
$2 = API usage pattern (e.g., 'Key usages')
$3 = Evidence type (e.g., 'File paths')
$4 = Definition source (e.g., 'src/network/httpClient.ts')
 -->

**How to show internal API usage**

1. Gather context by running `rg -n $1 . || grep -RIn $1 .`.
2. Summarize common usage patterns and potential misuses for the symbol.
3. Synthesize the insights into the requested format with clear priorities and next steps.

**Output format**

- Begin with a concise summary that restates the goal: Show how an internal API is used across the codebase.
- Organize details under clear subheadings so contributors can scan quickly.
- Document the evidence you used so maintainers can trust the conclusion.

**Example**

- Input: $2
- Expected output: 
  - Definition: $3
  - Key usages: $4

```


--- temp-prompts-refactored/openapi-generate.md ---
# OpenAPI Generate

Trigger: $1

Purpose: $2

**Steps:**

1. $3
2. $4
3. $5
4. $6
5. $7

**Output format:** $8

**Examples:** $9

**Notes:** $10

---

## Affected files

## Root cause

## Proposed fix

## Tests

## Docs gaps

## Open questions


--- temp-prompts-refactored/reference-implementation.md ---
<!-- $1=Template title, $2=Trigger command, $3=Purpose statement, $4=Step 1 description, $5=Step 2 description, $6=Step 3 description, $7=Output format description -->

**How-to**

# $1

Trigger: $2

Purpose: $3

## Steps

1. $4
2. $5
3. $6

## Output format

- $7


--- temp-prompts-organized/prompt-front-matter/00-ideation__design__api-contract.design.refactor.md ---
# API Contract Design

## Inputs
- Feature or domain string (e.g., "accounts & auth")
- Existing documentation and requirements
- Preference for OpenAPI 3.1 or GraphQL SDL

## Canonical taxonomy (exact strings)
- design
- specification
- contract generation

### Stage hints (for inference)
- design → initial creation of a contract from inputs
- specification → detailed schema definition
- implementation → code generation phase

## Algorithm
1. Extract signals from $1  
   * Titles/headings, imperative verbs, intent sentences, explicit tags, and dependency phrasing.

2. Determine the primary identifier  
   * Prefer explicit input; otherwise infer from main action + object.  
   * Normalize (lowercase, kebab-case, length-capped, starts with a letter).  
   * De-duplicate.

3. Determine categories  
   * Prefer explicit input; otherwise infer from verbs/headings vs $5.  
   * Validate, sort deterministically, and de-dupe (≤3).

4. Determine lifecycle/stage (optional)  
   * Prefer explicit input; otherwise map categories via $6.  
   * Omit if uncertain.

5. Determine dependencies (optional)  
   * Parse phrases implying order or prerequisites; keep id-shaped items (≤5).

6. Determine provided artifacts (optional)  
   * Short list (≤3) of unlocked outputs.

7. Compose summary  
   * One sentence (≤120 chars): “Do <verb> <object> to achieve <outcome>.”

8. Produce metadata in the requested format  
   * Default to a human-readable serialization; honor any requested alternative.

9. Reconcile if input already contains metadata  
   * Merge: explicit inputs > existing > inferred.  
   * Validate lists; move unknowns to an extension field if needed.  
   * Remove empty keys.

## Assumptions & Constraints
- Emit exactly one document: metadata, a single blank line, then $1.
- Limit distinct placeholders to ≤ 7.
- All categories must be from the canonical taxonomy.
- Stage mapping is deterministic and context-aware.

## Validation
- Identifier matches a normalized id pattern (e.g., api-contract).
- Categories non-empty and drawn from $5 (≤3).
- Stage, if present, is one of the allowed stages implied by $6.
- Dependencies, if present, are id-shaped (≤5).
- Dependencies must be explicit or inferable from input structure.
- Artifacts list ≤3 items; all valid outputs.
- Summary ≤120 chars; punctuation coherent.
- Body text $1 is not altered.

## Output format examples
- Identifier: `api-contract`  
- Categories: design, specification, contract generation  
- Stage: design  
- Dependencies: feature/domain input, existing documentation  
- Artifacts: openapi.yaml, schema.graphql, changelog entry  
- Summary: "Do generate an API contract from requirements to achieve a standardized specification for endpoints."

---

# API Contract

Trigger: /api-contract "<feature or domain>"

Purpose: Author an initial OpenAPI 3.1 or GraphQL SDL contract from requirements.

**Steps:**

1. Parse inputs and existing docs. If REST, prefer OpenAPI 3.1 YAML; if GraphQL, produce SDL.
2. Define resources, operations, request/response schemas, error model, auth, and rate limit headers.
3. Add examples for each endpoint or type. Include pagination and filtering conventions.
4. Save to `apis/<domain>/openapi.yaml` or `apis/<domain>/schema.graphql`.
5. Emit changelog entry `docs/api/CHANGELOG.md` with rationale and breaking-change flags.

**Output format:**

- `Contract Path`, `Design Notes`, and a fenced code block with the spec body.

**Examples:**

- `/api-contract "accounts & auth"` → `apis/auth/openapi.yaml` with OAuth 2.1 flows.

**Notes:**

- Follow JSON:API style for REST unless caller specifies otherwise. Include `429` and `5xx` models.


--- temp-prompts-organized/00-ideation/design/api-contract.design.md ---
---
phase: "P2 App Scaffold & Contracts"
gate: "Test Gate lite"
status: "contract checked into repo with sample generation running cleanly."
previous:
  - "/scaffold-fullstack"
next:
  - "/openapi-generate"
  - "/modular-architecture"
---

# API Contract

Trigger: /api-contract "<feature or domain>"

Purpose: Author an initial OpenAPI 3.1 or GraphQL SDL contract from requirements.

**Steps:**

1. Parse inputs and existing docs. If REST, prefer OpenAPI 3.1 YAML; if GraphQL, produce SDL.
2. Define resources, operations, request/response schemas, error model, auth, and rate limit headers.
3. Add examples for each endpoint or type. Include pagination and filtering conventions.
4. Save to `apis/<domain>/openapi.yaml` or `apis/<domain>/schema.graphql`.
5. Emit changelog entry `docs/api/CHANGELOG.md` with rationale and breaking-change flags.

**Output format:**

- `Contract Path`, `Design Notes`, and a fenced code block with the spec body.

**Examples:**

- `/api-contract "accounts & auth"` → `apis/auth/openapi.yaml` with OAuth 2.1 flows.

**Notes:**

- Follow JSON:API style for REST unless caller specifies otherwise. Include `429` and `5xx` models.



--- design-assets.md ---
---
phase: "P4 Frontend UX"
gate: "Accessibility checks queued"
status: "ensure assets support design review."
previous:
  - "/modular-architecture"
next:
  - "/ui-screenshots"
  - "/logging-strategy"
---

# Design Assets

Trigger: /design-assets

Purpose: Generate favicons and small design snippets from product brand.

## Steps

1. Extract brand colors and name from README or config.
2. Produce favicon set, social preview, and basic UI tokens.
3. Document asset locations and references.

## Output format

- Asset checklist and generation commands.



--- modular-architecture.md ---
phase: "P2 App Scaffold & Contracts"
gate: "Test Gate lite"
status: "boundaries documented and lint/build scripts still pass; revisit during P4 Frontend UX for UI seams."
previous:

- "/openapi-generate"
next:
- "/db-bootstrap"
- "/ui-screenshots"
- "/design-assets"

---

# Modular Architecture

Trigger: /modular-architecture

Purpose: Enforce modular boundaries and clear external interfaces.

## Steps

1. Identify services/modules and their public contracts.
2. Flag cross-module imports and circular deps.
3. Propose boundaries, facades, and internal folders.
4. Add "contract tests" for public APIs.

## Output format

- Diagram-ready list of modules and edges, plus diffs.



--- temp-prompts-refactored/design-assets.md ---
# Design Assets

Trigger: $1

Purpose: $2

## Steps

1. $3
2. $4
3. $5

## Output format

- $6


--- temp-prompts-refactored/modular-architecture.md ---
# Modular Architecture

Trigger: $1

Purpose: $2

## Steps

$3

## Output format

$4

---

### Affected files

*To be filled*

### Root cause

*To be filled*

### Proposed fix

*To be filled*

### Tests

*To be filled*

### Docs gaps

*To be filled*

### Open questions

*To be filled*


--- temp-prompts-organized/prompt-front-matter/00-ideation__architecture__adr-new.architecture.refactor.md ---
# ADR Drafting Assistant

Task: Given the following prompt, produce a structured **metadata block** and then emit the original body unchanged. The metadata must expose identifiers, categories, optional lifecycle/stage, optional dependencies, optional provided artifacts, and a concise summary. Output = metadata, blank line, then the input text.

## Inputs
- Input prompt: "You are a CLI assistant focused on helping contributors with the task: Draft an Architecture Decision Record with pros/cons."
- Workflow steps: Gather context from `README.md`, draft ADR (Context, Decision, Status, Consequences), synthesize insights.
- Output requirements: Concise summary of goal; workflow triggers/failing jobs/proposed fixes; documented evidence for maintainers' trust.

## Canonical taxonomy (exact strings)
- architecture
- decision-making
- documentation

### Stage hints (for inference)
- ideation → early drafting, context gathering
- planning → structured output design
- implementation → actual code changes
- review → peer feedback or approval

## Algorithm
1. Extract signals from input:
   - Titles/headings, imperative verbs, intent sentences, explicit tags, and dependency phrasing.
2. Determine the primary identifier:
   - Prefer explicit input; otherwise infer from main action + object.
   - Normalize (lowercase, kebab-case, length-capped, starts with a letter).
   - De-duplicate.
3. Determine categories:
   - Prefer explicit input; otherwise infer from verbs/headings vs canonical taxonomy.
   - Validate, sort deterministically, and de-dupe (≤3).
4. Determine lifecycle/stage (optional):
   - Prefer explicit input; otherwise map categories via stage hints.
   - Omit if uncertain.
5. Determine dependencies (optional):
   - Parse phrases implying order or prerequisites; keep id-shaped items (≤5).
6. Determine provided artifacts (optional):
   - Short list (≤3) of unlocked outputs.
7. Compose summary:
   - One sentence (≤120 chars): “Do <verb> <object> to achieve <outcome>.”
8. Produce metadata in the requested format:
   - Default to a human-readable serialization; honor any requested alternative.
9. Reconcile if input already contains metadata:
   - Merge: explicit inputs > existing > inferred.
   - Validate lists; move unknowns to an extension field if needed.
   - Remove empty keys.

## Assumptions & Constraints
- Emit exactly one document: metadata, a single blank line, then the original body.
- Limit distinct placeholders to ≤7.

## Validation
- Identifier matches a normalized id pattern (e.g., kebab-case, lowercase).
- Categories non-empty and drawn from canonical taxonomy (≤3).
- Stage, if present, is one of the allowed stages implied by stage hints.
- Dependencies, if present, are id-shaped (≤5).
- Artifacts are short (≤3) and relevant to output.
- Summary ≤120 chars; punctuation coherent.
- Body text is not altered.

## Output format examples
- Identifier: `adr-draft`
- Categories: architecture, decision-making, documentation
- Lifecycle stage: ideation
- Dependencies: README.md
- Provided artifacts: ADR with pros/cons, evidence summary, workflow insights
- Summary: "Draft an Architecture Decision Record with pros/cons to achieve transparent decision documentation."


--- temp-prompts-organized/prompt-front-matter/00-ideation__architecture__logging-strategy.architecture.refactor.md ---
# Logging Strategy

## Metadata

- identifier: logging-strategy
- categories: [observability, operations, security]
- stage: design
- dependencies: []
- provided_artifacts: ["diff hunks", "short guideline section"]
- summary: Do add or remove diagnostic logs with privacy in mind to achieve structured observability.

## Steps

1. Identify hotspots from recent failures.
2. Insert structured logs with contexts and correlation IDs.
3. Remove noisy or PII-leaking logs.
4. Document log levels and sampling in `OBSERVABILITY.md`.

## Output format

- Diff hunks and a short guideline section.


--- temp-prompts-organized/prompt-front-matter/00-ideation__architecture__modular-architecture.architecture.refactor.md ---
# Modular Architecture

## Metadata

- **identifier**: modular-architecture  
- **categories**: architecture  
- **stage**: design  
- **dependencies**: [module-boundaries-identification]  
- **provided-artifacts**: [module-graph, dependency-diff, contract-test-plan]  
- **summary**: Do modularize services to achieve clear boundaries and testable interfaces.

## Steps

1. Identify services/modules and their public contracts.
2. Flag cross-module imports and circular deps.
3. Propose boundaries, facades, and internal folders.
4. Add "contract tests" for public APIs.

## Output format

- Diagram-ready list of modules and edges, plus diffs.


--- temp-prompts-organized/prompt-front-matter/00-ideation__architecture__stack-evaluation.architecture.refactor.md ---
# Stack Evaluation

## Metadata

- identifier: stack-evaluation
- categories: [evaluation, analysis, recommendation]
- stage: evaluation
- dependencies: []
- provided_artifacts: ["decision memo", "next steps"]
- summary: Evaluate language/framework choices to achieve informed stay-or-switch decisions.

## Steps

1. Detect current stack and conventions.
2. List tradeoffs: maturity, tooling, available examples, hiring, and AI training coverage.
3. Recommend stay-or-switch with migration outline if switching.

## Output format

- Decision memo with pros/cons and next steps.


--- temp-prompts-organized/prompt-front-matter/00-ideation__design__action-diagram.design.refactor.md ---
# Action Diagram Metadata

## Inputs
- Source file path: C:\Users\user\projects\prompts\temp-prompts\00-ideation\design\action-diagram.design.md
- Maximum placeholders allowed: 7

## Canonical taxonomy (exact strings)
- devops
- pipeline
- workflow

### Stage hints (for inference)
- build → development stage
- deploy → production stage
- push → trigger stage

## Algorithm
1. Extract signals from $1  
   * Titles/headings, imperative verbs, intent sentences, explicit tags, and dependency phrasing.

2. Determine the primary identifier  
   * Prefer explicit input; otherwise infer from main action + object.  
   * Normalize (lowercase, kebab-case, length-capped, starts with a letter).  
   * De-duplicate.

3. Determine categories  
   * Prefer explicit input; otherwise infer from verbs/headings vs $5.  
   * Validate, sort deterministically, and de-dupe (≤3).

4. Determine lifecycle/stage (optional)  
   * Prefer explicit input; otherwise map categories via $6.  
   * Omit if uncertain.

5. Determine dependencies (optional)  
   * Parse phrases implying order or prerequisites; keep id-shaped items (≤5).

6. Determine provided artifacts (optional)  
   * Short list (≤3) of unlocked outputs.

7. Compose summary  
   * One sentence (≤120 chars): “Do <verb> <object> to achieve <outcome>.”

8. Produce metadata in the requested format  
   * Default to a human-readable serialization; honor any requested alternative.

9. Reconcile if input already contains metadata  
   * Merge: explicit inputs > existing > inferred.  
   * Validate lists; move unknowns to an extension field if needed.  
   * Remove empty keys.

## Assumptions & Constraints
- Emit exactly one document: metadata, a single blank line, then $1.
- Limit distinct placeholders to ≤ 7.

## Validation
- Identifier matches a normalized id pattern.
- Categories non-empty and drawn from $5 (≤3).
- Stage, if present, is one of the allowed stages implied by $6.
- Dependencies, if present, are id-shaped (≤5).
- Summary ≤120 chars; punctuation coherent.
- Body text $1 is not altered.

## Output format examples
- Identifier: build  
- Categories: devops, pipeline, workflow  
- Lifecycle stage: none  
- Dependencies: push  
- Provided artifacts: deployment artifact  
- Summary: Do build to achieve deployment after push

## Metadata
- identifier: build
- categories: ["devops", "pipeline", "workflow"]
- lifecycle_stage: null
- dependencies: ["push"]
- provided_artifacts: ["deployment artifact"]
- summary: Do build to achieve deployment after push

## Nodes
- build
- deploy

## Edges
- push -> build
- build -> deploy


--- temp-prompts-organized/prompt-front-matter/00-ideation__design__design-assets.design.refactor.md ---
# Design Assets

## Metadata

- **Identifier**: design-assets
- **Categories**: design, brand assets
- **Stage**: generate
- **Dependencies**: brand-colors, brand-name
- **Provided Artifacts**: asset-checklist, generation-commands
- **Summary**: Generate favicons and small design snippets from product brand to achieve consistent visual identity.

## Steps

1. Extract brand colors and name from README or config.
2. Produce favicon set, social preview, and basic UI tokens.
3. Document asset locations and references.

## Output format

- Asset checklist and generation commands.


--- CHANGELOG.md ---
# Changelog

All notable changes to this project will be documented in this file.

The format is based on Keep a Changelog, and this project adheres to Semantic Versioning when applicable.

## [Unreleased]
### Added
- Introduce interactive `prompts scaffold <slug>` command for creating and managing prompt metadata
- Add prompt scaffolding workflow and metadata guardrails
- Add task enrichment pipeline to augment task data during ingestion
- Add local script execution tools (`workflow_run_script`, `workflow_run_task_action`, `workflow_run_tests`, `workflow_run_build`, `workflow_run_lint`; originally published with `workflow/` prefixes)
- Add abstraction layer for LLM providers
- Add `--verbose` and `--unsafe-logs` flags to CLI and MCP server for improved observability
- Add `docs/client_setup.md` and `docs/observability.md`
- Add new `@prompts/tools` package to expose core task management logic
- Implement a stateful workflow engine with a dedicated CLI and MCP server
- Introduce structured research and planning workflow with new prompt tools
- Add a comprehensive CLI and MCP server guide
- Introduce Task-Master CLI and MCP server
- Implement Task-Master ingest adapter with schema validation and status normalization
- Implement Jest testing framework and define a canonical JSON schema for tasks
- Rebaseline project with a new Product Requirements Document (PRDv2) for Task-Master interoperability
- Introduce Task-Master state engine and a suite of `/tm-*` slash commands
- Add `/plan-delta` prompt for mid-project planning changes
- Prepare for initial npm publish by updating `package.json` and adding a release runbook
- Complete and verify the CLI distribution workflow
- Introduce `prompts` CLI for workflow management
- Add `/docfetch-check` prompt to enforce documentation freshness
- Implement a graph-based workflow planner and state management engine
- Dynamically register prompts from YAML as executable MCP tools
- Implement a robust `StateStore` for managing project state
- Implement a dynamic, stateful prompt workflow and automation engine
- Initialize project tasks from a Product Requirements Document (PRD)
- Add initial Product Requirements Document (PRD) for the prompt pack
- Add research report for building a proactive workflow assistant
- Automate README table generation from the prompt catalog
- Add a script to build the prompt catalog
- Add trigger and purpose metadata to prompts
- Add a guide for converting prompt libraries into MCP servers
- Document `actions.json` mapping format for `workflow_run_task_action` (formerly `workflow/run_task_action`) and add sample file under `examples/actions.json`. Clarifies default lookup, `actionsPath` override, and execution gating via `--exec-enabled` + allowlist.
- CI: add pack contents check to ensure `schemas/` and `dist/mcp/server.js` are included in the npm package (GitHub Actions + verify-pack-contains.mjs).
- Add `workflow_run_lint` tool paralleling test/build wrappers (initially published as `workflow/run_lint`); register on server; document in docs/mcp-cli.md; add integration tests for dry-run and exec-gate behaviour.
- Tests: add live execution test for `workflow_run_script` with `PROMPTS_EXEC_ALLOW=1` using allowlisted `noop` script.
- Introduce the MCP workflow assistant concept
- Add lifecycle metadata front matter to prompts and a validator script
- Implement a dynamic router for documentation MCP servers
- Add a future enhancements roadmap
- Add `/prd-generate` prompt to create a PRD from a README
- Introduce a self-contained MCP server for managing and serving prompts
- Add `/pr-desc` prompt for generating pull request descriptions
- Integrate Task Master AI for managing agentic workflows
- Implement a structured instruction and execution framework for the agent
- Add a comprehensive set of prompts for building an application from scratch
- Introduce a comprehensive end-to-end development workflow guide
- Define an end-to-end application development workflow document
- Add full reference implementation documentation for a Prompts MCP server
- Add `GEMINI.md` and `AGENTS.md` for project context

### Changed
- Update `package.json` for public publishing and modernize `tsconfig.json` to use `NodeNext` module resolution
- Rename workflow execution tools to use underscores (e.g. `workflow_run_script`) for compatibility with OpenAI tool name validation.
- `workflow_run_task_action` (formerly `workflow/run_task_action`) now resolves actions from a new `actions.json` file
- Centralize state management into a new `TaskService`
- Implement batched memory updates for the agent, synchronized with Task Master status
- Update README with usage examples and server launch instructions
- Update `AGENTS.md` with Task Master integration guides
- Formalize PRD in Markdown and refactor research log
- Implement a more efficient batched memory update system for the agent
- Improve prompt metadata validation with `zod` and `glob`
- Automate workflow phase synchronization
- Describe catalog maintenance workflow in documentation
- Align `generate` prompt with unit test workflow
- Clarify commit assistant workflow in documentation
- Update README to clarify `/gemini-map` usage
- Update `/audit` prompt for direct slash command usage
- Align prompts with the gated lifecycle workflow
- Refactor agent's instructional context and planning artifacts with a formal PRD
- Overhaul and categorize the prompt catalog in the README

### Deprecated
- Deprecate previous prompt-authoring workstream as part of PRDv2 rebaseline

### Removed
- Revert prompt scaffolding workflow and metadata guardrails
- Remove the automated documentation maintenance flow in favor of a dynamic router
- Remove obsolete workflow and PRD documentation
- Remove duplicate `/pr-desc` prompt
- Delete old `PRD-v2.txt` file

### Fixed
- Correct the Zod schema for the `advance_state` tool's `outputs` field to prevent JSON schema generation errors
- Make schema path resolution more robust
- Consolidate and fix the test suite to run via Jest
- Correct test limit for multi-byte character truncation
- Improve payload capping logic for UTF-8 character boundaries and edge cases
- Correct Mermaid syntax in workflow diagrams

### Security
- Placeholder for upcoming changes.

## [0.1.0] - 2025-09-22
### Added

- MCP/acidic_soil_prompts_integration_rollup_2025_09_21_america_chicago.md
- cross-check.md
- evidence-capture.md
- query-set.md
- research-batch.md
- research-item.md
- roll-up.md

<!--
Maintenance notes:
- When merging changes, add entries under Added/Changed/Deprecated/Removed/Fixed/Security.
- Prefer concise, user-facing descriptions over commit-level details.
- Optionally link PR numbers or commit hashes if helpful.
-->

--- changelog-from-commits.md ---
# Draft CHANGELOG From Commits

Trigger: /changelog-from-commits

Purpose: Produce a first-draft six-section CHANGELOG block from commit messages and PR titles between two refs.

Steps:

1. Inputs: `since=<ref or tag>` optional, `until=<ref>` default HEAD, `include_prs=true|false` default true.
2. Gather data with:
   - `git log --pretty=%H%x09%s%x09%b <since>.. <until>`
   - If available, `gh pr view` for merged PR titles by commit SHA; else rely on merge commit subjects.
3. Heuristics:
   - Map types: `feat|add`→Added, `fix|bug`→Fixed, `perf|refactor|opt`→Changed, `deprecate`→Deprecated, `remove|drop`→Removed, `sec|cve|security`→Security.
   - Shorten to 12–80 chars. Strip scope parentheses.
4. Emit Markdown with only non-empty sections and a short preface noting the range.

Output format:

- Range preface line
- Six-section Markdown block

Examples:
Input → `/changelog-from-commits since=v2.0.0 until=HEAD`
Output →

```
Range: v2.0.0..HEAD

### Added
- Import data from XLSX (#612)

### Fixed
- Correct null check in OAuth callback (#615)
```

Notes:

- This is a draft; run `/update-changelog` to finalize and create links.
- Keep bullets user-facing; avoid internal refactor noise.


--- changelog-verify.md ---
# Verify CHANGELOG Completeness

Trigger: /changelog-verify

Purpose: Check that the latest merge introduced a CHANGELOG entry with the six-section policy and that sections are concise and non-empty where applicable.

Steps:

1. Parse `CHANGELOG.md` and locate `## [Unreleased]` or the latest version heading.
2. Validate presence and order of sections: Added, Changed, Deprecated, Removed, Fixed, Security.
3. Flag anti-patterns: paragraphs longer than 2 lines, trailing periods, internal-only jargon, file paths, or empty sections left in place.
4. Cross-check against commits since last tag to detect missing items.
5. Emit a diagnostic report and a suggested patch to fix ordering and brevity issues.

Output format:

- "Status: PASS|FAIL"
- Table of findings with line numbers and reasons
- Suggested normalized Markdown block
- Unified diff to apply

Examples:
Input → `/changelog-verify`
Output →

```
Status: FAIL
- L42: Section order incorrect (Found Fixed before Removed)
- Missing Security section stub

Suggested block:
### Added
- Bulk upload for SKUs

### Security
- Bump OpenSSL to 3.0.14
```

Notes:

- Static analysis only; no network calls.
- Treat any section with 0 bullets as removable unless policy requires stubs.


--- CONTRIBUTING.md ---
# Contributing

Thanks for helping extend the prompt catalog. Follow these guidelines so metadata stays consistent with the lifecycle workflow.

## Metadata expectations

- Every lifecycle prompt must begin with the YAML front matter documented in [README.md](README.md).
- `phase` values must reference headings from [WORKFLOW.md](WORKFLOW.md). Use an array when the prompt spans multiple phases.
- Keep `previous` and `next` arrays focused on slash commands or named prerequisites.

## Validation

1. Install dev dependencies once: `npm install`.
2. Run the metadata check before adding or editing prompts: `npm run validate:metadata`.
3. Regenerate the catalog artifacts every time you touch a prompt: `npm run build:catalog`.
4. Fix any reported issues before opening a PR.

`npm run build:catalog` rewrites `catalog.json` and the README tables so downstream tooling and the MCP roadmap work on tool exposure and state tracking have accurate metadata. Our pre-commit hook and CI guard run these commands and will fail if generated files are stale, so make sure to include the refreshed artifacts in your commit.

The validator ensures front matter stays in sync with the workflow gates and stops regressions early.


--- license-report.md ---
You are a CLI assistant focused on helping contributors with the task: Summarize third‑party licenses and risk flags.

1. Gather context by running `npx --yes license-checker --summary 2>/dev/null || echo 'license-checker not available'` for the if license tools are present, their outputs; inspecting `package.json` for the if license tools are present, their outputs.
2. Create a license inventory with notices of copyleft/unknown licenses.
3. Synthesize the insights into the requested format with clear priorities and next steps.

Output:

- Begin with a concise summary that restates the goal: Summarize third‑party licenses and risk flags.
- Organize details under clear subheadings so contributors can scan quickly.
- Flag copyleft or unknown licenses and suggest remediation timelines.

Example Input:
(none – command runs without arguments)

Expected Output:

- MIT (12) — low risk
- GPL-3.0 (1) — requires legal review


--- release-notes.md ---
---
phase: "P7 Release & Ops"
gate: "Release Gate"
status: "notes compiled for staging review and production rollout."
previous:
  - "/pr-desc"
next:
  - "/version-proposal"
  - "/monitoring-setup"
---

# Release Notes

Trigger: /release-notes <git-range>

Purpose: Generate human-readable release notes from recent commits.

You are a CLI assistant focused on helping contributors with the task: Generate human‑readable release notes from recent commits.

1. Gather context by running `git log --pretty='* %s (%h) — %an' --no-merges {{args}}` for the commit log (no merges).
2. Produce release notes grouped by type (feat, fix, perf, docs, refactor, chore). Include a Highlights section and a full changelog list.
3. Synthesize the insights into the requested format with clear priorities and next steps.

Output:

- Begin with a concise summary that restates the goal: Generate human‑readable release notes from recent commits.
- Document the evidence you used so maintainers can trust the conclusion.

Example Input:
src/example.ts

Expected Output:
## Features

- Add SSO login flow (PR #42)

## Fixes

- Resolve logout crash (PR #57)



--- release-notes-prepare.md ---
# Prepare Release Notes From CHANGELOG

Trigger: /release-notes-prepare

Purpose: Convert the latest CHANGELOG section into release notes suitable for GitHub Releases with the six-section layout.

Steps:

1. Detect latest version heading and extract its section.
2. Normalize bullets to sentence fragments without trailing periods.
3. Add short highlights at top (3 bullets max) derived from Added/Changed.
4. Emit a "copy-ready" Markdown body.

Output format:

- Title line: `Release X.Y.Z — YYYY-MM-DD`
- Highlights list
- Six sections with bullets

Examples:
Input → `/release-notes-prepare`
Output →

```
Release 1.6.0 — 2025-09-22

**Highlights**
- Custom roles and permissions
- Faster cold starts

### Added
- Role-based access control
```

Notes:

- Strictly derived from `CHANGELOG.md`. Do not invent content.
- If no version is found, fall back to Unreleased with a warning.


--- update-changelog.md ---
# Update CHANGELOG

Trigger: /update-changelog

Purpose: Generate a user-facing CHANGELOG entry for the latest merge range and insert under the correct version or Unreleased with the six standard sections.

Steps:

1. Inspect repo state:
   - Detect current branch and latest tag: `git describe --tags --abbrev=0`.
   - Identify range: `${SINCE:-<latest-tag>}..HEAD`. If a merge commit hash or tag is provided, use that.
2. Collect changes:
   - Prefer Conventional Commits in `git log --pretty=%s %b` over the range.
   - Map commit types to sections: feat→Added, perf/refactor→Changed, deprecate→Deprecated, remove→Removed, fix→Fixed, security→Security.
   - Merge PR titles: `git log --merges --pretty=%s` and include PR numbers.
3. De-dupe and rewrite:
   - Collapse internal-only chatter. Use terse, user-facing wording. No file paths unless end-user relevant.
   - Keep bullets short. One line each. Present tense. No trailing periods.
4. Emit Markdown snippet with the six sections. Omit empty sections.
5. Decide placement:
   - If a release tag was created in this merge, use `## [X.Y.Z] - YYYY-MM-DD`.
   - Else place under `## [Unreleased]`.
6. Provide a unified diff showing insertion into `CHANGELOG.md`. Do not run it; just output the patch.

Output format:

- Heading line with target section (Unreleased or version)
- Six-section block in Markdown with only non-empty sections in order: Added, Changed, Deprecated, Removed, Fixed, Security
- A short "Link references" block suggestion for `[Unreleased]` and new version comparison links
- A unified diff (context 3) for `CHANGELOG.md`

Examples:
Input →

```
/update-changelog since=v1.4.2 notes=include-prs
```

Output →

```
## [Unreleased]
### Added
- Export CSV from reports page (#482)

### Changed
- Speed up dashboard load times on first visit (#479)

### Fixed
- Resolve 500 error when saving profile with empty bio (#481)

[Unreleased]: https://github.com/OWNER/REPO/compare/v1.4.2...HEAD
```

Notes:

- Assumes git repository is available and tags follow SemVer.
- Keep content end-user focused. Avoid internal file names and refactor notes.
- If no Conventional Commits, infer section from message heuristics.
- Do not include secrets or internal ticket links.


--- temp-prompts-refactored/changelog-from-commits.md ---
<!-- $1=command syntax (e.g., "since=v2.0.0 until=HEAD"), $2=commit range (e.g., "v2.0.0..HEAD"), $3=change type category (e.g., "Added"), $4=change description (e.g., "Import data from XLSX"), $5=PR reference (e.g., "#612") -->
**Draft CHANGELOG From Commits**

Purpose: Produce a first-draft six-section CHANGELOG block from commit messages and PR titles between $1.

Steps:

1. Inputs: `since=<ref or tag>` optional, `until=<ref>` default HEAD, `include_prs=true|false` default true.
2. Gather data with:
   - `git log --pretty=%H%x09%s%x09%b <since>.. <until>`
   - If available, `gh pr view` for merged PR titles by commit SHA; else rely on merge commit subjects.
3. Heuristics:
   - Map types: `feat|add`→$3, `fix|bug`→$3, `perf|refactor|opt`→$3, `deprecate`→$3, `remove|drop`→$3, `sec|cve|security`→$3.
   - Shorten to 12–80 chars. Strip scope parentheses.
4. Emit Markdown with only non-empty sections and a short preface noting the range.

Expected output:
- Range preface line: `Range: $2`
- Six-section Markdown block (each section starts with $3, followed by bullet points of $4 and $5)

Notes:
- This is a draft; run `/update-changelog` to finalize and create links.
- Keep bullets user-facing; avoid internal refactor noise.


--- temp-prompts-refactored/changelog-verify.md ---
<!-- $1 = target command (e.g., /changelog-verify) -->
<!-- $2 = validation status (PASS|FAIL) -->
<!-- $3 = line number in CHANGELOG -->
<!-- $4 = reason for failure (e.g., "Section order incorrect") -->
<!-- $5 = suggested normalized Markdown block -->
<!-- $6 = unified diff to apply -->

# Verify CHANGELOG Completeness

Trigger: $1

Purpose: Check that the latest merge introduced a CHANGELOG entry with the six-section policy and that sections are concise and non-empty where applicable.

Steps:

1. Parse $1 and locate `## [Unreleased]` or the latest version heading.
2. Validate presence and order of sections: Added, Changed, Deprecated, Removed, Fixed, Security.
3. Flag anti-patterns: paragraphs longer than 2 lines, trailing periods, internal-only jargon, file paths, or empty sections left in place.
4. Cross-check against commits since last tag to detect missing items.
5. Emit a diagnostic report and a suggested patch to fix ordering and brevity issues.

Output format:

- `$2`
- Table of findings with line numbers ($3) and reasons ($4)
- Suggested normalized Markdown block ($5)
- Unified diff to apply ($6)

Examples:
Input → $1
Output →

```
$2
- $3: $4
- Missing section stub

$5
```

Constraints:
- Static analysis only; no network calls.
- Treat any section with 0 bullets as removable unless policy requires stubs.


--- AGENTS.md ---
## Instruction Loading Model (Extended)

- **Baseline:** This file (AGENTS.md) is the canonical baseline.

- **Two extension types:**

  1. **Behavior extensions**. Plain instruction files.

     - Locations: `instructions/**/*.md`, `instructions/**/*.txt`, `.cursor/rules/**/*.mdc`
     - Ignored: paths starting with `_`, or names containing `.archive.`
  2. **Rule-packs**. Files with YAML front matter:

     ```markdown
     ---
     description: <one-line purpose>
     globs: <glob or comma-list, e.g., **/* or src/**/*.ts,apps/**>
     alwaysApply: true|false
     ---
     <optional body>
     ```

     - Locations: `instructions/**/*.md`, `instructions/**/*.mdc`, `.cursor/rules/**/*.mdc`
     - Detection: file starts with `---` and declares `description`, `globs`, `alwaysApply`

- **Scope semantics:**

  - `globs` limits where the pack claims relevance. If empty or missing, scope is repo-wide.
  - A pack can be listed even if no current files match, but it is marked “out of scope”.

- **Order and precedence:**

  - Default load order: lexicographic by path.
  - User can reorder before apply. Later wins on conflict.
  - If `alwaysApply: true`, default selection is **on** and pinned to the **end** of the order unless the user moves it.

---

## Startup confirmation flow

1. **Discover**

   - Collect behavior extensions and rule-packs from the locations above.
   - Exclude `_` prefixed paths and `.archive.` files.
2. **Summarize**

   - Title = first `# H1` if present, else filename.
   - For rule-packs show `alwaysApply` and `globs`.
3. **Confirm**

   - Options: **Apply all**, **Apply none**, **Select subset and order**.
   - Default = **Apply none** for behavior extensions. **Apply** for rule-packs with `alwaysApply: true`.
4. **Record**

   - Write final order to `DocFetchReport.approved_instructions[]`.
   - Write approved rule-packs to `DocFetchReport.approved_rule_packs[]` with `{path, alwaysApply, globs}`.

---

## Validation

- Behavior extension: must be readable text.
- Rule-pack: must have front matter with `description` (string), `globs` (string), `alwaysApply` (bool).
- Invalid items are skipped and logged in `DocFetchReport.validation_errors[]`.

---

## Execution flow (docs integration)

- **Preflight must list:**

  - `DocFetchReport.context_files[]` from `@{...}`
  - `DocFetchReport.approved_instructions[]`
  - `DocFetchReport.approved_rule_packs[]`
- Compose in this order:

  1. Baseline
  2. Behavior extensions
  3. Rule-packs
- Proceed only when `DocFetchReport.status == "OK"`.

---

## Conflict resolution

- **Specificity rule:** If both target the same scope, the later item in the final order wins.
- **Pack vs. extension:** No special case. Order controls outcome.
- **Out-of-scope packs:** Do not affect behavior. They remain listed as informational.

---

## Tagging Syntax (context only)

**Context tag — `@{file}` (no behavior change)**

- Purpose: include files for retrieval only.
- Syntax: `@{path/to/file.md}`. Globs allowed.
- Report under `DocFetchReport.context_files[]`.

---

## Resolution and loading

1. Baseline = AGENTS.md.
2. Load **approved behavior extensions** in final order.
3. Load **approved rule-packs** in final order. Packs with `alwaysApply: true` default to the end.
4. Apply later-wins on conflicts.
5. Record `DocFetchReport.status`.

---

## Failure handling

- If any approved item cannot be loaded:

  - Do not finalize. Return a “Docs Missing” plan with missing paths and a fix.
- If any context file fails:

  - Continue. Add to `DocFetchReport.gaps.context_missing[]`. Suggest a fix in the plan.

---

## DocFetchReport Addendum

When discovery/confirmation is used, add:

```json
{
  "DocFetchReport": {
    "approved_instructions": [
      {"path": "instructions/prd_generator.md", "loaded": true, "order": 1},
      {"path": "instructions/security/guardrails.md", "loaded": true, "order": 2}
    ],
    "context_files": [
      {"path": "docs/changelog.md", "loaded": true}
    ],
    "memory_ops": [
      {"tool": "memory", "op": "create_entities|create_relations|add_observations|read_graph|search_nodes", "time_utc": "<ISO8601>", "scope": "project:${PROJECT_TAG}"}
    ],
    "proposed_mcp_servers": [
      {"name": "<lib> Docs", "url": "https://gitmcp.io/{OWNER}/{REPO}", "source": "techstack-bootstrap", "status": "proposed"}
    ],
    "owner_repo_resolution": [
      {"library": "<lib>", "candidates": ["owner1/repo1", "owner2/repo2"], "selected": "owner/repo", "confidence": 0.92, "method": "registry|package_index|docs_link|search"}
    ],
    "server_inventory": [
      {"name": "fastapi_docs", "url": "https://gitmcp.io/tiangolo/fastapi", "source": "project-local|user|generated", "writable": true}
    ],
    "server_diff": {
      "missing": ["fastapi_docs", "httpx_docs"],
      "extra": ["legacy_docs_server"]
    },
    "server_actions": [
      {"action": "add", "name": "httpx_docs", "target": "project-local", "accepted": true}
    ]
  }
}
```

> Note: Omit any fields related to generating or writing `config/mcp_servers.generated.toml`. Use a separate instruction file such as `instructions/mcp_servers_generated_concise.md` if present.

Also log memory batching and status coupling events when applicable:

- Each memory flush: append `{tool:"memory", op:"upsert_batch", time_utc, scope, batch_id, count}` to `memory_ops[]`.
- Each Task Master status call: append `{action:"status", name:"task-master", value, status}` to `server_actions[]`.

---

## A) Preflight: Latest Docs Requirement (**MUST**, Blocking)

**Goal:** Ensure the assistant retrieves and considers the *latest relevant docs* before planning, acting, or finalizing.

**Primary/Fallback Order (consolidated):**

1. **contex7-mcp** (primary)
2. **gitmcp** (fallback)

**What to do:**

- For every task that could touch code, configuration, APIs, tooling, or libraries:

  - Call **contex7-mcp** to fetch the latest documentation or guides.
  - If the **primary** call **fails**, retry with **gitmcp**.
- Each successful call **MUST** capture:

  - Tool name, query/topic, retrieval timestamp (UTC), and source refs/URLs (or repo refs/commits).
- Scope:

  - Fetch docs for each **area to be touched** (framework, library, CLI, infra, etc.).
  - Prefer focused topics (e.g., "exception handlers", "lifespan", "retry policy", "schema").

**Failure handling:**

- If **all** providers fail for a required area, **do not finalize**. Return a minimal plan that includes:

  - The attempted providers and errors
  - The specific topics/areas still uncovered
  - A safe, read-only analysis and suggested next checks (or user confirmation).

**Proof-of-Work Artifact (required):**

- Produce and attach a `DocFetchReport` (JSON) with `status`, `tools_called[]`, `sources[]`, `coverage`, `key_guidance[]`, `gaps`, and `informed_changes[]`.

---

## A.1) Tech & Language Identification (Pre-Requirement)

- Before running Preflight (§A), the assistant must determine both:

  1. The **primary language(s)** used in the project (e.g., TypeScript, Python, Go, Rust, Java, Bash).
  2. The **current project’s tech stack** (frameworks, libraries, infra, tools).

- Sources to infer language/stack:

  - Project tags (`${PROJECT_TAG}`), memory checkpoints, prior completion records.
  - Files present in repo (e.g., manifests like `package.json`, `pyproject.toml`, `go.mod`, `Cargo.toml`, `pom.xml`, CI configs).
  - File extensions in repo (`.ts`, `.js`, `.py`, `.go`, `.rs`, `.java`, `.sh`, `.sql`, etc.).
  - User/task context (explicit mentions of frameworks, CLIs, infra).

- **Repo mapping requirement:** Resolve the **canonical GitHub OWNER/REPO** for each detected library/tool whenever feasible.

  - **Resolution order:**

    1. **Registry mapping** (maintained lookup table for common libs).
    2. **Package index metadata** (e.g., npm `repository.url`, PyPI `project_urls` → `Source`/`Homepage`).
    3. **Official docs → GitHub link** discovery.
    4. **Targeted search** (as a last resort) with guardrails below.
  - **Guardrails:** Prefer official orgs; require name similarity and recent activity; avoid forks and mirrors unless explicitly chosen.
  - Record outcomes in `DocFetchReport.owner_repo_resolution[]` with candidates, selected repo, method, and confidence score.

- Doc retrieval (§A) **must cover each identified language and stack element** that will be touched by the task.

- Record both in the `DocFetchReport`:

```json
"tech_stack": ["<stack1>", "<stack2>"],
"languages": ["<lang1>", "<lang2>"]
```

---

## B) Decision Gate: No Finalize Without Proof (**MUST**)

- The assistant **MUST NOT**: finalize, apply diffs, modify files, or deliver a definitive answer **unless** `DocFetchReport.status == "OK"`.
- The planner/executor must verify `ctx.docs_ready == true` (set when at least one successful docs call exists **per required area**).
- If `status != OK` or `ctx.docs_ready != true`:

  - Stop. Return a **Docs Missing** message that lists the exact MCP calls and topics to run.

---

## 0) Debugging (Revised turn handling)

- **Use consolidated docs-first flow** before touching any files or finalizing: try **contex7-mcp** → **gitmcp**. Record results in `DocFetchReport`.
- **Turn (canonical):** exactly one *attempt cycle* consisting of (1) applying a non-empty code/config patch to address the current error class, then (2) running the allowed tests/checks per §8 (Safety Gate). The turn completes when the checks finish with pass/fail. Planning-only or analysis-only replies are **not** turns.
- **Non-turn events (excluded):** safety deferrals (no tests executed), docs-refresh cycles, cache rebuilds, or re-running identical tests without a new patch. These do **not** increment the turn counter.

### 0.0) Error class

- **Definition:** hash the tuple `{top 5 stack frames (symbolized), failing test ids/names, normalized primary error code/message, top 3 file paths}`. A change in this fingerprint defines a **new error class**.

### 0.1) Docs Staleness Re-check Policy (**3-fail refresh gate**)

**Trigger:** If **three consecutive turns** (as defined above) on the **same error class** fail, the **fourth cycle MUST start with a docs context refresh**.

**Action:**

1. Re-run the **Dynamic Docs MCP Router** (§7) to discover and rank all `*-docs-mcp` servers relevant to the error topics (derive from stack traces, failing commands, and test names).
2. Fetch **latest pertinent docs**, then run `contex7-mcp` → `gitmcp`.
3. Rebuild `DocFetchReport` with `refresh_reason: "3_fail_cycles",`since\_turns: 3`, and`since\_time\_utc\` from the last successful run.
4. Set `ctx.docs_ready = false` until the refreshed report returns `status == "OK"`.
5. Compute and record `doc_delta` vs the previous report (changed sources, new guidance, version bumps). Attach to `DocFetchReport.changed_guidance[]`.

**Post-refresh reset:** On a successful docs refresh, set `turns_failed_in_row = 0` and `ctx.docs_ready = true`. Proceed to plan the next patch using the new guidance.

**Notes:**

- Count a **turn** only after a full Patch+Test cycle completes. Do **not** count planning-only messages, safety deferrals, or identical test re-runs without a new patch.
- Reset the failure counter on a green run (all checks pass), on a **new error class** (fingerprint change), or after a successful docs refresh.
- Persist telemetry in `DocFetchReport.turns`:

```json
{
   "turns_total": <int>,
   "turns_failed_in_row": <int>,
   "last_error_class": "<hash>",
   "last_turn": {
      "patch_id": "<hash of diff>",
      "tests_run": ["<id>"],
      "result": "pass|fail|deferred",
      "time_utc": "<ISO8601>"
   }
}
```

---

## 1) Memory: Boot, Plan, Log

### 1.a Bootstrap (idempotent)

- On chat/session start, initialize **memory** (graph namespace for this project) and hydrate prior context.
- **Server alias:** `memory` (e.g., Smithery Memory MCP `@modelcontextprotocol/server-memory`).
- **Required tools:** `create_entities`, `create_relations`, `add_observations`, `delete_entities`, `delete_observations`, `delete_relations`, `read_graph`, `search_nodes`, `open_nodes`.
- **Ops (treat all creates as upserts):**

  - Upsert base entity: `project:${PROJECT_TAG}`.
  - Link existing tasks/files if present.
  - `read_graph` / `search_nodes` to hydrate working context.
- **Concepts:**

  - **Entities** → `{name, entityType, observations[]}` (names unique; observations are atomic facts).
  - **Relations** → directed, **active voice**: `{from, to, relationType}`.
  - **Observations** → strings attached to entities; add/remove independently.
- If memory is unavailable, set `memory_unavailable: true` in the session preamble and proceed read‑only.

### 1.b Subtask plan & finish-in-one-go

- **Before execution**, derive a **subtask plan** with clear **Definition of Done (DoD)** per subtask.
- **Finish-in-one-go policy:** Work the list to completion within the session unless blocked by §A/§B gates or external dependencies. If blocked, record the block and propose an unblock plan. Persist the plan under `task:${task_id}.observations.plan` with timestamps.

### 1.c Logging & batching policy

- Maintain an in-memory buffer of observations and relation updates.
- **Flush triggers (any):**

  1. Subtask boundary (from **§1.b**),
  2. Status transition intent (`in-progress|verify|done|blocked|needs-local-tests`),
  3. 10 buffered observations,
  4. 60s since last flush.
- **Batched upsert** on flush with `{batch_id, dedupe_keys[], context:{task_id, files_touched, guidance_refs}}`.
- **Observation item schema:**
  `{subtask_id, action, ts_utc, files_touched[], summary, details_md, metrics:{time_ms?, tokens?, pass?}, guidance_refs[], dedupe_key}` with `dedupe_key = hash(task_id, subtask_id, action, summary)`.
- **Merge & summarize:** Coalesce multiple items targeting the same entity within a window into a concise summary. Avoid trivial micro-events.
- **Mirror links while working:** `task:${task_id}` —\[touches]→ `file:<path>` as you proceed.

### 1.d Consistency, errors, backoff

- On finalization flush, `open_nodes` to verify write result. If mismatch, retry once with 250–500 ms backoff and record the outcome.
- Network errors: exponential backoff starting at 250 ms, max 3 attempts per flush. On persistent failure, mark `memory_unavailable:true` and proceed read-only.

### 1.e Task Master coupling (post‑save)

- After any successful flush that changes `percent_complete` or carries a `status_intent`, update **Task Master** (CLI/MCP):

  - Start of execution → **`in-progress`**.
  - Internal **verify** phase (see §2.1) → keep Task Master **`in-progress`**; note verification in memory.
  - Outcomes: success → **`done`**; failure/deferral → **`deferred`** with reason (e.g., `blocked`, `needs-local-tests`).
- Log hook result in memory: `{hook:"task-master", result:"ok|error", ts_utc}` and reflect any `percent_complete` changes only at flush time.

---

## 2) On task completion (status → done)

- **Before finalizing:**

  - Force a final buffer flush with `final:true`.
  - Attach a consolidated `completion_summary_md` and `evidence_refs[]` to `project:${PROJECT_TAG}.task:${task_id}`.

- Write a concise completion to memory including:

  - `project`, `task_id`, `title`, `status`, `next step`
  - Files touched
  - Commit/PR link (if applicable)
  - Test results (if applicable)

- **Completion criteria (explicit):**

  - All subtasks from **§1.b** are marked **done** and their DoD satisfied.
  - Required gates passed (§A, §B).
  - Post-completion checks executed or proposed (§2.1).

- **Update the Knowledge Graph (memory)**:

  - Ensure base entity `project:${PROJECT_TAG}` exists.
  - Upsert `task:${task_id}` and any `file:<path>` entities touched.
  - Create/refresh relations:

    - `project:${PROJECT_TAG}` —\[owns]→ `task:${task_id}`
    - `task:${task_id}` —\[touches]→ `file:<path>`
    - `task:${task_id}` —\[status]→ `<status>`
    - Optional: `task:${task_id}` —\[depends\_on]→ `<entity>`
  - Attach `observations` capturing key outcomes (e.g., perf metrics, regressions, decisions).

- Seed/Update the knowledge graph **before** exiting the task so subsequent sessions can leverage it.

- Do **NOT** write to `AGENTS.md` beyond these standing instructions.

### 2.1) Post-completion checks and tests

- **Order of operations:**
  a) Append subtask completion logs to **memory** (**§1.c**).
  b) Set task status to **`verify`** (new intermediate state).
  c) Evaluate the **Safety Gate** per **§8 Environment & Testing Policy**.
  d) If **safe**, run **stateless checks automatically** (see §8 Allowed Automatic Checks).
  e) If **unsafe**, **defer** execution and emit a **Local Test Instructions** block for the user.

- **Recording:** Write outcomes to `project:${PROJECT_TAG}.task:${task_id}.observations.test_results` and also set:

  - `tests_deferred: true|false`
  - `tests_deferred_reason: <string|null>`
  - `test_log_path: artifacts/test/<UTC-timestamp>.log` when any checks are executed.

- **Status transitions:**

  - On all checks passing → set status **`done`**.
  - On failures → set status **`deferred`** and record an unblock plan.
  - On deferral → set status **`todo`** and include the instructions block.

- **Post-save hook:**

  - After any successful memory flush that changes `percent_complete` or `status_intent`, call Task Master MCP to set status:

    - first execution flush → `in-progress`,
    - post-completion pre-checks → `done`,
    - after §2.1 outcomes → `done|deferred|`.
  - Record hook outcome in memory as an observation: `{hook:"task-master", result:"ok|error", ts_utc}`.

- **Local Test Instructions (example, Proposed — not executed):**

```bash
# Proposed Local Test Instructions — copy/paste locally
# Reason: Safety Gate triggered deferral (e.g., venv detected, risk of global mutation, or OS mismatch)

# TypeScript/Node (stateless checks)
npm run typecheck --silent || exit 1
npm run lint --silent || exit 1
npm run -s build --dry-run || exit 1

# Python (use uv; do NOT activate any existing venv)
# Pure unit tests only; no network, no DB, no writes outside ./artifacts
uv run -q python -c "import sys; sys.exit(0)" || exit 1
uv run -q ruff check . || exit 1
uv run -q pyright || exit 1
uv run -q pytest -q tests/unit -k "not integration" || exit 1

# Expected: exit code 0 on success; non-zero indicates failures to review.
```

---

## 3) Status management

- Use Task Master **only** for external status sync with Task Master’s documented vocabulary.

  - **`in-progress`** on start of execution after §A and **§1.b** planning.
  - Keep **`in-progress`** during the internal **verify** phase (§2.1). Do not write `verify` to Task Master.
  - Set **`done`** when all subtasks are done, gates passed, and §2.1 checks succeed.
  - On failures or deferral via the Safety Gate (§2.1), set **`deferred`** and include a reason in the note (e.g., `blocked` or `needs-local-tests`).

- **Transition rules (internal → Task Master mapping):**

  - Internal: `in-progress → verify → done` ⇒ Task Master: `in-progress → done`.
  - Internal: `verify → blocked` or `verify → needs-local-tests` ⇒ Task Master: `deferred` (with reason note).

- **Transition gating:**

  - Emit Task Master status changes only immediately after a successful memory flush.
  - If the Task Master call fails, mark `status_pending` in memory; retry on next flush or at T+2m with capped backoff.

---

## 4) Tagging for retrieval

- Use tags: `${PROJECT_TAG}`, `project:${PROJECT_TAG}`, `memory_checkpoint`, `completion`, `agents`, `routine`, `instructions`, plus task-specific tags.
- For memory entities/relations, mirror tags on observations (e.g., `graph`, `entity:task:${task_id}`, `file:<path>`), to ease cross-referencing with memory.

---

## 5) Handling user requests for code or docs

- When a task or a user requires **code**, **setup/config**, or **library/API documentation**:

  - **MUST** run the **Preflight** (§A) using the consolidated order (**contex7-mcp → gitmcp**).
  - Only proceed to produce diffs or create files after `DocFetchReport.status == "OK"`.

---

## 6) Project tech stack specifics

- Apply §A Preflight for the **current** stack and language(s).
- Prefer official documentation and repositories resolved in §A.1.
- If coverage is weak after **contex7-mcp → gitmcp**, fall back to targeted web search and record gaps.

---

## 6.1) Layered Execution Guides

Each layer defines role, task, context, reasoning, output format, and stop conditions. Use these blocks when planning and reviewing work in that layer.

### Client/UI Layer — Frontend Engineer, UI Engineer, UX Designer

1. **Role**

   - Own user-facing components and flows.
2. **Task**

   - Draft routes and components. Specify Tailwind classes and responsive breakpoints. Define accessibility and keyboard flows. List loading, empty, and error states. Map data needs to hooks. Set performance budgets.
3. **Context**

   - Next.js app router. Tailwind. shadcn/ui allowed. Target Core Web Vitals good. WCAG 2.2 AA.
4. **Reasoning**

   - Prefer server components for data. Client components only for interactivity. Avoid prop drilling. Use context only for shared UI state. Minimize re-renders. Defer non-critical JS.
5. **Output format**

   - Markdown table: Route | Component | Type (server|client) | States | Data source | Notes.
6. **Stop conditions**

   - Any route lacks explicit states. Missing keyboard flow. CLS or LCP budget undefined. Unmapped data dependency.

### Build & Tooling Layer — Frontend Build Engineer, Tooling Engineer, DevOps Engineer

1. **Role**

   - Maintain fast, reproducible builds and CI.
2. **Task**

   - Define bundler settings, lint, format, typecheck, unit test, and build steps. Configure CI matrix and caching. Enforce pre-commit hooks.
3. **Context**

   - Turbopack or Vite. ESLint. Prettier. Vitest. Node LTS.
4. **Reasoning**

   - Fail fast. Cache effectively. Keep steps isolated and deterministic. No network during tests unless mocked.
5. **Output format**

   - Checklist with commands, CI job matrix, cache keys, and expected durations.
6. **Stop conditions**

   - Non-deterministic builds. Unpinned tool versions. CI steps mutate global state. Missing cache strategy.

### Language & Type Layer — Software Engineer, TypeScript/JavaScript Specialist

1. **Role**

   - Enforce language rules and type safety.
2. **Task**

   - Set `tsconfig` targets. Define strict compiler flags. Establish type patterns and utilities. Require JSDoc where types are complex.
3. **Context**

   - TypeScript strict mode. ESM. Node and browser targets as needed.
4. **Reasoning**

   - Prefer explicit types. Use generics judiciously. Model nullability. Narrow unions at boundaries.
5. **Output format**

   - `tsconfig` fragment, lint rules list, and target type coverage goals by package.
6. **Stop conditions**

   - `any` or `unknown` leaks to public APIs. Implicit `any` enabled. Type coverage goals undefined.

### State & Data Layer — Frontend Engineer, State Management Specialist, Full-Stack Engineer

1. **Role**

   - Control state lifecycles and data fetching.
2. **Task**

   - Choose client vs server state. Define stores, selectors, and cache policy. Specify invalidation and suspense boundaries.
3. **Context**

   - React state, context, or a store. Fetch via RSC or client hooks. Cache with SWR or equivalent.
4. **Reasoning**

   - Server fetch by default. Co-locate state with consumers. Keep derived state computed. Normalize entities.
5. **Output format**

   - State inventory table: State | Scope | Source | Lifetime | Invalidation | Consumers.
6. **Stop conditions**

   - Duplicate sources of truth. Unbounded caches. Missing invalidation plan. Server-client mismatch.

### API/Backend Layer — Backend Engineer, API Developer, Full-Stack Engineer

1. **Role**

   - Provide stable contracts for data and actions.
2. **Task**

   - Define endpoints or schema. Specify auth, pagination, filtering, errors, and idempotency. Version contracts.
3. **Context**

   - REST or GraphQL. JSON. OpenAPI or SDL.
4. **Reasoning**

   - Design for evolution. Prefer standard status codes. Use cursor pagination. Return problem details.
5. **Output format**

   - OpenAPI snippet or GraphQL SDL. Error model table and example requests.
6. **Stop conditions**

   - Breaking change without versioning. Inconsistent pagination. Ambiguous error semantics. Missing auth notes.

### Database & Persistence Layer — Database Engineer, Data Engineer, Backend Engineer

1. **Role**

   - Safeguard data correctness and performance.
2. **Task**

   - Model schema. Choose indexes. Define migrations and retention. Set transaction boundaries.
3. **Context**

   - SQL or NoSQL. Managed service or local. Migration tool required.
4. **Reasoning**

   - Normalize until it hurts then denormalize where measured. Prefer declarative migrations. Use constraints.
5. **Output format**

   - Schema diagram or DDL. Migration plan with up and down steps. Index plan with rationale.
6. **Stop conditions**

   - Missing primary keys. Unsafe destructive migration. No rollback path. Unbounded growth.

### Deployment & Hosting Layer — DevOps Engineer, Cloud Engineer, Platform Engineer

1. **Role**

   - Ship safely and reversibly.
2. **Task**

   - Define environments, build artifacts, release gates, and rollout strategy. Set rollback procedures.
3. **Context**

   - IaC preferred. Blue/green or canary. Artifact registry. Secrets manager.
4. **Reasoning**

   - Immutable artifacts. Promote by provenance. Automate checks before traffic shift.
5. **Output format**

   - Release plan: envs, gates, rollout, rollback steps, and metrics to watch.
6. **Stop conditions**

   - Manual pet deployments. No rollback tested. Secrets in env without manager. Drift from IaC.

### Observability & Ops Layer — Site Reliability Engineer, DevOps Engineer, Monitoring Specialist

1. **Role**

   - Ensure reliability and rapid diagnosis.
2. **Task**

   - Define logs, metrics, traces, dashboards, SLOs, and alerts. Set runbooks.
3. **Context**

   - Centralized logging. Metrics store. Tracing. On-call rotation.
4. **Reasoning**

   - Measure user impact first. Alert on symptoms not causes. Keep noise low.
5. **Output format**

   - SLO doc. Alert rules list. Dashboard inventory with owners.
6. **Stop conditions**

   - No SLOs. Alerts without ownership. Missing runbooks. High alert noise.

### Security & Auth Layer — Security Engineer, Identity and Access Management Engineer, DevOps with Security Focus

1. **Role**

   - Protect assets and identities.
2. **Task**

   - Define authN and authZ flows. Manage secrets. Apply threat modeling and controls.
3. **Context**

   - OAuth or OIDC. Role based access. Secret store.
4. **Reasoning**

   - Least privilege. Rotate secrets. Validate all inputs. Log security events.
5. **Output format**

   - Auth flow diagram. Permission matrix. Threat model checklist.
6. **Stop conditions**

   - Hardcoded secrets. Missing MFA for admin. Broad tokens. Unvalidated input at boundaries.

---

## 7) Library docs retrieval (topic-focused)

- Use **contex7-mcp** first to fetch current docs before code changes.
- UI components: call shadcn-ui-mcp-server to retrieve component recipes and scaffolds before writing code; then generate. Log under DocFetchReport.tools\_called\[].
- If **contex7-mcp** fails, use **gitmcp** (repo docs/source) to retrieve equivalents.
- Summarize key guidance inline in `DocFetchReport.key_guidance` and map each planned change to a guidance line.
- Always note in the task preamble that docs were fetched and which topics/IDs were used.

### Dynamic Docs MCP Router

**Discovery**

- Query the MCP client for active servers. Filter names matching `/\-docs\-mcp$/i`.
- Record the set as `DocsServers[]` in `DocFetchReport.server_inventory`.

**Ranking (per question)**

1. Token match: prefer servers whose **display name or label** appears in the question
   (e.g., “pydantic”, “vite”, “mastra”, “devin”, “mintlify”, “e2b”).
2. Source match: call `list_doc_sources` on candidates; boost servers whose URLs’ host/path
   contain query tokens.
3. Specific term: if the question contains “langgraph”, prefer `langgraph-docs-mcp` **if present**.
4. Tie-break: stable alphabetical by server name.

**Primary / Fallback selection**

- Default: primary = top-ranked from `DocsServers`. Fallbacks = remaining in rank order.
- Override via inline tags in the user message or internal planner notes:

  - `[primary:<server-name>]` to force a primary.
  - `[fallback:+<server-name>]` to append extra fallbacks.
  - Multiple `fallback:+...` allowed. Unknown names are ignored.
- Final chain always appends non-docs providers already defined in this file:
  `… → contex7-mcp → gitmcp`.

**Retrieval loop (stop when coverage is sufficient)**
For each server in the chain:

1. `list_doc_sources` → collect available `llms.txt` refs and topic URLs.
2. Reflect on the question and sources; pick relevant URLs.
3. `fetch_docs` on selected URLs.
4. Synthesize guidance; if remaining gaps exist, continue to next server.

**Recording (required)**

- Append to `DocFetchReport.tools_called[]` for every call with:
  `{server, tool, query_or_url, time_utc}`.
- Save chosen `primary`, evaluated `fallbacks[]`, and `coverage` summary.
- If all providers fail, return **Docs Missing** with attempted servers and errors.

**Example prompts**

- “For ANY question about a library/tool, route to the best `*-docs-mcp` dynamically.”
- “If `[primary:vite-docs-mcp]` is present, start with that server.”
- “If `[fallback:+e2b-docs-mcp]` is present, add it before `contex7-mcp`.”

**Safety**

- Do not finalize answers that depend on docs until `DocFetchReport.status == "OK"` (§B).

---

## 8) Environment & Testing Policy

**Safety Gate checklist (used by §2.1 step c):**

1. **Execution scope** is read-only or hermetic.
2. **No environment activation** of existing shells or venvs.
3. **No network** access unless explicitly approved.
4. **No global writes** or package installs; all outputs limited to `./artifacts/`.
5. **Command review** shows only stateless operations.

**Allowed Automatic Checks (when Safety Gate passes):**

- Typecheck, lint, and format-check.
- Build **dry-run** only.
- Docs build if it does not write outside `./artifacts`.
- Unit tests limited to **pure** functions with no I/O, no network, and no writes beyond `./artifacts`.

**Deferral conditions** (any true ⇒ **defer** and emit Local Test Instructions):

- Active Python venv detected (`VIRTUAL_ENV` set) or `.venv/` present in repo.
- Managed envs or shims detected (e.g., `conda`, `poetry env`, Nix shells).
- Commands imply global mutations, installs, migrations, DB writes, or network calls without explicit approval.
- OS or shell mismatch risk in user context.

**Python rule:** If Python commands are needed and considered safe, use **`uv`** exclusively (`uv run`, `uvx`). **Do not** activate existing venvs.

**Pre-run Test Plan (required for any automatic run):**

- Exact commands, working directory, environment variables, and expected exit codes.
- A short **Risk table** explaining why each command is safe under the Safety Gate.

**Execution protocol (automatic checks):**

- Run the approved commands exactly.
- Capture stdout/stderr to `artifacts/test/<UTC-timestamp>.log`.
- Do not modify other files.

**Post-run recording:**

- Add outcomes to the completion note and `DocFetchReport`.
- Update `task:${task_id}.observations.test_results` and `test_log_path`.

**Deferral protocol:**

- Emit a **Local Test Instructions** block (see §2.1 example) with copy-paste commands, rationale, and expected exit codes.
- Record `tests_deferred_reason` and set status to **`needs-local-tests`**.

---

### System-prompt scaffold (enforcement)

```markdown
SYSTEM: You operate under a blocking docs-first policy.
1) Preflight (§A):
   - Call contex7-mcp → gitmcp as needed.
   - Build DocFetchReport (status must be OK).
   - If the **3-turn staleness trigger** (§0.1) fires, force a docs refresh before proceeding.
2) Planning:
   - Map each planned change to key_guidance items in DocFetchReport.
   - Build a subtask plan with DoD (§1.b) and record to memory.
3) Decision Gate (§B):
   - If DocFetchReport.status != OK → STOP and return "Docs Missing" with exact MCP calls.
4) Execution:
   - Proceed only if ctx.docs_ready == true.
   - Log subtask progress to memory (§1.c). Finish-in-one-go unless blocked.
5) Completion:
   - After memory updates, set status to **verify** and evaluate §8 Safety Gate.
   - Run allowed automatic checks or defer with Local Test Instructions (§2.1, §8).
   - Verify all subtasks done, then set status **done** on success; otherwise **blocked** or **needs-local-tests**.
   - Attach DocFetchReport and write completion memory (§2).
```

---

## Prompt→Gate Router

- **Scope Gate:** `/scope-control`, `/plan`, `/planning-process`
- **Test Gate:** `/integration-test`, `/regression-guard`, `/coverage-guide`
- **Review Gate:** `/review`, `/review-branch`, `/pr-desc`, `/owners`
- **Release Gate:** `/release-notes`, `/version-proposal`
- **Reset path:** `/reset-strategy`

---

## Prompt Safety & Proof Hooks

- Prompts cannot bypass Preflight (§A) or Decision Gate (§B).
- If a prompt suggests stateful changes, require Safety Gate review first.
- Before acting on prompt output, confirm `DocFetchReport.status == "OK"`.
- Append invoked prompts to `DocFetchReport.tools_called[]` as `{tool: "prompt", name: "/<cmd>", path: "~/.codex/prompts/<file>.md", time_utc}`.

---

## Workflow Expansion Example

To demonstrate prompts in practice, a full-stack app workflow includes:

1. **Preflight Docs** → ensure latest docs. Block if `status != OK`.
2. **Planning** → `/planning-process`, `/scope-control`, `/stack-evaluation`.
3. **Scaffold** → `/scaffold-fullstack`, `/api-contract`, `/openapi-generate`.
4. **Data/Auth** → `/db-bootstrap`, `/migration-plan`, `/auth-scaffold`.
5. **Frontend** → `/modular-architecture`, `/ui-screenshots`, `/design-assets`.
6. **Testing** → `/e2e-runner-setup`, `/integration-test`, `/coverage-guide`, `/regression-guard`.
7. **CI/CD** → `/version-control-guide`, `/devops-automation`, `/env-setup`, `/secrets-manager-setup`, `/iac-bootstrap`.
8. **Release** → `/owners`, `/review`, `/review-branch`, `/pr-desc`, `/release-notes`, `/version-proposal`.
9. **Ops** → `/monitoring-setup`, `/slo-setup`, `/logging-strategy`, `/audit`.
10. **Post-release** → `/error-analysis`, `/fix`, `/refactor-suggestions`, `/file-modularity`, `/dead-code-scan`, `/cleanup-branches`, `/feature-flags`.
11. **Model tactics** → `/model-strengths`, `/model-evaluation`, `/compare-outputs`, `/switch-model`.

## Terminology Normalization (Memory MCP)

- “append observations” → **batched observations with flush triggers**
- “subtask starts/finishes” → **semantic boundary events**
- “after it saves memories” → **post-save hook to Task Master MCP**
- “more detail” → **rich observation schema**
  Deprecated: per-event immediate writes.

## Retention Map (Memory sections)

- Startup hydration → preserved (now §1.a).
- Subtask planning DoD → preserved (now §1.b).
- Execution logging → enhanced with batching and schema (now §1.c).
- Completion updates → enhanced with read-back verification (§2).
- Status management → preserved and now post-save synchronized (§1.e, §3).

## Validation Checklist (Memory batching)

- Batching fires only on the four triggers.
- Each flush shows `upsert_batch` in `DocFetchReport.memory_ops`.
- No per-event memory calls during a subtask.
- Dedupe removes repeats with identical `dedupe_key`.
- A Task Master status call follows every successful flush that changes progress or status intent.
- Finalization performs read-back verification and writes `completion_summary_md`.

---

## Build Artifacts Policy — `dist/` (Do not edit build outputs)

Short answer: yes—that’s the normal rule.

`dist/` is build output (from tsc, Rollup, Webpack, Vite, etc.). It gets regenerated, so don’t hand-edit files in `dist/`; make changes in `src/` (and config) and rebuild.

A few practical notes:

- Put it in ignores:
  `.gitignore` → `dist/`
  `.eslintignore` / `.prettierignore` / test coverage excludes → `dist/`
- Typical patterns all work; simplest is just `dist/`. (`dist/*` or `dist/**/*` are fine; they just mean “everything under dist recursively.”)
- Exceptions where teams *do* keep built files:

  - Publishing a library to npm: don’t commit `dist/` to Git, but **do** ship compiled files in the npm package. Use `"files"` in `package.json` (e.g., `"files": ["dist"]`) and a build in `prepublishOnly`/`prepare`.
  - Repos that deploy straight from the repo (e.g., GitHub Pages from a `docs`/`dist` branch). In that case the generated folder is committed on purpose.

If none of those special cases apply, ignore `dist/` and never edit it directly.

---

*End of file.*


--- README.md ---
[![Pack contents check](https://github.com/AcidicSoil/prompts/actions/workflows/pack-contents.yml/badge.svg?branch=main)](https://github.com/AcidicSoil/prompts/actions/workflows/pack-contents.yml)


# Codex Prompts — Vibe Coding Additions

This pack extends the default Codex CLI prompts with vibe-coding playbooks inspired by YC cadences. Drop the folder into `~/.codex/prompts` and you get a catalog of opinionated helpers covering planning, scope control, testing, audits, and model orchestration.

## Installation

1. Clone or copy this repository into `~/.codex/prompts`. The CLI hot-reloads changes, but restarting Codex guarantees the new commands are registered.
2. Optionally commit the directory into your dotfiles so the prompts travel with your workstation setup.

## Contributor workflow

Run these commands whenever you add or edit prompts so the generated catalog stays in sync:

1. `npm install` — install the TypeScript tooling used by the validation scripts.
2. `npm run validate:metadata` — confirm every prompt’s front matter matches the lifecycle workflow.
3. `npm run build:catalog` — regenerate `catalog.json` and refresh the README tables.

`npm run build:catalog` must run after each prompt change; it keeps our published metadata accurate for the upcoming [MCP roadmap](#future-enhancements) work on tool exposure and state tracking. The pre-commit hook and CI guard execute these checks and will fail when `catalog.json` or the README tables are stale, so expect local or remote failures if the command is skipped.

### Prompts CLI

Use the bundled CLI to drive Task-Master ingestion and readiness logic without an MCP client:

- `npm run prompts -- ingest [--tasks <path>] [--tag <tag>]` — validate `tasks.json` against the canonical schema and emit the normalized task list with a remap report.

  ```bash
  npm run prompts -- ingest --tasks .taskmaster/tasks/tasks.json --tag master --pretty
  ```

- `npm run prompts -- next` — print the highest-priority ready task alongside the full ready queue.
- `npm run prompts -- advance <id> <status> [--write]` — update a task’s canonical status. Without `--write` the command runs in dry-run mode and leaves the source file untouched.

  ```bash
  npm run prompts -- advance 42 done --write --tasks ./project/tasks.json
  ```

- `npm run prompts -- graph [--format dot|json]` — export the dependency graph as JSON (default) or Graphviz DOT text.
- `npm run prompts -- status` — summarise totals per status, the current `next` pick, and the ready list. Pass `--pretty` to pretty-print JSON.

CLI options:

- `--tasks <path>` defaults to `.taskmaster/tasks/tasks.json` relative to the current working directory.
- `--tag <tag>` selects a named Task Master tag; defaults to `master`.
- `--write` toggles persistence for `advance`. When omitted the command runs in read-only mode and returns the updated payload without saving.
- `--pretty` enables multi-line, indented JSON output.

The CLI shares the same logic as the MCP tools, so any changes remain consistent across all surfaces.

👉 For a full command reference (including options and example output), see [`docs/mcp-cli.md`](docs/mcp-cli.md).

Once the npm package is published you can install the CLI globally with `npm install -g prompts` (or invoke it ad‑hoc via `npx prompts`). Until then, run it locally with the script above after `npm run build`, or execute `node bin/prompts <command>` directly. The shim loads `dist/cli/main.js`, so remember to rebuild after TypeScript edits.

### Server capabilities overview

The MCP server bundled in this repository now ships with the following surfaced features:

- **Prompt resources** – every prompt markdown file is exposed as a capped `file://` MCP resource, enabling Inspector or other clients to browse the catalog without direct filesystem access.
- **Dynamic prompt tools** – `registerPromptTools` converts each prompt definition into a callable MCP tool with Zod‑derived schemas and structured JSON output alongside trimmed previews.
- **Workflow automation** – `workflow/refresh_metadata`, `workflow/export_task_list`, and `workflow/advance_state` provide scripted catalog regeneration, backlog export, and persistent status logging.
- **State persistence** – completions recorded through `advance_state` (or the CLI equivalent) are saved atomically to `.mcp/state.json` via the `StateStore`, ensuring MCP clients and local scripts share the same progress view.
- **Script parity** – the CLI wraps the MCP tools so automation can run from shell environments, CI, or MCP clients with identical behavior.

#### Running the stdio server

1. Build the project (`npm run build`) so the compiled server exists under `dist/mcp/server.js`.
2. Launch the server with stdio transport:

   ```bash
   node dist/mcp/server.js --tasks .taskmaster/tasks/tasks.json --tag master --write=false
   ```

   - `--tasks` and `--tag` mirror the CLI flags and default to `.taskmaster/tasks/tasks.json` and `master` respectively.
   - `--write` defaults to `false`; set to `true` only when you want `set_task_status` calls to persist changes back to `tasks.json`.
3. Register the process with your MCP client (Codex, Gemini, Cursor, etc.) as a Command/stdio server.

Available task tools:

- `next_task` — returns the highest-priority ready task plus the full ready queue.
- `list_tasks` — emits the normalized task list exactly as the ingest adapter provides it.
- `get_task` — fetches a single task (including subtasks) by numeric ID.
- `graph_export` — returns dependency graph nodes suitable for DOT conversion.
- `set_task_status` — updates a task when the server runs with `--write=true`.

Workflow tools (`refresh_metadata`, `export_task_list`, `advance_state`) remain available for prompt maintenance and state capture. Each tool uses the same shared utilities as the CLI, keeping behaviour consistent across surfaces.

👉 Detailed setup instructions and MCP client snippets live in [`docs/mcp-cli.md`](docs/mcp-cli.md).

## CLI Distribution and Usage Lifecycle

The CLI ships alongside the MCP server, so publishing it cleanly keeps both entry points in sync. Use the checklist below whenever you cut a release or help teammates install the tool locally.

### Prepare the package

1. Confirm the `bin` field in `package.json` still maps the public command (`prompts`) to the shim at `bin/prompts`.
2. Ensure the CLI source (`src/cli/main.ts`) retains its shebang (`#!/usr/bin/env node`) before compiling.
3. Keep the `files` array limited to distributable assets (`bin/`, `dist/`, `resources/`, `prompts/`, docs) so source tests stay out of the tarball.
4. Run `npm run validate:metadata` and `npm run build` to refresh generated artifacts.
5. The `prepublishOnly` script now executes `npm run test && npm run build`; leave it intact so `npm publish` fails fast when tests or builds break.

### Local development workflow

1. Compile the CLI: `npm run build` (rerun after any TypeScript change).
2. Exercise the command directly without linking: `node bin/prompts next`.
3. Link globally for shell usage:

   ```bash
   npm link
   prompts next
   ```

4. When finished, remove the symlink: `npm unlink` (in the project) and `npm unlink -g prompts` (globally) to avoid stale binaries.

### Publishing to npm

1. Authenticate (`npm login`) and bump the version: `npm version <major|minor|patch>`.
2. Build and inspect the package before shipping:

   ```bash
   npm run test
   npm run build
   npm_config_cache=$(pwd)/tmp/npm-cache npm pack --dry-run
   ```

   Verify only the expected files appear in the dry-run output.
3. Publish with provenance: `npm publish --provenance --access public` (have OTP ready for 2FA accounts).
4. Post-publish, double-check `npm view prompts version` and skim the npm README for formatting regressions.
5. Tag corrections or rollbacks: adjust dist-tags (`npm dist-tag add prompts@<version> latest`) or, within the 24-hour window, `npm unpublish prompts@<version>` if absolutely necessary.

### End-user installation and maintenance

- Install globally: `npm install -g prompts` (or run once with `npx prompts list`).
- Update to the latest release: `npm update -g prompts`.
- Remove the CLI when no longer needed: `npm uninstall -g prompts`.
- If troubleshooting, check the published README on npm or run `npm view prompts readme` to confirm the expected commands and flags.

Share these steps with downstream users when announcing a new release so they can upgrade confidently.

### Running the MCP server with Inspector

When testing the MCP server via stdio transports, launch the CLI entrypoint so stdout remains reserved for JSON-RPC traffic.

1. Build the server once: `npm run build` (produces `dist/mcp/server.js`).
2. In MCP Inspector set:
   - **Transport Type:** `STDIO`
   - **Command:** `node`
   - **Arguments:** `dist/mcp/server.js`
   - **Working Directory:** project root

All logging is emitted to stderr, so Inspector receives a clean protocol stream without any npm banners.

### Daily MCP User Flow

The MCP server is meant to sit alongside your editor or chat client all day. It exposes the same task logic as the CLI so humans and agents share a single source of truth.

1) Discover next work item (prioritized and dependency-aware)

```bash
node dist/mcp/server.js --tasks .taskmaster/tasks/tasks.json --tag master --write=false
# From your MCP client, call: next_task → { task, ready }
```

2) Do the work, then record completion (safe by default)

- Read-only by default (`--write=false`).
- Enable persistence only when ready:

```bash
node dist/mcp/server.js --tasks .taskmaster/tasks/tasks.json --tag master --write=true
# From MCP: set_task_status { id, status: "done" } → { task, persisted: true }
# Or: workflow/advance_state to log richer completion context
```

3) Execute mapped actions (optional, gated)

- Map a task id to a local script via task `metadata.action` or `actions.json`.
- Preview first:

```bash
# From MCP: workflow_run_task_action { taskId, dryRun: true } → shows `npm run` command
```

- Enable exec when ready (allowlisted scripts only):

```bash
node dist/mcp/server.js --tasks .taskmaster/tasks/tasks.json --tag master --write=true --exec-enabled
# From MCP: workflow_run_task_action { taskId, dryRun: false } → { ok, exitCode, output }
```

4) Re-plan quickly with visibility

- Inspect dependencies and risk:

```bash
# From MCP: graph_export → { nodes }
# From MCP: workflow/export_task_list → curated list for dashboards/agents
```

Why this helps daily:

- Deterministic “what’s next” signal that respects dependencies.
- Fast feedback loop: status updates immediately inform the next suggestion.
- Shared truth for people and tools (CLI, MCP clients, automations) so plans don’t drift.
- Safety-first defaults (read-only), with explicit gates for persistence and execution.

### MCP tools

The server exposes task tools, workflow helpers, and execution tools.

Workflow helpers:

- `refresh_metadata`: runs the same scripts we call manually:

- `npm run validate:metadata`
- `npm run build:catalog` (pass `updateWorkflow: true` to also run with `--update-workflow`)

Invoke this tool from Inspector or any MCP client to regenerate `catalog.json`, README tables, and—optionally—`WORKFLOW.md` in one step.

- `export_task_list`: reads `resources/prompts.meta.yaml` and returns a normalized task list (`[{ id, title, dependsOn, status: 'pending' }]`) to feed external dashboards or automations.
- `advance_state`: writes task/tool completion snapshots (with optional artifacts) to `.mcp/state.json`, providing durable history for MCP clients and the CLI.

Execution tools (gated; see docs/mcp-cli.md for details):

- `workflow_run_task_action`: resolve `{script,args}` from task `metadata.action` or `actions.json` and dispatch through the safe executor.
- `workflow_run_script`: run an allowlisted npm script with optional args; supports `dryRun`; live runs require `--exec-enabled`.
- `workflow_run_tests`: wrapper that calls `test:jest` via `run_script`.
- `workflow_run_build`: wrapper that calls `build` via `run_script`.
- `workflow_run_lint`: wrapper that calls `lint` via `run_script`.

Notes:

- Scripts must be allowlisted in `package.json#mcpAllowScripts`.
- Live execution is disabled by default; enable by launching the server with `--exec-enabled` (or setting `PROMPTS_EXEC_ALLOW=1`).
- Map task ids to actions via task `metadata.action` or `actions.json` (keyed by task id). See docs/mcp-cli.md for schema and examples.

## Using these prompts

- **Direct slash commands**: Invoke the files that declare a `Trigger:` (table below) straight from Codex. Example: `/planning-process Add OAuth login` opens `planning-process.md` and walks through the feature plan template.
- **Gemini mapper prompt**: `/gemini-map` is a single translator prompt (`gemini-map.md`) that converts Gemini CLI TOML commands into Codex prompt files. Use it only when migrating Gemini content; all other prompts run directly with their own slash commands.

## Prompt metadata

Every lifecycle prompt starts with YAML front matter so docs and tooling stay in sync:

```yaml
---
phase: "P5 Quality Gates & Tests"
gate: "Test Gate"
status: "Runner green locally and wired into CI before expanding coverage."
previous:
  - "/auth-scaffold"
  - "/ui-screenshots"
next:
  - "/integration-test"
  - "/coverage-guide"
---
```

- `phase` — primary stage(s) from [WORKFLOW.md](WORKFLOW.md). Use a string for a single phase or a YAML list for cross-phase helpers.
- `gate` — named gate or checkpoint the prompt supports.
- `status` — the success criteria required to pass that gate.
- `previous` — prerequisite prompts or setup tasks.
- `next` — recommended follow-up prompts once the gate clears.

Maintainers and the metadata validator rely on this block to keep the stage catalog coherent.

## Core slash commands

Commands are grouped by development phase. Stage headings link back to
[WORKFLOW.md](WORKFLOW.md) for owners, gates, and evidence expectations.

### [P0 Preflight Docs](WORKFLOW.md#p0-preflight-docs-blocking) — DocFetchReport must be **OK**

| Command | What it does |
| --- | --- |
| /docfetch-check | Enforce the documentation freshness gate before planning work begins. Run this guardrail to pull the latest references, update the DocFetchReport, and block further tasks until the report is OK. |
| /instruction-file | Generate or update `cursor.rules`, `windsurf.rules`, or `claude.md` with project-specific instructions. |

### [P1 Plan & Scope](WORKFLOW.md#p1-plan--scope) — pass the [Scope Gate](WORKFLOW.md#scope-gate)

| Command | What it does |
| --- | --- |
| /planning-process | Draft, refine, and execute a feature plan with strict scope control and progress tracking. |
| /prototype-feature | Spin up a standalone prototype in a clean repo before merging into main. |
| /scope-control | Enforce explicit scope boundaries and maintain "won't do" and "ideas for later" lists. |
| /stack-evaluation | Evaluate language/framework choices relative to AI familiarity and repo goals. |

### [P2 App Scaffold & Contracts](WORKFLOW.md#p2-app-scaffold--contracts) — clear Test Gate lite

| Command | What it does |
| --- | --- |
| /api-contract "<feature or domain>" | Author an initial OpenAPI 3.1 or GraphQL SDL contract from requirements. |
| /api-docs-local | Fetch API docs and store locally for offline, deterministic reference. |
| /openapi-generate <server\|client> <lang> <spec-path> | Generate server stubs or typed clients from an OpenAPI spec. |
| /prototype-feature | Spin up a standalone prototype in a clean repo before merging into main. |
| /reference-implementation | Mimic the style and API of a known working example. |
| /scaffold-fullstack <stack> | Create a minimal, production-ready monorepo template with app, API, tests, CI seeds, and infra stubs. |

### [P3 Data & Auth](WORKFLOW.md#p3-data--auth) — migrations must dry-run cleanly

| Command | What it does |
| --- | --- |
| /auth-scaffold <oauth\|email\|oidc> | Scaffold auth flows, routes, storage, and a basic threat model. |
| /db-bootstrap <postgres\|mysql\|sqlite\|mongodb> | Pick a database, initialize migrations, local compose, and seed scripts. |
| /migration-plan "<change summary>" | Produce safe up/down migration steps with checks and rollback notes. |

### [P4 Frontend UX](WORKFLOW.md#p4-frontend-ux) — queue accessibility checks

| Command | What it does |
| --- | --- |
| /design-assets | Generate favicons and small design snippets from product brand. |
| /ui-screenshots | Analyze screenshots for UI bugs or inspiration and propose actionable UI changes. |

### [P5 Quality Gates & Tests](WORKFLOW.md#p5-quality-gates--tests) — meet the [Test Gate](WORKFLOW.md#test-gate)

| Command | What it does |
| --- | --- |
| /coverage-guide | Propose high-ROI tests to raise coverage using uncovered areas. |
| /e2e-runner-setup <playwright\|cypress> | Configure an end-to-end test runner with fixtures and a data sandbox. |
| /generate <source-file> | Generate unit tests for a given source file. |
| /integration-test | Generate E2E tests that simulate real user flows. |
| /regression-guard | Detect unrelated changes and add tests to prevent regressions. |

### [P6 CI/CD & Env](WORKFLOW.md#p6-cicd--env) — satisfy the [Review Gate](WORKFLOW.md#review-gate)

| Command | What it does |
| --- | --- |
| /devops-automation | Configure servers, DNS, SSL, CI/CD at a pragmatic level. |
| /env-setup | Create .env.example, runtime schema validation, and per-env overrides. |
| /iac-bootstrap <aws\|gcp\|azure\|fly\|render> | Create minimal Infrastructure-as-Code for the chosen platform plus CI hooks. |
| /secrets-manager-setup <provider> | Provision a secrets store and map application variables to it. |
| /version-control-guide | Enforce clean incremental commits and clean-room re-implementation when finalizing. |
| commit | Generate a conventional, review-ready commit message from the currently staged changes. |

### [P7 Release & Ops](WORKFLOW.md#p7-release--ops) — clear the [Release Gate](WORKFLOW.md#release-gate)

| Command | What it does |
| --- | --- |
| /audit | Audit repository hygiene and suggest improvements. |
| /explain-code | Provide line-by-line explanations for a given file or diff. |
| /monitoring-setup | Bootstrap logs, metrics, and traces with dashboards per domain. |
| /owners <path> | Suggest likely owners or reviewers for the specified path. |
| /pr-desc <context> | Draft a PR description from the branch diff. |
| /release-notes <git-range> | Generate human-readable release notes from recent commits. |
| /review <pattern> | Review code matching a pattern and deliver actionable feedback. |
| /review-branch | Provide a high-level review of the current branch versus origin/main. |
| /slo-setup | Define Service Level Objectives, burn alerts, and runbooks. |
| /version-proposal | Propose the next semantic version based on commit history. |

### [P8 Post-release Hardening](WORKFLOW.md#p8-post-release-hardening) — resolve Sev-1 issues

| Command | What it does |
| --- | --- |
| /cleanup-branches | Recommend which local branches are safe to delete and which to keep. |
| /dead-code-scan | Identify likely dead or unused files and exports using static signals. |
| /error-analysis | Analyze error logs and enumerate likely root causes with fixes. |
| /feature-flags <provider> | Integrate a flag provider, wire the SDK, and enforce guardrails. |
| /file-modularity | Enforce smaller files and propose safe splits for giant files. |
| /fix "<bug summary>" | Propose a minimal, correct fix with diff-style patches. |
| /refactor-suggestions | Propose repo-wide refactoring opportunities after tests exist. |

### [P9 Model Tactics](WORKFLOW.md#p9-model-tactics-cross-cutting) — document uplift before switching defaults

| Command | What it does |
| --- | --- |
| /compare-outputs | Run multiple models or tools on the same prompt and summarize best output. |
| /model-evaluation | Try a new model and compare outputs against a baseline. |
| /model-strengths | Choose model per task type. |
| /switch-model | Decide when to try a different AI backend and how to compare. |

### [Reset Playbook](WORKFLOW.md#reset-playbook) and other cross-cutting helpers

| Command | Stage tie-in | What it does |
| --- | --- | --- |
| /content-generation | 11) Evidence Log | Draft docs, blog posts, or marketing copy aligned with the codebase. |
| /reset-strategy | Reset Playbook | Decide when to hard reset and start clean to avoid layered bad diffs. |
| /voice-input | Support | Support interaction from voice capture and convert to structured prompts. |

## Reference assets

- `workflow.mmd` — Mermaid source for the end-to-end workflow shown below.
- `codefetch/codebase.md` — Quick peek of local config snippets used by the prompts (e.g., markdownlint defaults).

## Example flow

1. `/planning-process Add OAuth login` to align on goals, risks, and validation.
2. Implement the scoped tasks, checking `/scope-control` to document non-goals and later ideas.
3. `/integration-test` to add coverage for the new flow, then `/regression-guard` to verify no unrelated files drifted.
4. `/version-control-guide` to clean the final diff, followed by `/pr-desc` or `/release-notes` to communicate the change.

## Mermaid flowchart

```mermaid
flowchart TD
  subgraph P0["P0 Preflight Docs"]
    preflight["Preflight Docs (§A) AGENTS"]
  end

  subgraph P1["P1 Plan & Scope"]
    plan[/planning-process/]
    scope[/scope-control/]
    stack[/stack-evaluation/]
  end

  subgraph P2["P2 App Scaffold & Contracts"]
    scaffold[/scaffold-fullstack/]
    api_contract[/api-contract/]
    openapi[/openapi-generate/]
    modular[/modular-architecture/]
  end

  subgraph P3["P3 Data & Auth"]
    db[/db-bootstrap/]
    migrate[/migration-plan/]
    auth[/auth-scaffold/]
  end

  subgraph P4["P4 Frontend UX"]
    assets[/design-assets/]
    screenshots[/ui-screenshots/]
  end

  subgraph P5["P5 Quality Gates & Tests"]
    e2e[/e2e-runner-setup/]
    integration[/integration-test/]
    coverage[/coverage-guide/]
    regression[/regression-guard/]
  end

  subgraph P6["P6 CI/CD & Env"]
    vcs[/version-control-guide/]
    devops[/devops-automation/]
    env[/env-setup/]
    secrets[/secrets-manager-setup/]
    iac[/iac-bootstrap/]
  end

  subgraph P7["P7 Release & Ops"]
    owners[/owners/]
    review[/review/]
    review_branch[/review-branch/]
    pr_desc[/pr-desc/]
    release_notes[/release-notes/]
    version[/version-proposal/]
    monitoring[/monitoring-setup/]
    slo[/slo-setup/]
    logging[/logging-strategy/]
  end

  subgraph Deploy["Deployment Flow"]
    deploy_staging[Deploy Staging]
    canary[Canary + Health]
    deploy_prod[Deploy Prod]
    rollback[Rollback]
  end

  subgraph P8["P8 Post-release Hardening"]
    error[/error-analysis/]
    fix[/fix/]
    refactor[/refactor-suggestions/]
    modularity[/file-modularity/]
    deadcode[/dead-code-scan/]
    cleanup[/cleanup-branches/]
    flags[/feature-flags/]
  end

  subgraph P9["P9 Model Tactics"]
    strengths[/model-strengths/]
    evaluation[/model-evaluation/]
    compare[/compare-outputs/]
    switch[/switch-model/]
  end

  scope_gate{Scope Gate}
  test_gate_lite{Test Gate lite}
  ux_gate{Accessibility checks queued}
  test_gate{Test Gate}
  review_gate{Review Gate}
  release_gate{Release Gate}
  hardening_gate{Sev-1 resolved}

  preflight --> plan
  plan --> scope --> stack --> scope_gate
  scope_gate --> scaffold
  scaffold --> api_contract --> openapi --> modular --> test_gate_lite
  test_gate_lite --> db
  db --> migrate --> auth --> assets --> screenshots --> ux_gate
  ux_gate --> e2e --> integration --> coverage --> regression --> test_gate
  test_gate --> vcs --> devops --> env --> secrets --> iac --> review_gate
  review_gate --> owners --> review --> review_branch --> pr_desc --> release_notes --> version --> release_gate
  release_gate --> deploy_staging --> canary --> deploy_prod
  canary --> rollback
  deploy_prod --> monitoring --> slo --> logging --> hardening_gate
  deploy_prod --> error
  error --> fix --> refactor --> modularity --> deadcode --> cleanup --> flags --> hardening_gate
  deploy_prod --> flags
  deploy_prod --> strengths
  strengths --> evaluation --> compare --> switch
  flags --> strengths
```

## Future enhancements

The current release already exposes the prompt catalog as MCP resources/tools, bundles workflow automation, and delivers a publishable CLI. Next milestones focus on rounding out automation and platform ergonomics:

- **Task Master parity** — Point `workflow/export_task_list` at `.taskmaster/tasks/tasks.json` so MCP clients mirror the Task Master backlog (Task 20).
- **Rate limiting utilities** — Ship the token-bucket helper (Task 9) for future outbound integrations.
- **Expanded lifecycle prompts** — Author remaining planning/scaffolding/testing/release prompts (Tasks 13–17) and validate them end-to-end (Task 18).
- **MCP notifications & sync** — Emit DocFetch and completion events, and optionally push Task Master status updates through MCP once bidirectional APIs are available.
- **CLI niceties** — After the npm package is live, add command completions, structured logging flags, and richer `export` formats to support external automations.


--- WORKFLOW.md ---
# WORKFLOW\.md

## 1) Goal

Ship a production-grade full-stack web app from zero to deployed with audit trails, tests, and rollback. Use Codex slash commands as the execution surface.&#x20;

## 2) Scope

### In

Greenfield repo. Web UI, API, DB, auth, CI/CD, observability, security baseline, docs, release. Full run uses prompts end-to-end.&#x20;

### Out / Won’t do

Vendor lock-in choices, bespoke infra, ML features, mobile clients, data science. No auto-running prompts; all manual per AGENTS baseline.&#x20;

### Ideas for later

Multi-region, blue/green, SSO, feature flags, load testing, A/B infra.

## 3) Roles & Owners

Planner, Full-stack dev, API dev, Frontend dev, QA, DevOps, Security, Docs. One person may hold multiple roles. Owners per phase below.

## 4) Milestones

M1 Plan approved.
M2 Scaffold + CI green.
M3 E2E happy path green.
M4 Staging deployed.
M5 Production release with rollback tested.

## 5) Phases

<!-- BEGIN GENERATED PHASES -->
### P0 Preflight Docs (Blocking)

- **Purpose**: Enforce docs-first policy and record DocFetchReport.&#x20;
- **Inputs**: Empty repo, tool access.
- **Steps**: Run Preflight Latest Docs. Record approved instructions and packs. Stop if status≠OK.&#x20;
- **Gate Criteria**: DocFetchReport.status==OK.
- **Outputs**: DocFetchReport JSON.
- **Risks**: Missing docs.
- **Owners**: Planner.

<!-- commands:start -->
- _No catalog commands mapped to this phase._
<!-- commands:end -->

### P1 Plan & Scope

- **Purpose**: Lock scope and acceptance.
- **Steps**: `/planning-process "<app one-line>"` → draft plan. `/scope-control` → In/Out, Won’t do. `/stack-evaluation` → pick stack.&#x20;
- **Gate**: Scope Gate passed.
- **Outputs**: PLAN.md, scope table.
- **Owners**: Planner.

<!-- commands:start -->
- `/planning-process` — Draft, refine, and execute a feature plan with strict scope control and progress tracking.
- `/prototype-feature` — Spin up a standalone prototype in a clean repo before merging into main.
- `/scope-control` — Enforce explicit scope boundaries and maintain "won't do" and "ideas for later" lists.
- `/stack-evaluation` — Evaluate language/framework choices relative to AI familiarity and repo goals.
<!-- commands:end -->

### P2 App Scaffold & Contracts

- **Purpose**: Create minimal working app.
- **Steps**:

  - `/scaffold-fullstack <stack>` → create repo, packages, app, api, infra stubs. **(new)**
  - `/api-contract` or `/openapi-generate` → draft API spec. **(new)**
  - `/modular-architecture` → boundaries. `/reference-implementation` if copying style.&#x20;
- **Gate**: Test Gate lite = build runs, lint clean.
- **Outputs**: repo tree, OpenAPI/SDL.
- **Owners**: Full-stack dev.

<!-- commands:start -->
- `/api-contract "<feature or domain>"` — Author an initial OpenAPI 3.1 or GraphQL SDL contract from requirements.
- `/api-docs-local` — Fetch API docs and store locally for offline, deterministic reference.
- `/openapi-generate <server|client> <lang> <spec-path>` — Generate server stubs or typed clients from an OpenAPI spec.
- `/prototype-feature` — Spin up a standalone prototype in a clean repo before merging into main.
- `/reference-implementation` — Mimic the style and API of a known working example.
- `/scaffold-fullstack <stack>` — Create a minimal, production-ready monorepo template with app, API, tests, CI seeds, and infra stubs.
<!-- commands:end -->

### P3 Data & Auth

- **Purpose**: Persistence and identity.
- **Steps**: `/db-bootstrap <db>` → schema, migrations, seeds. **(new)**
  `/auth-scaffold <oauth|email>` → flows + threat model. **(new)**
  `/migration-plan` → up/down scripts. **(new)**
- **Gate**: Migration dry-run ok. Threat checklist done.
- **Outputs**: migrations, seed script, auth routes.
- **Owners**: API dev, Security.

<!-- commands:start -->
- `/auth-scaffold <oauth|email|oidc>` — Scaffold auth flows, routes, storage, and a basic threat model.
- `/db-bootstrap <postgres|mysql|sqlite|mongodb>` — Pick a database, initialize migrations, local compose, and seed scripts.
- `/migration-plan "<change summary>"` — Produce safe up/down migration steps with checks and rollback notes.
<!-- commands:end -->

### P4 Frontend UX

- **Purpose**: Routes and components.
- **Steps**: `/modular-architecture` (UI), `/ui-screenshots` for reviews, `/design-assets` for favicon/brand, `/logging-strategy` client events.&#x20;
- **Gate**: Accessibility checks queued.
- **Outputs**: Screens, states, assets.
- **Owners**: Frontend.

<!-- commands:start -->
- `/design-assets` — Generate favicons and small design snippets from product brand.
- `/ui-screenshots` — Analyze screenshots for UI bugs or inspiration and propose actionable UI changes.
<!-- commands:end -->

### P5 Quality Gates & Tests

- **Purpose**: E2E-first coverage.
- **Steps**: `/e2e-runner-setup <playwright|cypress>` **(new)** → runner + fixtures.
  `/integration-test` → happy path E2E. `/coverage-guide` → target areas. `/regression-guard` → unrelated drift.&#x20;
- **Gate**: Test Gate = E2E happy path green.
- **Outputs**: E2E suite, coverage plan.
- **Owners**: QA.

<!-- commands:start -->
- `/coverage-guide` — Propose high-ROI tests to raise coverage using uncovered areas.
- `/e2e-runner-setup <playwright|cypress>` — Configure an end-to-end test runner with fixtures and a data sandbox.
- `/generate <source-file>` — Generate unit tests for a given source file.
- `/integration-test` — Generate E2E tests that simulate real user flows.
- `/regression-guard` — Detect unrelated changes and add tests to prevent regressions.
<!-- commands:end -->

### P6 CI/CD & Env

- **Purpose**: Reproducible pipeline and environments.
- **Steps**: `/version-control-guide` → commit rules. `/devops-automation` → CI, DNS, SSL, deploy. `/env-setup` + `/secrets-manager-setup` **(new)**. `/iac-bootstrap` **(new)**.&#x20;
- **Gate**: Review Gate = CI green, approvals, no unrelated churn.
- **Outputs**: CI config, IaC, secret store wiring.
- **Owners**: DevOps.

<!-- commands:start -->
- `/devops-automation` — Configure servers, DNS, SSL, CI/CD at a pragmatic level.
- `/env-setup` — Create .env.example, runtime schema validation, and per-env overrides.
- `/iac-bootstrap <aws|gcp|azure|fly|render>` — Create minimal Infrastructure-as-Code for the chosen platform plus CI hooks.
- `/secrets-manager-setup <provider>` — Provision a secrets store and map application variables to it.
- `/version-control-guide` — Enforce clean incremental commits and clean-room re-implementation when finalizing.
- `commit` — Generate a conventional, review-ready commit message from the currently staged changes.
<!-- commands:end -->

### P7 Release & Ops

- **Purpose**: Ship safely.
- **Steps**: `/pr-desc`, `/owners`, `/review`, `/review-branch`, `/release-notes`, `/version-proposal`. `/monitoring-setup` + `/slo-setup` **(new)**. `/logging-strategy` server. `/audit` security/hygiene.&#x20;
- **Gate**: Release Gate = canary ok, rollback tested.
- **Outputs**: Release notes, dashboards, runbooks.
- **Owners**: Dev, DevOps, SRE.

<!-- commands:start -->
- `/audit` — Audit repository hygiene and suggest improvements.
- `/explain-code` — Provide line-by-line explanations for a given file or diff.
- `/monitoring-setup` — Bootstrap logs, metrics, and traces with dashboards per domain.
- `/owners <path>` — Suggest likely owners or reviewers for the specified path.
- `/pr-desc <context>` — Draft a PR description from the branch diff.
- `/release-notes <git-range>` — Generate human-readable release notes from recent commits.
- `/review <pattern>` — Review code matching a pattern and deliver actionable feedback.
- `/review-branch` — Provide a high-level review of the current branch versus origin/main.
- `/slo-setup` — Define Service Level Objectives, burn alerts, and runbooks.
- `/version-proposal` — Propose the next semantic version based on commit history.
<!-- commands:end -->

### P8 Post-release Hardening

- **Purpose**: Stability and cleanup.
- **Steps**: `/error-analysis`, `/fix`, `/refactor-suggestions`, `/file-modularity`, `/dead-code-scan`, `/cleanup-branches`. `/feature-flags` **(new)**.&#x20;
- **Gate**: All Sev-1 fixed.
- **Outputs**: Clean diff, flags in place.
- **Owners**: Dev.

<!-- commands:start -->
- `/cleanup-branches` — Recommend which local branches are safe to delete and which to keep.
- `/dead-code-scan` — Identify likely dead or unused files and exports using static signals.
- `/error-analysis` — Analyze error logs and enumerate likely root causes with fixes.
- `/feature-flags <provider>` — Integrate a flag provider, wire the SDK, and enforce guardrails.
- `/file-modularity` — Enforce smaller files and propose safe splits for giant files.
- `/fix "<bug summary>"` — Propose a minimal, correct fix with diff-style patches.
- `/refactor-suggestions` — Propose repo-wide refactoring opportunities after tests exist.
<!-- commands:end -->

### P9 Model Tactics (cross-cutting)

- **Purpose**: Optimize prompting/model choice.
- **Steps**: `/model-strengths`, `/model-evaluation`, `/compare-outputs`, `/switch-model`.&#x20;
- **Gate**: Model delta improves QoS.
- **Owners**: Planner.

<!-- commands:start -->
- _No catalog commands mapped to this phase._
<!-- commands:end -->

### 11) Evidence Log

- **Purpose**: _Document the goal for 11) Evidence Log._
- **Steps**: _Outline the prompts and activities involved._
- **Gate Criteria**: _Capture the exit checks before advancing._
- **Outputs**: _List the deliverables for this phase._
- **Owners**: _Assign accountable roles._

<!-- commands:start -->
- `/content-generation` — Draft docs, blog posts, or marketing copy aligned with the codebase.
<!-- commands:end -->

### P0 Preflight Docs

- **Purpose**: _Document the goal for P0 Preflight Docs._
- **Steps**: _Outline the prompts and activities involved._
- **Gate Criteria**: _Capture the exit checks before advancing._
- **Outputs**: _List the deliverables for this phase._
- **Owners**: _Assign accountable roles._

<!-- commands:start -->
- `/instruction-file` — Generate or update `cursor.rules`, `windsurf.rules`, or `claude.md` with project-specific instructions.
<!-- commands:end -->

### P9 Model Tactics

- **Purpose**: _Document the goal for P9 Model Tactics._
- **Steps**: _Outline the prompts and activities involved._
- **Gate Criteria**: _Capture the exit checks before advancing._
- **Outputs**: _List the deliverables for this phase._
- **Owners**: _Assign accountable roles._

<!-- commands:start -->
- `/compare-outputs` — Run multiple models or tools on the same prompt and summarize best output.
- `/model-evaluation` — Try a new model and compare outputs against a baseline.
- `/model-strengths` — Choose model per task type.
- `/switch-model` — Decide when to try a different AI backend and how to compare.
<!-- commands:end -->

### Reset Playbook

- **Purpose**: _Document the goal for Reset Playbook._
- **Steps**: _Outline the prompts and activities involved._
- **Gate Criteria**: _Capture the exit checks before advancing._
- **Outputs**: _List the deliverables for this phase._
- **Owners**: _Assign accountable roles._

<!-- commands:start -->
- `/reset-strategy` — Decide when to hard reset and start clean to avoid layered bad diffs.
<!-- commands:end -->

### Support

- **Purpose**: _Document the goal for Support._
- **Steps**: _Outline the prompts and activities involved._
- **Gate Criteria**: _Capture the exit checks before advancing._
- **Outputs**: _List the deliverables for this phase._
- **Owners**: _Assign accountable roles._

<!-- commands:start -->
- `/voice-input` — Support interaction from voice capture and convert to structured prompts.
<!-- commands:end -->
<!-- END GENERATED PHASES -->
## 6) Dev Loop Rules

Commit small. One concern per PR. Use clean-room finalize if diff grows. Reset when E2E red for >60m or design drift detected. Enforce branch policy via `/version-control-guide`.&#x20;

## 7) Test Strategy

E2E first. Happy path before edge cases. Regression guards on changed areas and critical paths. Coverage targets: lines 80%, branches 60%, critical modules 90%. Use `/integration-test`, `/coverage-guide`, `/regression-guard`.&#x20;

## 8) CI/CD Plan

Jobs: lint, typecheck, unit, build, e2e, package, deploy. Artifacts: build outputs, test logs, coverage, SBOM. Envs: preview, staging, prod. Rollback: pinned version + IaC plan. Use `/devops-automation` and `/iac-bootstrap`.&#x20;

## 9) Observability & Logging Plan

Structured logs, metrics, traces. Dashboards by domain. Alerts on SLO burn. Client and server logging strategies via `/logging-strategy`.&#x20;

## 10) Risk Register & Mitigations

Scope creep → Scope Gate. Flaky E2E → isolate and retry matrix. Secrets leakage → secrets manager, scans. Infra drift → IaC. Auth gaps → threat model.&#x20;

## 11) Evidence Log

- Command catalog and flows: README table and Mermaid.&#x20;
- Baseline precedence, Preflight, DocFetchReport, gates: AGENTS baseline.&#x20;

## 12) Release Notes Checklist

Scope summary, changes by area, migration steps, breaking changes, version bump, commit range, contributors, links to dashboards. Use `/release-notes` and `/version-proposal`.&#x20;

---

### Missing prompts needed

- `/scaffold-fullstack` — generate repo, workspace, app, api, tests, CI seeds.
- `/api-contract` — author initial OpenAPI/GraphQL contract from requirements.
- `/openapi-generate` — codegen server and client from OpenAPI.
- `/db-bootstrap` — pick DB, init migrations, local compose, seed scripts.
- `/migration-plan` — write up/down plans with safety checks.
- `/auth-scaffold` — OAuth/OIDC/email templates, routes, threat model.
- `/e2e-runner-setup` — Playwright/Cypress config, fixtures, data sandbox.
- `/env-setup` — `.env.example`, schema validation, per-env overrides.
- `/secrets-manager-setup` — provision secret store, map app vars.
- `/iac-bootstrap` — minimal IaC for chosen cloud, state, pipelines.
- `/monitoring-setup` — logs/metrics/traces bootstrap.
- `/slo-setup` — SLOs, alerts, dashboards.
- `/feature-flags` — flag provider, SDK wiring, guardrails.
  These integrate with existing commands and respect AGENTS gating.

---

## workflow\.mmd

```mermaid
flowchart TD
  A["Preflight Docs (§A) AGENTS"] -->|DocFetchReport OK| B[/planning-process/]
  B --> C[/scope-control/]
  C --> D[/stack-evaluation/]
  D --> E[/scaffold-fullstack/]
  E --> F[/api-contract/]
  F --> G[/openapi-generate/]
  G --> H[/modular-architecture/]
  H --> I[/db-bootstrap/]
  I --> J[/migration-plan/]
  J --> K[/auth-scaffold/]
  K --> L[/e2e-runner-setup/]
  L --> M[/integration-test/]
  M --> N[/coverage-guide/]
  N --> O[/regression-guard/]
  O --> P[/version-control-guide/]
  P --> Q[/devops-automation/]
  Q --> R[/env-setup/]
  R --> S[/secrets-manager-setup/]
  S --> T[/iac-bootstrap/]
  T --> U[/owners/]
  U --> V[/review/]
  V --> W[/review-branch/]
  W --> X[/pr-desc/]
  X --> Y{Gates}
  Y -->|Scope Gate pass| Z1[proceed]
  Y -->|Test Gate pass| Z2[proceed]
  Y -->|Review Gate pass| Z3[proceed]
  Z3 --> AA[/release-notes/]
  AA --> AB[/version-proposal/]
  AB --> AC{Release Gate}
  AC -->|pass| AD[Deploy Staging]
  AD --> AE[Canary + Health]
  AE -->|ok| AF[Deploy Prod]
  AE -->|fail| AR[Rollback]
  AF --> AG[/monitoring-setup/]
  AG --> AH[/slo-setup/]
  AH --> AI[/logging-strategy/]
  AI --> AJ[/error-analysis/]
  AJ --> AK[/fix/]
  AK --> AL[/refactor-suggestions/]
  AL --> AM[/file-modularity/]
  AM --> AN[/dead-code-scan/]
  AN --> AO[/cleanup-branches/]
  AF --> AP[/feature-flags/]
  AF --> AQ[/model-strengths/]
  AQ --> AR2[/model-evaluation/]
  AR2 --> AS[/compare-outputs/]
  AS --> AT[/switch-model/]
```

---

## Nodes & Edges list

**Nodes**: Preflight, planning-process, scope-control, stack-evaluation, scaffold-fullstack, api-contract, openapi-generate, modular-architecture, db-bootstrap, migration-plan, auth-scaffold, e2e-runner-setup, integration-test, coverage-guide, regression-guard, version-control-guide, devops-automation, env-setup, secrets-manager-setup, iac-bootstrap, owners, review, review-branch, pr-desc, Gates, release-notes, version-proposal, Deploy Staging, Canary + Health, Deploy Prod, Rollback, monitoring-setup, slo-setup, logging-strategy, error-analysis, fix, refactor-suggestions, file-modularity, dead-code-scan, cleanup-branches, feature-flags, model-strengths, model-evaluation, compare-outputs, switch-model.
**Edges**: Preflight→planning-process→scope-control→stack-evaluation→scaffold-fullstack→api-contract→openapi-generate→modular-architecture→db-bootstrap→migration-plan→auth-scaffold→e2e-runner-setup→integration-test→coverage-guide→regression-guard→version-control-guide→devops-automation→env-setup→secrets-manager-setup→iac-bootstrap→owners→review→review-branch→pr-desc→Gates→release-notes→version-proposal→Deploy Staging→Canary + Health→Deploy Prod→monitoring-setup→slo-setup→logging-strategy→error-analysis→fix→refactor-suggestions→file-modularity→dead-code-scan→cleanup-branches and Deploy Prod→feature-flags and model-strengths→model-evaluation→compare-outputs→switch-model; Canary + Health→Rollback on fail.

---

## Gate checklists

### Scope Gate

- Problem, users, Done criteria defined.
- In/Out lists and Won’t do recorded.
- Stack chosen and risks listed.
  Evidence: `/planning-process`, `/scope-control`, `/stack-evaluation`.&#x20;

### Test Gate

- E2E happy path green locally and in CI.
- No unrelated file churn.
- Regression guards added for changed modules.
  Evidence: `/integration-test`, `/regression-guard`.&#x20;

### Review Gate

- Clean diff per `/version-control-guide`.
- PR reviewed via `/review` and `/review-branch`.
- Owners assigned and approvals met.&#x20;

### Release Gate

- Staging deploy passes checks.
- Canary health metrics stable.
- Rollback rehearsed and documented.
  Evidence: `/devops-automation`, IaC, monitoring setup.&#x20;

---

## Reset Playbook

**When**: E2E red >60m, diff noisy, plan drift, large rebase pain, conflicting designs.
**Command path**: `/reset-strategy` → propose clean slice. Create new branch from main, cherry-pick minimal commits, re-run Gate sequence.&#x20;
**Data-loss warning**: Uncommitted local changes will be dropped if hard reset. Stash before reset.

---

## Model Eval Block

**When**: Contentious generation, flaky refactors, new model availability.
**Steps**: `/model-strengths` → route candidates. `/model-evaluation` → baseline vs new. `/compare-outputs` → pick best. `/switch-model` → roll change with guardrails. Success = higher test pass rate or smaller diff with same tests.&#x20;

## Support

**Purpose**: Cross-cutting helpers that smooth transitions between gated stages.
**Steps**: `/voice-input` → turn transcripts into structured prompts. `/content-generation` → broadcast updates aligned with the Evidence Log.
**Gate**: Clarify requests before triggering lifecycle prompts and keep documentation current with delivered work.

---

**Notes**

- Baseline precedence and Preflight come from AGENTS baseline. Prompts are manual. No auto-invoke.&#x20;
- Command catalog and many building blocks exist already; this plan wires them into a complete “from scratch” path and lists required new prompts.&#x20;


--- action-diagram.md ---
You are a CLI assistant focused on helping contributors with the task: Explain workflow triggers and dependencies as a diagram‑ready outline.

1. Gather context by inspecting `.github/workflows`.
2. Explain workflow triggers and dependencies as a diagram‑ready outline.
3. Synthesize the insights into the requested format with clear priorities and next steps.

Output:

- Begin with a concise summary that restates the goal: Explain workflow triggers and dependencies as a diagram‑ready outline.
- Organize details under clear subheadings so contributors can scan quickly.
- List nodes and edges to make diagram creation straightforward.
- Highlight workflow triggers, failing jobs, and proposed fixes.

Example Input:
(none – command runs without arguments)

Expected Output:

## Nodes

- build
- deploy

## Edges

- push -> build
- build -> deploy


--- adr-new.md ---
You are a CLI assistant focused on helping contributors with the task: Draft an Architecture Decision Record with pros/cons.

1. Gather context by inspecting `README.md` for the project context.
2. Draft a concise ADR including Context, Decision, Status, Consequences. Title: <args>.
3. Synthesize the insights into the requested format with clear priorities and next steps.

Output:

- Begin with a concise summary that restates the goal: Draft an Architecture Decision Record with pros/cons.
- Highlight workflow triggers, failing jobs, and proposed fixes.
- Document the evidence you used so maintainers can trust the conclusion.

Example Input:
src/example.ts

Expected Output:

- Actionable summary aligned with the output section.


--- adr-new.refactor.md ---
**{$2 or Inferred Name}**

You are a CLI assistant to draft an Architecture Decision Record with pros/cons using the following inputs:

1. Analyze project context from $1.
2. Generate a concise ADR with Context, Decision, Status, Consequences. Title: $3.
3. Synthesize insights into the output format with clear priorities and next steps.

**Output Requirements**:
- Provide a summary restating the goal.
- Highlight $4, $5, and $6.
- Document $7 to ensure maintainability.

**Example Input**: $2

**Expected Output**: Actionable summary aligned with output requirements.


--- audit.md ---
---
phase: "P7 Release & Ops"
gate: "Release Gate"
status: "readiness criteria before shipping."
previous:
  - "/logging-strategy"
next:
  - "/error-analysis"
  - "/fix"
---

# Audit

Trigger: /audit

Purpose: Audit repository hygiene and suggest improvements.

## Steps

1. Gather context by running `ls -la` for the top-level listing. Inspect `.editorconfig`, `.gitignore`, `.geminiignore`, `.eslintrc.cjs`, `.eslintrc.js`, `tsconfig.json`, and `pyproject.toml` if present to understand shared conventions.
2. Assess repository hygiene across documentation, testing, CI, linting, and security. Highlight gaps and existing automation.
3. Synthesize the findings into a prioritized checklist with recommended next steps.

## Output format

- Begin with a concise summary that restates the goal: Audit repository hygiene and suggest improvements.
- Offer prioritized, actionable recommendations with rationale.
- Call out test coverage gaps and validation steps.
- Highlight workflow triggers, failing jobs, and proposed fixes.

## Example input

(none – command runs without arguments)

## Expected output

- Structured report following the specified sections.



--- auth-scaffold.md ---
---
phase: "P3 Data & Auth"
gate: "Migration dry-run"
status: "auth flows threat-modeled and test accounts wired."
previous:
  - "/migration-plan"
next:
  - "/modular-architecture"
  - "/ui-screenshots"
  - "/e2e-runner-setup"
---

# Auth Scaffold

Trigger: /auth-scaffold <oauth|email|oidc>

Purpose: Scaffold auth flows, routes, storage, and a basic threat model.

**Steps:**

1. Select provider (OAuth/OIDC/email) and persistence for sessions.
2. Generate routes: login, callback, logout, session refresh.
3. Add CSRF, state, PKCE where applicable. Include secure cookie flags.
4. Document threat model: replay, fixation, token leakage, SSRF on callbacks.
5. Wire to frontend with protected routes and user context.

**Output format:** route list, config keys, and mitigations table.

**Examples:** `/auth-scaffold oauth` → NextAuth/Passport/Custom adapter plan.

**Notes:** Never print real secrets. Use placeholders in `.env.example`.



--- blame-summary.md ---
You are a CLI assistant focused on helping contributors with the task: Summarize authorship hotspots for a file using git blame.

1. Gather context by running `git blame -w --line-porcelain {{args}} | sed -n 's/^author //p' | sort | uniq -c | sort -nr | sed -n '1,25p'` for the blame authors (top contributors first).
2. Given the blame summary below, identify ownership hotspots and potential reviewers.
3. Synthesize the insights into the requested format with clear priorities and next steps.

Output:

- Begin with a concise summary that restates the goal: Summarize authorship hotspots for a file using git blame.
- Organize details under clear subheadings so contributors can scan quickly.
- Reference evidence from CODEOWNERS or git history for each owner suggestion.

Example Input:
src/components/Button.tsx

Expected Output:

- Refactor proposal extracting shared styling hook with before/after snippet.


--- changed-files.md ---
You are a CLI assistant focused on helping contributors with the task: Summarize changed files between HEAD and origin/main.

1. Gather context by running `git diff --name-status origin/main...HEAD`.
2. List and categorize changed files: added/modified/renamed/deleted. Call out risky changes.
3. Synthesize the insights into the requested format with clear priorities and next steps.

Output:

- Begin with a concise summary that restates the goal: Summarize changed files between HEAD and origin/main.
- Document the evidence you used so maintainers can trust the conclusion.

Example Input:
(none – command runs without arguments)

Expected Output:

- Structured report following the specified sections.


--- check.md ---
You are a CLI assistant focused on helping contributors with the task: Check adherence to .editorconfig across the repo.

1. Gather context by inspecting `.editorconfig`; running `git ls-files | sed -n '1,400p'`.
2. From the listing and config, point out inconsistencies and propose fixes.
3. Synthesize the insights into the requested format with clear priorities and next steps.

Output:

- Begin with a concise summary that restates the goal: Check adherence to .editorconfig across the repo.
- Offer prioritized, actionable recommendations with rationale.
- Highlight workflow triggers, failing jobs, and proposed fixes.

Example Input:
(none – command runs without arguments)

Expected Output:

- Structured report following the specified sections.


--- cleanup-branches.md ---
---
phase: "P8 Post-release Hardening"
gate: "Post-release cleanup"
status: "repo tidy with stale branches archived."
previous:
  - "/dead-code-scan"
next:
  - "/feature-flags"
  - "/model-strengths"
---

# Cleanup Branches

Trigger: /cleanup-branches

Purpose: Recommend which local branches are safe to delete and which to keep.

You are a CLI assistant focused on helping contributors with the task: Suggest safe local branch cleanup (merged/stale).

1. Gather context by running `git branch --merged` for the merged into current upstream; running `git branch --no-merged` for the branches not merged; running `git for-each-ref --sort=-authordate --format='%(refname:short) — %(authordate:relative)' refs/heads` for the recently updated (last author dates).
2. Using the lists below, suggest local branches safe to delete and which to keep. Include commands to remove them if desired (DO NOT execute).
3. Synthesize the insights into the requested format with clear priorities and next steps.

Output:

- Begin with a concise summary that restates the goal: Suggest safe local branch cleanup (merged/stale).
- Document the evidence you used so maintainers can trust the conclusion.

Example Input:
(none – command runs without arguments)

Expected Output:

- Structured report following the specified sections.



--- code-review-high.md ---
<task>
$1
</task>

<role>
  You are a senior software engineer, security reviewer, and performance specialist. Review the provided change with a focus on correctness, security, performance, integration, test quality, and long-term maintainability. Be precise, cite file paths and line ranges, and prioritize risks that could impact users, data, uptime, or developer velocity.
</role>

<objectives>
- Identify correctness defects, code smells, and anti-patterns.
- Surface exploitable security issues and data-protection risks.
- Spot performance/regression risks and complexity hotspots.
- Check integration points (APIs, events, DB, configs, CI/CD, infra) for compatibility and rollout safety.
- Assess tests for sufficiency, signal, reliability, and coverage.
- Recommend minimal, safe, high-leverage improvements and monitoring.
</objectives>

<severity_rubric>
- BLOCKER: Exploitable security flaw, data loss risk, broken build/deploy, user-impacting crash, irreversible migration risk, leaked secrets.
- HIGH: Likely prod incident or major regression; authz/auth gaps; significant perf degradation; schema incompatibility.
- MEDIUM: Correctness edge cases; non-exploitable but risky pattern; moderate perf concerns; flaky tests.
- LOW: Maintainability, readability, style, minor test gaps; suggestions.
- NIT: Optional polish.
</severity_rubric>

<tasks>
- Scope & Impact: Map all affected files/modules and why each is implicated. Note transitive and runtime impact (build, deploy, config, data).
- Root Cause / Risk Analysis: Explain the change intent, risks introduced, and any hidden assumptions or environmental dependencies.
- Security Review: Use the checklist below; escalate any secret exposure, injection, auth/authz flaws, SSRF/XXE/path traversal, insecure deserialization, command execution, mass assignment, CSRF/XSS, prototype pollution, weak crypto, missing TLS verification, permissive CORS, logging of secrets/PII, dependency vulns, or IaC/container misconfigurations.
- Performance Review: Identify complexity issues, N+1 queries, unbounded loops, memory churn/leaks, blocking I/O on hot paths, missing indexes, cache misuse, chatty network calls, unnecessary allocations/boxing, and concurrency contention.
- Integration Review: Validate API schema changes, versioning, backward/forward compatibility, idempotency, retries/timeouts/circuit breakers, feature-flag rollout, DB migrations (order/locking/rollback), message/event contracts, and config drift.
- Testing Review: Evaluate unit/integration/e2e tests, coverage, negative/property-based cases, concurrency/time-dependent tests, fixture health, determinism, and flakiness risk. Propose a targeted test plan.
- Observability & Operations: Check log levels, PII in logs, correlation/trace IDs, metrics and alerts, runbooks. Recommend what to monitor post-merge.
- Documentation & DX: Flag missing or outdated README/CHANGELOG/ADRs/API docs/config comments/schema diagrams. Note onboarding and maintenance friction.
- Minimal, Safe Fix: Propose the smallest viable change to eliminate blockers/high risks. Include tests and rollout/rollback steps.
</tasks>

<detailed_checklist>
  <category name="Correctness & Code Smells">
- Duplicate code / long methods / large classes / deep nesting.
- Leaky abstractions, tight coupling, poor cohesion, improper layering.
- Dead code, unused variables/imports, TODOs that should be addressed now.
- Non-idempotent operations where idempotency is required.
- Edge cases: null/empty/NaN/overflow/encoding/timezone/locale.
- Concurrency: shared state, race conditions, improper locking, async misuse.
  </category>

  <category name="Security">
- Secrets in code/logs/env/examples; credential handling, key rotation, KMS/secret manager usage.
- Input validation & output encoding; SQL/NoSQL/LDAP/OS injection; XSS (reflected/stored/DOM); CSRF.
- AuthN/AuthZ: broken access control, least privilege, multi-tenant boundaries, insecure direct object references.
- SSRF/XXE/path traversal/file upload validation; sandboxing for untrusted inputs.
- Crypto: algorithms, modes, IVs, nonces, randomness, key sizes, cert pinning/TLS verification.
- CORS/security headers (CSP/HSTS/X-Frame-Options/SameSite), cookie flags.
- Dependency & supply chain: pinned versions, known CVEs, pre/post-install scripts, integrity checks.
- IaC/Containers: public buckets, open security groups (0.0.0.0/0), missing encryption, root containers, mutable latest tags.
- Data protection & privacy: PII/PHI handling, minimization, retention, encryption at rest/in transit.
  </category>

  <category name="Performance">
- Time/space complexity, hot-path allocations, unnecessary synchronization.
- N+1 queries, missing DB indexes, inefficient joins, full scans, pagination vs. streaming.
- Caching: invalidation, eviction, key design, stampedes.
- Network patterns: chattiness, batching, compression, timeouts, backoff.
- Client-side perf (if UI): bundle size/regressions, critical path, images/fonts.
  </category>

  <category name="Integration & Rollout Safety">
- Backward/forward compatibility; versioned contracts; consumer-producer alignment.
- DB migrations: zero-downtime (expand/migrate/contract), locks, data backfills, rollback plan.
- Feature flags: default off, kill switch, gradual rollout, owner/expiry.
- Resilience: retries with jitter, timeouts, circuit breakers, idempotency keys.
- Config changes: validation, defaults, environment parity, secrets not in plain text.
- CI/CD: reproducibility, cache safety, test gates, artifact signing.
  </category>

  <category name="Testing & Quality Signals">
- Tests exist for new behavior and regressions; meaningful assertions.
- Coverage on critical branches/edge cases; mutation score (if available).
- Isolation: minimal mocking vs. over-mocking; flaky patterns (sleep-based timing, order reliance).
- Property-based/fuzz tests for parsers/validators/serializers.
- Load/soak tests where perf risk exists; snapshot tests stability (if UI).
  </category>

  <category name="Docs, Observability, Accessibility, i18n">
- README/CHANGELOG/ADR/API docs updated; code comments for non-obvious logic.
- Logs/metrics/traces with actionable context; PII redaction; alert thresholds.
- Accessibility (if UI): semantics, focus order, labels, contrast, keyboard nav, ARIA use.
- i18n/l10n: hard-coded strings, pluralization, date/time/number formats.
  </category>
</detailed_checklist>

  <output_requirements>
    <instructions>
- Produce a concise but comprehensive report.
- Group findings by category and severity.
- Reference exact file paths and line ranges (e.g., src/foo/bar.py:120–147).
- Include brief code excerpts only as necessary (≤20 lines per finding).
- Prefer specific, minimal fixes and tests that maximize risk reduction.
- If information is missing, state the assumption and its impact.
    </instructions>
  </output_requirements>

  <report_skeleton>
- Summary:
  - What changed: <concise overview>
  - Top risks: <1-3 bullets>
  - Approval: <approve|comment|request_changes|blocker>

- Affected files:
  - <path> — <reason> (<added|modified|deleted>)

- Root cause & assumptions:
  - <analysis>
  - Assumptions: <items>

- Findings (repeat per finding):
  - [<severity>] [<category>] <short title>
    - Where: <file:line-range>
    - Evidence: <brief snippet_trace>
    - Impact: <what breaks_who is affected>
    - Standards: <CWE/OWASP/Policy refs>
    - Repro: <steps>
    - Recommendation: <minimal fix>
    - Tests: <tests to add_update>

- Performance:
  - Hotspots: <items>
  - Complexity notes: <items>
  - Bench/Monitoring plan: <how to measure & watch>

- Integration:
  - API/contracts: <compat/versioning/idempotency>
  - DB migrations: <expand-migrate-contract, locks, rollback>
  - Feature flags & rollout: <plan/kill switch_owner>
  - Resilience: <timeouts/retries/circuits>
  - Rollback plan: <how to revert safely>

- Testing:
  - Coverage: <statements_branches_critical_paths>
  - Gaps: <cases>
  - Flakiness risks: <items>
  - Targeted test plan: <Given_When_Then bullets>

- Docs & Observability:
  - Docs to update/create: <paths/sections>
  - Logs/Metrics/Traces/Alerts: <plan>
  - Runbook: <updates>

- Open questions:
  - <items>

- Final recommendation:
  - Decision: <approve|comment|request_changes|blocker>
  - Must-fix before merge: <items>
  - Nice-to-have post-merge: <items>
  - Confidence: <low|medium|high>
  </report_skeleton>

<process_notes>
- Prioritize BLOCKER/HIGH issues. If any are found, set approval to “blocker” or “request_changes”.
- Favor minimal, safe changes and targeted tests over broad refactors (unless safety demands it).
- If diff is very large, focus on high-risk/new code paths, public interfaces, security-critical modules, and hot paths.
- Reference concrete files/lines. Keep code excerpts minimal (≤20 lines). Do not rewrite large code blocks.
- If required inputs are missing (e.g., DB migration script or API schema), flag as a risk and propose what is needed.
</process_notes>

<constraints>
- DO NOT write or generate full code implementations in this review. Provide patch outlines, pseudocode, or stepwise instructions only.
- Maintain confidentiality: if a secret or sensitive data appears, describe it without reproducing it verbatim.
</constraints>

<success_criteria>
- Findings are specific, actionable, and ordered by severity and blast radius.
- Every high-risk change has a minimal fix and a concrete test/monitoring plan.
- Output follows the provided report skeleton (markdown text only).
</success_criteria>

<reminder>DON'T CODE YET.</reminder>


--- code-review-low.md ---
<task>
$1
</task>

## Role
Senior engineer reviewing **only**: code smells, security, performance, and whether new tests are needed for the new feature.

## Inputs
- {CHANGE_SUMMARY}
- {DIFF} + {FILES}
- {CI_LOGS} {COVERAGE_SUMMARY} (optional)
- {ENVIRONMENT} {API_SCHEMAS} {DB_MIGRATIONS} {DEPENDENCIES} (if relevant)

## What to check
### Code Smells
- Duplicates, long methods, deep nesting, dead code, unused imports
- Leaky abstractions, tight coupling, improper layering
- Edge cases: null, empty, timezones, encodings
- Concurrency misuse and non-idempotent ops where required

### Security
- Secrets in code/logs; proper secret management
- Input validation and output encoding; SQL/NoSQL/OS injection; XSS/CSRF
- AuthN/AuthZ and multi-tenant boundaries
- SSRF/XXE/path traversal/file upload validation
- Crypto choices; TLS verification; CORS and security headers
- Dependency CVEs and supply-chain risks; IaC/container misconfig

### Performance
- Time/space complexity; hot-path allocations; blocking I/O
- N+1 queries; missing indexes; inefficient joins; full scans
- Caching correctness and stampedes
- Chatty network calls; batching; timeouts; backoff
- Client bundle size and critical path (if UI)

### Tests needed
- Does new behavior have unit/integration/e2e tests
- Edge cases, negative cases, concurrency/time-based cases
- Minimal test plan to guard the change

## Output format
- **Summary**: what changed, top 1–3 risks, **Decision**: approve | request_changes | blocker
- **Findings** grouped by **Smell | Security | Performance | Tests**
  - `[severity] <title>`  
    - Where: `<file:line-range>`  
    - Impact: `<who/what is affected>`  
    - Recommendation: `<smallest safe fix>`  
    - Tests: `<tests to add/update>`
- Cite exact files and line ranges. Keep code excerpts ≤20 lines.

## Constraints
- No full implementations. Pseudocode or patch outline only.
- If data is missing, state the assumption and risk.

--- commit.md ---
---
phase: "P6 CI/CD & Env"
gate: "Review Gate"
status: "clean diff, CI green, and approvals ready."
previous:
  - "/version-control-guide"
next:
  - "/devops-automation"
  - "/env-setup"
---

# Commit Message Assistant

Trigger: `commit`

Purpose: Generate a conventional, review-ready commit message from the currently staged changes.

Output: A finalized commit message with a 50–72 character imperative subject line, optional scope, and supporting body lines describing the rationale, evidence, and tests.

## Steps

1. Verify there is staged work with `git status --short` and stop with guidance if nothing is staged.
2. Inspect the staged diff with `git diff --staged` and identify the primary change type (feat, fix, chore, docs, refactor, etc.) and optional scope (e.g., package or module).
3. Draft a concise subject line in the form `<type>(<scope>): <imperative summary>` or `<type>: <imperative summary>` when no scope applies. Keep the line under 73 characters.
4. Capture essential details in the body as wrapped bullet points or paragraphs: what changed, why it was necessary, and any follow-up actions.
5. Document validation in a trailing section (e.g., `Tests:`) noting commands executed or why tests were skipped.

## Example Output

```
fix(auth): prevent session expiration loop

- guard refresh flow against repeated 401 responses
- add regression coverage for expired refresh tokens

Tests: npm test -- auth/session.test.ts
```


--- compare-outputs.md ---
---
phase: "P9 Model Tactics"
gate: "Model uplift"
status: "comparative data compiled before switching defaults."
previous:
  - "/model-evaluation"
next:
  - "/switch-model"
---

# Compare Outputs

Trigger: /compare-outputs

Purpose: Run multiple models or tools on the same prompt and summarize best output.

## Steps

1. Define evaluation prompts and expected properties.
2. Record outputs from each model/tool with metadata.
3. Score using a rubric: correctness, compile/run success, edits required.
4. Recommend a winner and suggested settings.

## Output format

- Matrix comparison and a one-paragraph decision.



--- content-generation.md ---
---
phase: "11) Evidence Log"
gate: "Evidence Log"
status: "Ensure docs stay synced with current phase deliverables."
previous:
  - "Stage-specific work just completed"
next:
  - "/release-notes"
  - "/summary (if sharing updates)"
---

# Content Generation

Trigger: /content-generation

Purpose: Draft docs, blog posts, or marketing copy aligned with the codebase.

## Steps

1. Read repo README and recent CHANGELOG or commits.
2. Propose outlines for docs and posts.
3. Generate content with code snippets and usage examples.

## Output format

- Markdown files with frontmatter and section headings.



--- cross-check.md ---
# Conflict Resolver

Trigger: /cross-check

Purpose: Compare conflicting findings and decide which source prevails with rationale.

Steps:

1. Accept a list of SourceIDs or URLs with short findings.
2. Evaluate publisher authority, recency, directness to primary data.
3. Select the prevailing source; note contradictions and rationale.

Output format:

```
### Contradictions
- {S2 vs S5 → rationale}

### Prevails
- {SourceID} because {reason}
```

Examples:

- Input: `/cross-check S2: blog vs S5: RFC`
- Output: RFC prevails due to primary standard.

Notes:

- Always explain why one source prevails.


--- db-bootstrap.md ---
---
phase: "P3 Data & Auth"
gate: "Migration dry-run"
status: "migrations apply/rollback cleanly with seeds populated."
previous:
  - "/modular-architecture"
next:
  - "/migration-plan"
  - "/auth-scaffold"
---

# DB Bootstrap

Trigger: /db-bootstrap <postgres|mysql|sqlite|mongodb>

Purpose: Pick a database, initialize migrations, local compose, and seed scripts.

**Steps:**

1. Create `db/compose.yaml` for local dev (skip for sqlite).
2. Choose ORM/driver (Prisma or Drizzle for SQL). Add migration config.
3. Create `prisma/schema.prisma` or `drizzle/*.ts` with baseline tables (users, sessions, audit_log).
4. Add `pnpm db:migrate`, `db:reset`, `db:seed` scripts. Write seed data for local admin user.
5. Update `.env.example` with `DATABASE_URL` and test connection script.

**Output format:** Migration plan list and generated file paths.

**Examples:** `/db-bootstrap postgres` → Prisma + Postgres docker-compose.

**Notes:** Avoid destructive defaults; provide `--preview-feature` warnings if relevant.



--- dead-code-scan.md ---
---
phase: "P8 Post-release Hardening"
gate: "Post-release cleanup"
status: "ensure code removals keep prod stable."
previous:
  - "/file-modularity"
next:
  - "/cleanup-branches"
  - "/feature-flags"
---

# Dead Code Scan

Trigger: /dead-code-scan

Purpose: Identify likely dead or unused files and exports using static signals.

You are a CLI assistant focused on helping contributors with the task: List likely dead or unused files and exports (static signals).

1. Gather context by running `rg -n "export |module.exports|exports\.|require\(|import " -g '!node_modules' .` for the file reference graph (best‑effort).
2. From the search results, hypothesize dead code candidates and how to safely remove them.
3. Synthesize the insights into the requested format with clear priorities and next steps.

Output:

- Begin with a concise summary that restates the goal: List likely dead or unused files and exports (static signals).
- Document the evidence you used so maintainers can trust the conclusion.

Example Input:
(none – command runs without arguments)

Expected Output:

- Structured report following the specified sections.



--- devops-automation.md ---
---
phase: "P6 CI/CD & Env"
gate: "Review Gate"
status: "CI pipeline codified, rollback steps rehearsed."
previous:
  - "/version-control-guide"
next:
  - "/env-setup"
  - "/secrets-manager-setup"
  - "/iac-bootstrap"
---

# DevOps Automation

Trigger: /devops-automation

Purpose: Configure servers, DNS, SSL, CI/CD at a pragmatic level.

## Steps

1. Inspect repo for IaC or deploy scripts.
2. Generate Terraform or Docker Compose templates if missing.
3. Propose CI workflows for tests, builds, and deploys.
4. Provide runbooks for rollback.

## Output format

- Infra plan with checkpoints and secrets placeholders.



--- e2e-runner-setup.md ---
---
phase: "P5 Quality Gates & Tests"
gate: "Test Gate"
status: "runner green locally and wired into CI before expanding coverage."
previous:
  - "/auth-scaffold"
  - "/ui-screenshots"
next:
  - "/integration-test"
  - "/coverage-guide"
---

# E2E Runner Setup

Trigger: /e2e-runner-setup <playwright|cypress>

Purpose: Configure an end-to-end test runner with fixtures and a data sandbox.

**Steps:**

1. Install runner and add config with baseURL, retries, trace/videos on retry only.
2. Create fixtures for auth, db reset, and network stubs. Add `test:serve` script.
3. Provide CI job that boots services, runs E2E, uploads artifacts.

**Output format:** file list, scripts, and CI snippet fenced code block.

**Examples:** `/e2e-runner-setup playwright`.

**Notes:** Keep runs under 10 minutes locally; parallelize spec files.



--- env-setup.md ---
---
phase: "P6 CI/CD & Env"
gate: "Review Gate"
status: "environment schemas enforced and CI respects strict loading."
previous:
  - "/devops-automation"
next:
  - "/secrets-manager-setup"
  - "/iac-bootstrap"
---

# Env Setup

Trigger: /env-setup

Purpose: Create .env.example, runtime schema validation, and per-env overrides.

**Steps:**

1. Scan repo for `process.env` usage and collected keys.
2. Emit `.env.example` with comments and safe defaults.
3. Add runtime validation via `zod` or `envsafe` in `packages/config`.
4. Document `development`, `staging`, `production` precedence and loading order.

**Output format:** `.env.example` content block and `config/env.ts` snippet.

**Examples:** `/env-setup`.

**Notes:** Do not include real credentials. Enforce `STRICT_ENV=true` in CI.



--- error-analysis.md ---
---
phase: "P8 Post-release Hardening"
gate: "Post-release cleanup"
status: "Sev-1 incidents triaged with fixes scheduled."
previous:
  - "/logging-strategy"
  - "/audit"
next:
  - "/fix"
  - "/refactor-suggestions"
---

# Error Analysis

Trigger: /error-analysis

Purpose: Analyze error logs and enumerate likely root causes with fixes.

## Steps

1. Collect last test logs or application stack traces if present.
2. Cluster errors by symptom. For each cluster list 2–3 plausible causes.
3. Propose instrumentation or inputs to disambiguate.
4. Provide minimal patch suggestions and validation steps.

## Output format

- Table: error → likely causes → next checks → candidate fix.

## Examples

- "TypeError: x is not a function" → wrong import, circular dep, stale build.



--- eslint-review.md ---

You are a CLI assistant focused on helping contributors with the task: Review ESLint config and suggest rule tweaks.

1. Gather context by inspecting `.eslintrc.cjs`; inspecting `.eslintrc.js`; inspecting `package.json`.
2. Explain key rules, missing plugins, and performance considerations.
3. Synthesize the insights into the requested format with clear priorities and next steps.

Output:

- Begin with a concise summary that restates the goal: Review ESLint config and suggest rule tweaks.
- Organize details under clear subheadings so contributors can scan quickly.
- Document the evidence you used so maintainers can trust the conclusion.

Example Input:
(none – command runs without arguments)

Expected Output:

- Structured report following the specified sections.


--- evidence-capture.md ---
# Evidence Logger

Trigger: /evidence-capture

Purpose: Capture sources for a specified claim with dates, ≤25-word quotes, findings, relevance, and confidence.

Steps:

1. Read the claim text and optional URLs provided.
2. For each source, record metadata and a ≤25-word quote.
3. Add a brief Finding, Relevance (H/M/L), and Confidence (0.0–1.0).

Output format:

```
### Evidence Log
| SourceID | Title | Publisher | URL | PubDate | Accessed | Quote (≤25w) | Finding | Rel | Conf |
|---|---|---|---|---|---|---|---|---|---|
```

Examples:

- Input: `/evidence-capture "Next.js 15 requires React 19 RC"` with official links.
- Output: Evidence table entries with dates.

Notes:

- Mark missing PubDate as n/a. Prefer official documentation.


--- explain-code.md ---
---
phase: "P7 Release & Ops"
gate: "Review Gate"
status: "Improve reviewer comprehension before approvals."
previous:
  - "/owners"
  - "/review"
next:
  - "/review-branch"
  - "/pr-desc"
---

# Explain Code

Trigger: /explain-code

Purpose: Provide line-by-line explanations for a given file or diff.

## Steps

1. Accept a file path or apply to staged diff.
2. Explain blocks with comments on purpose, inputs, outputs, and caveats.
3. Highlight risky assumptions and complexity hot spots.

## Output format

- Annotated markdown with code fences and callouts.



--- explain-failures.md ---
You are a CLI assistant focused on helping contributors with the task: Analyze recent test failures and propose fixes.

1. Gather context by running `ls -1 test-results 2>/dev/null || echo 'no test-results/ directory'` for the recent test output (if present); running `find . -maxdepth 2 -name 'junit*.xml' -o -name 'TEST-*.xml' -o -name 'last-test.log' -print -exec tail -n 200 {} \; 2>/dev/null` for the recent test output (if present).
2. From the following logs, identify root causes and propose concrete fixes.
3. Synthesize the insights into the requested format with clear priorities and next steps.

Output:

- Begin with a concise summary that restates the goal: Analyze recent test failures and propose fixes.
- Offer prioritized, actionable recommendations with rationale.
- Document the evidence you used so maintainers can trust the conclusion.

Example Input:
(none – command runs without arguments)

Expected Output:

- Structured report following the specified sections.


--- explain-symbol.md ---
You are a CLI assistant focused on helping contributors with the task: Explain where and how a symbol is defined and used.

1. Gather context by running `rg -n {{args}} . || grep -RIn {{args}} .` for the results.
2. Explain where and how a symbol is defined and used.
3. Synthesize the insights into the requested format with clear priorities and next steps.

Output:

- Begin with a concise summary that restates the goal: Explain where and how a symbol is defined and used.
- Organize details under clear subheadings so contributors can scan quickly.
- Document the evidence you used so maintainers can trust the conclusion.

Example Input:
HttpClient

Expected Output:

- Definition: src/network/httpClient.ts line 42
- Key usages: services/userService.ts, hooks/useRequest.ts


--- feature-flags.md ---
---
phase: "P8 Post-release Hardening"
gate: "Post-release cleanup"
status: "guardrails added before toggling new flows."
previous:
  - "/cleanup-branches"
next:
  - "/model-strengths"
  - "/model-evaluation"
---

# Feature Flags

Trigger: /feature-flags <provider>

Purpose: Integrate a flag provider, wire the SDK, and enforce guardrails.

**Steps:**

1. Select provider (LaunchDarkly, Unleash, Flagsmith, custom).
2. Add SDK init in web/api with bootstrap values and offline mode for dev.
3. Define flag naming and ownership. Add kill‑switch pattern and monitoring.

**Output format:** SDK snippet, example usage, and guardrail checklist.

**Examples:** `/feature-flags launchdarkly`.

**Notes:** Ensure flags are typed and expire with tickets.



--- file-modularity.md ---
---
phase: "P8 Post-release Hardening"
gate: "Post-release cleanup"
status: "structure debt addressed without destabilizing prod."
previous:
  - "/refactor-suggestions"
next:
  - "/dead-code-scan"
  - "/cleanup-branches"
---

# File Modularity

Trigger: /file-modularity

Purpose: Enforce smaller files and propose safe splits for giant files.

## Steps

1. Find files over thresholds (e.g., >500 lines).
2. Suggest extraction targets: components, hooks, utilities, schemas.
3. Provide before/after examples and import updates.

## Output format

- Refactor plan with patches for file splits.



--- fix.md ---
---
phase: "P8 Post-release Hardening"
gate: "Post-release cleanup"
status: "validated fix with regression coverage before closing incident."
previous:
  - "/error-analysis"
next:
  - "/refactor-suggestions"
  - "/file-modularity"
---

# Fix

Trigger: /fix "<bug summary>"

Purpose: Propose a minimal, correct fix with diff-style patches.

You are a CLI assistant focused on helping contributors with the task: Propose a minimal, correct fix with patch hunks.

1. Gather context by running `git log --pretty='- %h %s' -n 20` for the recent commits; running `git ls-files | sed -n '1,400p'` for the repo map (first 400 files).
2. Bug summary: <args>. Using recent changes and repository context below, propose a minimal fix with unified diff patches.
3. Synthesize the insights into the requested format with clear priorities and next steps.

Output:

- Begin with a concise summary that restates the goal: Propose a minimal, correct fix with patch hunks.
- Provide unified diff-style patches when recommending code changes.
- Offer prioritized, actionable recommendations with rationale.

Example Input:
Authentication failure after password reset

Expected Output:

```
diff
- if (!user) return error;
+ if (!user) return { status: 401 };
```

Regression test: add case for missing user.



--- gemini-map.md ---
name: Gemini→Codex Mapper
command: /gemini-map
tags: migration, prompts, tooling
scope: toml-to-codex

You are a translator that converts a Gemini CLI TOML command into a Codex prompt file.

Steps:

1) Read TOML with `description` and `prompt`.
2) Extract the task, inputs, and outputs implied by the TOML.
3) Write a Codex prompt file ≤ 300 words:

    - Role line `You are ...`
    - Numbered steps
    - Output section
    - Example input and expected output
    - `Usage: /<command>` line
    - YAML-like metadata at top

4) Choose a short, hyphenated filename ≤ 32 chars.
5) Emit a ready-to-run bash snippet:
`cat > ~/.codex/prompts/<filename>.md << 'EOF'` … `EOF`.
6) Do not include destructive commands or secrets.

Example input:

```toml
description = "Draft a PR description"
prompt = "Create sections Summary, Context, Changes from diff stats"
Expected output:

A pr-desc.md file with the structure above and a bash cat > block.

Usage: /gemini-map


--- generate.md ---
---
phase: "P5 Quality Gates & Tests"
gate: "Test Gate"
status: "targeted unit tests authored for the specified module."
previous:
  - "/coverage-guide"
next:
  - "/regression-guard"
---

# Generate Unit Tests

Trigger: /generate <source-file>

Purpose: Generate unit tests for a given source file.

You are a CLI assistant focused on helping contributors with the task: Generate unit tests for a given source file.

## Steps

1. Inspect `package.json` to identify the unit test framework, runner scripts, and any helper utilities required for the suite.
2. Review the target source file with `sed -n '1,400p' {{args}}` to catalog exported members, branching logic, and error handling paths that must be exercised.
3. Outline the test file structure (location, naming, setup/teardown) and propose arrange/act/assert cases that cover happy paths, edge cases, and failure scenarios.
4. Provide guidance on implementing the tests and how to validate them locally (e.g., `npm test -- <pattern>` or framework-specific commands).

## Output

- Begin with a concise summary that restates the goal: Generate unit tests for a given source file.
- List the recommended test files, describe each test case, and highlight coverage gaps they close.
- Call out the command(s) to run the new tests and any fixtures or mocks required.
- Document the evidence you used (e.g., `package.json`, specific functions/branches in the source file) so maintainers can trust the conclusion.

## Example

**Input**

```
src/components/Button.tsx
```

**Output**

- Summary: Author React Testing Library unit tests for `Button` to cover rendering, disabled behavior, and click handling.
- Create `src/components/__tests__/Button.test.tsx` that:
  - Renders the button label and asserts it matches `props.children`.
  - Verifies `onClick` fires once when the button is enabled and is skipped when `disabled` is true.
  - Confirms the `variant="primary"` branch applies the `btn-primary` class.
- Validation: Run `npm test -- Button.test.tsx` to execute the suite.
- Evidence: `package.json` (scripts.test uses Jest + RTL), component branches in `src/components/Button.tsx` (disabled guard, variant styling).


--- grep.md ---
You are a CLI assistant focused on helping contributors with the task: Recursive text search with ripgrep/grep injection.

1. Gather context by running `rg -n {{args}} . || grep -RIn {{args}} .`.
2. Show matched lines with file paths and line numbers.
3. Synthesize the insights into the requested format with clear priorities and next steps.

Output:

- Begin with a concise summary that restates the goal: Recursive text search with ripgrep/grep injection.
- Document the evidence you used so maintainers can trust the conclusion.

Example Input:
HttpClient

Expected Output:

- Usage cluster in src/network/* with note on inconsistent error handling.


--- iac-bootstrap.md ---
---
phase: "P6 CI/CD & Env"
gate: "Review Gate"
status: "IaC applied in staging with drift detection configured."
previous:
  - "/secrets-manager-setup"
next:
  - "/owners"
  - "/review"
---

# IaC Bootstrap

Trigger: /iac-bootstrap <aws|gcp|azure|fly|render>

Purpose: Create minimal Infrastructure-as-Code for the chosen platform plus CI hooks.

**Steps:**

1. Select tool (Terraform, Pulumi). Initialize backend and state.
2. Define stacks for `preview`, `staging`, `prod`. Add outputs (URLs, connection strings).
3. Add CI jobs: plan on PR, apply on main with manual approval.
4. Document rollback and drift detection.

**Output format:** stack diagram, file list, CI snippets.

**Examples:** `/iac-bootstrap aws`.

**Notes:** Prefer least privilege IAM and remote state with locking.



--- instruction-file.md ---
---
phase: "P0 Preflight Docs"
gate: "DocFetchReport"
status: "capture approved instructions before proceeding."
previous:
  - "Preflight discovery (AGENTS baseline)"
next:
  - "/planning-process"
  - "/scope-control"
---

# Instruction File

Trigger: /instruction-file

Purpose: Generate or update `cursor.rules`, `windsurf.rules`, or `claude.md` with project-specific instructions.

## Steps

1. Scan repo for existing instruction files.
2. Compose sections: Context, Coding Standards, Review Rituals, Testing, Security, Limits.
3. Include "Reset and re-implement cleanly" guidance and scope control.
4. Write to chosen file and propose a commit message.

## Output format

- Markdown instruction file with stable headings.



--- integration-test.md ---
---
phase: "P5 Quality Gates & Tests"
gate: "Test Gate"
status: "happy path E2E must pass locally and in CI."
previous:
  - "/e2e-runner-setup"
next:
  - "/coverage-guide"
  - "/regression-guard"
---

# Integration Test

Trigger: /integration-test

Purpose: Generate E2E tests that simulate real user flows.

## Steps

1. Detect framework from `package.json` or repo (Playwright/Cypress/Vitest).
2. Identify critical path scenarios from `PLAN.md`.
3. Produce test files under `e2e/` with arrange/act/assert and selectors resilient to DOM changes.
4. Include login helpers and data setup. Add CI commands.

## Output format

- Test files with comments and a README snippet on how to run them.

## Examples

- Login, navigate to dashboard, create record, assert toast.

## Notes

- Prefer data-test-id attributes. Avoid brittle CSS selectors.



--- jest.config.ts ---
/** @jest-config-loader ts-node */
/** @jest-config-loader-options {"transpileOnly": true} */

import type {Config} from 'jest';

const config: Config = {
  preset: 'ts-jest/presets/default-esm',
  testEnvironment: 'node',
  extensionsToTreatAsEsm: ['.ts'],
  moduleNameMapper: {
    '^(\\.{1,2}/.*)\\.js$': '$1'
  },
  roots: ['<rootDir>/src', '<rootDir>/test', '<rootDir>/tests'],
  transform: {
    '^.+\\.(ts|tsx)$': [
      'ts-jest',
      {
        useESM: true,
        tsconfig: 'tsconfig.json'
      }
    ]
  }
};

export default config;


--- logging-strategy.md ---
phase: "P7 Release & Ops"
gate: "Release Gate"
status: "logging guardrails ready for canary/production checks; coordinate with P4 Frontend UX for client telemetry."
previous:

- "/monitoring-setup"
- "/slo-setup"
next:
- "/audit"
- "/error-analysis"

---

# Logging Strategy

Trigger: /logging-strategy

Purpose: Add or remove diagnostic logging cleanly with levels and privacy in mind.

## Steps

1. Identify hotspots from recent failures.
2. Insert structured logs with contexts and correlation IDs.
3. Remove noisy or PII-leaking logs.
4. Document log levels and sampling in `OBSERVABILITY.md`.

## Output format

- Diff hunks and a short guideline section.



--- migration-plan.md ---
---
phase: "P3 Data & Auth"
gate: "Migration dry-run"
status: "validated rollback steps and safety checks documented."
previous:
  - "/db-bootstrap"
next:
  - "/auth-scaffold"
  - "/e2e-runner-setup"
---

# Migration Plan

Trigger: /migration-plan "<change summary>"

Purpose: Produce safe up/down migration steps with checks and rollback notes.

**Steps:**

1. Describe current vs target schema, include data volume and lock risk.
2. Plan: deploy empty columns, backfill, dual-write, cutover, cleanup.
3. Provide SQL snippets and PR checklist. Add `can_rollback: true|false` flag.

**Output format:** `Plan`, `SQL`, `Rollback`, `Checks` sections.

**Examples:** `/migration-plan "orders add status enum"`.

**Notes:** Include online migration strategies for large tables.



--- model-evaluation.md ---
---
phase: "P9 Model Tactics"
gate: "Model uplift"
status: "experiments must beat baseline quality metrics."
previous:
  - "/model-strengths"
next:
  - "/compare-outputs"
  - "/switch-model"
---

# Model Evaluation

Trigger: /model-evaluation

Purpose: Try a new model and compare outputs against a baseline.

## Steps

1. Define a benchmark set from recent tasks.
2. Run candidates and collect outputs and metrics.
3. Analyze failures and summarize where each model excels.

## Output format

- Summary table and recommendations to adopt or not.



--- model-strengths.md ---
---
phase: "P9 Model Tactics"
gate: "Model uplift"
status: "capture baseline routing before experimentation."
previous:
  - "/feature-flags (optional)"
  - "Stage-specific blockers"
next:
  - "/model-evaluation"
  - "/compare-outputs"
---

# Model Strengths

Trigger: /model-strengths

Purpose: Choose model per task type.

## Steps

1. Classify task: UI, API, data, testing, docs, refactor.
2. Map historical success by model.
3. Recommend routing rules and temperatures.

## Output format

- Routing guide with examples.



--- monitoring-setup.md ---
---
phase: "P7 Release & Ops"
gate: "Release Gate"
status: "observability baselines ready before rollout."
previous:
  - "/version-proposal"
next:
  - "/slo-setup"
  - "/logging-strategy"
---

# Monitoring Setup

Trigger: /monitoring-setup

Purpose: Bootstrap logs, metrics, and traces with dashboards per domain.

**Steps:**

1. Choose stack: OpenTelemetry → Prometheus/Grafana, or vendor.
2. Instrument web and api for request latency, error rate, throughput, and core domain metrics.
3. Provide default dashboards JSON and alert examples.

**Output format:** instrumentation checklist and dashboard links/paths.

**Examples:** `/monitoring-setup`.

**Notes:** Avoid high‑cardinality labels. Sample traces selectively in prod.



--- owners.md ---
---
phase: "P7 Release & Ops"
gate: "Review Gate"
status: "confirm approvers and escalation paths before PR submission."
previous:
  - "/iac-bootstrap"
next:
  - "/review"
  - "/review-branch"
  - "/pr-desc"
---

# Owners

Trigger: /owners <path>

Purpose: Suggest likely owners or reviewers for the specified path.

You are a CLI assistant focused on helping contributors with the task: Suggest likely owners/reviewers for a path.

1. Gather context by inspecting `.github/CODEOWNERS` for the codeowners (if present); running `git log --pretty='- %an %ae: %s' -- {{args}} | sed -n '1,50p'` for the recent authors for the path.
2. Based on CODEOWNERS and git history, suggest owners.
3. Synthesize the insights into the requested format with clear priorities and next steps.

Output:

- Begin with a concise summary that restates the goal: Suggest likely owners/reviewers for a path.
- Reference evidence from CODEOWNERS or git history for each owner suggestion.
- Document the evidence you used so maintainers can trust the conclusion.

Example Input:
src/components/Button.tsx

Expected Output:

- Likely reviewers: @frontend-team (CODEOWNERS), @jane (last 5 commits).



--- plan-delta.md ---
# plan-delta

Trigger: /plan-delta

Purpose: Orchestrate mid-project planning deltas on an existing task graph with history preservation, lineage, and readiness recalculation.

Steps:

1. Discover repository context:
   1. Detect tasks file path: prefer `tasks.json`; else search `**/tasks.json`.
   2. Detect latest plan doc: prefer `PRD.md` or `docs/PRD.md`; else `**/*(prd|spec|plan)*.md`.
2. Snapshot:
   1. Create `./artifacts/` if missing.
   2. Copy the current tasks file to `./artifacts/tasks-$(date +%Y%m%d-%H%M%S).json` using: `cp -f <tasks.json> ./artifacts/tasks-$(date +%Y%m%d-%H%M%S).json`.
3. Input collection:
   1. Read new objectives, constraints, and findings from the user input or provided delta text.
   2. Parse selection rules to choose mode: **Continue**, **Hybrid Rebaseline**, or **Full Rebaseline**.
4. Delta Doc generation:
   1. Create `./artifacts/delta-$(date +%Y%m%d-%H%M%S).md` containing sections:
      - Objectives (new)
      - Constraints (new)
      - Impacts
      - Decisions
      - Evidence log (sources, dates, links)
5. Task graph update:
   1. Never alter historical states `done|in_progress|blocked` of existing tasks.
   2. Do not reuse IDs. For any replaced task, set `superseded_by` on the old task and include its ID in the new task's `supersedes[]`.
   3. Add `source_doc`, `lineage[]` on all new or changed tasks.
   4. Create new tasks only for new or changed work. Link predecessors via `dependencies` or `relations`.
   5. Keep deprecated tasks in graph with `status: "deprecated"` and a `reason`.
6. Graph maintenance:
   1. Recompute dependency order and validate acyclicity.
   2. Flag contradictions or invalidated edges as `blocked` with a machine-readable `blocked_reason`.
   3. Bubble critical-path tasks to the active frontier by recomputing earliest-start and slack.
7. Readiness and selection:
   1. Implement `ready/next()` over the graph: select tasks with all dependencies `done` and not `blocked`.
   2. Produce a short readiness report grouped by `ready | blocked | deprecated`.
8. Outputs:
   1. Write the updated tasks file in-place, preserving formatting where possible.
   2. Persist the Delta Doc under `./artifacts/`.
   3. Emit decision hooks: one line per change stating what it enables.
9. Termination:
   - Stop when all deltas are merged and readiness recalculated, or when a prerequisite cannot be resolved with available evidence.

Output format:

- Produce three artifacts:
  1. **Updated tasks file**: valid JSON. Preserve existing fields. Append only the new or changed tasks and relations. Do not mutate historical statuses.
  2. **Delta document**: Markdown with the exact headings `# Delta`, `## Objectives`, `## Constraints`, `## Impacts`, `## Decisions`, `## Evidence`.
  3. **Readiness report**: Plain text with sections `READY`, `BLOCKED`, `DEPRECATED`. Each item as `- <id> <title>`; blocked items add `[reason=<code>]`.
- Print **Decision hooks** as lines starting with `HOOK: <id> enables <capability>`.

Examples:

- Input →

  ```
  Mode: Continue
  New objectives: add offline export for tasks
  Constraints: no DB migrations
  Findings: existing export lib supports JSON only
  ```

  Output →
  - Updated `tasks.json` with new task `T-342` { title: "Add CSV export", dependencies: ["T-120"], source_doc: "delta-20250921.md", lineage: ["T-120"], supersedes: [] }.
  - `artifacts/delta-20250921-160500.md` populated with objectives, constraints, impacts, decisions, evidence.
  - Readiness report lists `T-342` under READY if deps done.

- Input →

  ```
  Mode: Hybrid Rebaseline
  Changes: ~30% of scope affected by auth provider swap
  ```

  Output →
  - Minor-plan version bump recorded in Delta Doc.
  - New tasks added for provider swap; prior tasks kept with `deprecated` or `blocked` and lineage links.

Notes:

- Never write outside the repo. Keep artifacts in `./artifacts/`.
- Evidence log entries include `source`, `date`, `summary`, and optional `link`.
- Selection rules: Continue (<20% change), Hybrid (20–40%), Full (>40% or goals/KPIs/architecture pivot).
- If inputs are insufficient, emit a TERMINATION note with missing evidence keys.


--- plan-review.md ---
<plan>
$1
</plan>

review the current <plan />

- reflects the current codebase (files, patterns, constraints)
- no fallbacks, no feature flags
- full change or full refactor only; no “future use” leftovers
- list code smells and caveats
- clear scope and out of scope
- performance, security, and privacy impact
- decision: if the analysis indicates >90% satisfaction with the implementation, mark GREEN LIGHT




--- planning-process.md ---
---
phase: "P1 Plan & Scope"
gate: "Scope Gate"
status: "confirm problem, users, Done criteria, and stack risks are logged."
previous:
  - "Preflight Docs (AGENTS baseline)"
next:
  - "/scope-control"
  - "/stack-evaluation"
---

# Planning Process

Trigger: /planning-process

Purpose: Draft, refine, and execute a feature plan with strict scope control and progress tracking.

## Steps

1. If no plan file exists, create `PLAN.md`. If it exists, load it.
2. Draft sections: **Goal**, **User Story**, **Milestones**, **Tasks**, **Won't do**, **Ideas for later**, **Validation**, **Risks**.
3. Trim bloat. Convert vague bullets into testable tasks with acceptance criteria.
4. Tag each task with an owner and estimate. Link to files or paths that will change.
5. Maintain two backlogs: **Won't do** (explicit non-goals) and **Ideas for later** (deferrable work).
6. Mark tasks done after tests pass. Append commit SHAs next to completed items.
7. After each milestone: run tests, update **Validation**, then commit `PLAN.md`.

## Output format

- Update or create `PLAN.md` with the sections above.
- Include a checklist for **Tasks**. Keep lines under 100 chars.

## Examples
**Input**: "Add OAuth login"

**Output**:

- Goal: Let users sign in with Google.
- Tasks: [ ] add Google client, [ ] callback route, [ ] session, [ ] E2E test.
- Won't do: org SSO.
- Ideas for later: Apple login.

## Notes

- Planning only. No code edits.
- Assume a Git repo with test runner available.



--- pr-desc.md ---
---
phase: "P7 Release & Ops"
gate: "Review Gate"
status: "PR narrative ready for approvals and release prep."
previous:
  - "/review-branch"
next:
  - "/release-notes"
  - "/version-proposal"
---

# PR Description

Trigger: /pr-desc <context>

Purpose: Draft a PR description from the branch diff.

You are a CLI assistant focused on helping contributors with the task: Draft a PR description from the branch diff.

1. Gather context by running `git diff --name-status origin/main...HEAD` for the changed files (name + status); running `git diff --shortstat origin/main...HEAD` for the high‑level stats.
2. Create a crisp PR description following this structure: Summary, Context, Changes, Screenshots (if applicable), Risk, Test Plan, Rollback, Release Notes (if user‑facing). Base branch: origin/main User context: <args>.
3. Synthesize the insights into the requested format with clear priorities and next steps.

Output:

- Begin with a concise summary that restates the goal: Draft a PR description from the branch diff.
- Offer prioritized, actionable recommendations with rationale.
- Call out test coverage gaps and validation steps.
- Highlight workflow triggers, failing jobs, and proposed fixes.

Example Input:
src/example.ts

Expected Output:

- Actionable summary aligned with the output section.



--- prd-generator.md ---
<!-- $1=project plan text (visible link text), $2=product name, $3=problem statement, $4=key constraints -->
**PRD Generator Template**

Output a plain-text file named `prd.txt` containing **only** these sections in this order (separated by one blank line):
# Overview
# Core Features
# User Experience
# Technical Architecture
# Development Roadmap
# Logical Dependency Chain
# Risks and Mitigations
# Appendix

**Output Format**

- `# Overview`: $3
- `# Core Features`: Each includes *What*, *Why*, *High-level How*, and BDD criteria:
  `Given ...`
  `When ...`
  `Then ...`
- `# User Experience`: Personas, key flows, UI/UX, accessibility
- `# Technical Architecture`: Components, data models, APIs/integrations, infrastructure, NFRs
- `# Development Roadmap`: MVP and Future Enhancements with acceptance criteria (no dates)
- `# Logical Dependency Chain`: Work ordering for foundations, earliest front end, extensible units
- `# Risks and Mitigations`: Each includes *Description*, *Likelihood*, *Impact*, *Mitigation*
- `# Appendix`:
  • Assumptions (bulleted)
  • Research findings from $1
  • Context notes (`- <visible text> — inferred topic`)
  • Technical specs

**Validation Checks**

- Headers present and ordered
- All BDD criteria included for features/fallbacks
- Risks include likelihood and impact
- No URLs/secrets; exactly one blank line between sections
- $1 contains **only** visible link text 


--- prettier-adopt_Migration_report.md ---
You are a CLI assistant focused on helping contributors with the task: Plan a Prettier adoption or migration with minimal churn.

1. Gather context by inspecting `package.json`; running `git ls-files '*.*' | sed -n '1,400p'`.
2. Given the files and package.json, propose a rollout plan and ignore patterns.
3. Synthesize the insights into the requested format with clear priorities and next steps.

Output:

- Begin with a concise summary that restates the goal: Plan a Prettier adoption or migration with minimal churn.
- Offer prioritized, actionable recommendations with rationale.
- Document the evidence you used so maintainers can trust the conclusion.

Example Input:
(none – command runs without arguments)

Expected Output:

- Structured report following the specified sections.


--- problem-analyzer.md ---
<problem>
$1
</problem>

Tasks:
1) Locate all files/modules affected by the issue. List paths and why each is implicated.
2) Explain the root cause(s): what changed, how it propagates to the failure, and any environmental factors.
3) Propose the minimal, safe fix. Include code-level steps, side effects, and tests to add/update.
4) Flag any missing or outdated documentation/configs/schemas that should be updated or added (especially if code appears outdated vs. current behavior). Specify exact docs/sections to create or amend.

Output format:
- Affected files:
  - <path>: <reason>
- Root cause:
  - <concise explanation>
- Proposed fix:
  - <steps/patch outline>
  - Tests:
- Documentation gaps:
  - <doc_section_what_to_update_add>
- Open questions/assumptions:
  - <items>


--- prompt-sequence-generator.md ---
# Prompt: Generate Prompt Execution Sequence

**Purpose:** Given a high-level goal and a set of available prompts, generate the logical execution sequence required to accomplish that goal by chaining the prompts together.

---

### **Inputs**

*   **High-Level Goal:** {{high_level_goal}}
    *   *A clear, one-sentence description of the final outcome the user wants to achieve.*
    *   *Example: "Create and document a pull request for the currently staged changes."*

*   **Available Prompts:**
    ```
    {{available_prompts}}
    ```
    *   *A list of candidate prompt names (e.g., from the output of `rank-root-prompts`).*
    *   *Example: ['pr-desc.md', 'commit-msg.md', 'changed-files.md', 'review.md', 'release-notes.md']*

*   **Context (Optional):** {{context}}
    *   *Any additional context, such as the current state of the git repository or specific files of interest.*
    *   *Example: "The user has already staged files using `git add`."*

---

### **Instructions for the AI**

1.  **Analyze the Goal:** Deconstruct the `{{high_level_goal}}` into a series of logical steps required to get from the starting state to the final outcome.

2.  **Map Prompts to Steps:** For each logical step, identify the most suitable prompt from the `{{available_prompts}}` list that can perform that step.
    *   Consider the inputs and outputs of each prompt to determine dependencies. A prompt's input is often the output of a previous one.

3.  **Establish Order:** Arrange the selected prompts into a numbered sequence based on their dependencies. The sequence should represent a complete and logical workflow.

4.  **Identify Gaps:** If any necessary step in the workflow cannot be fulfilled by one of the available prompts, explicitly state what action or prompt is missing.

---

### **Required Output Format**

**Execution Sequence:**

1.  **`[prompt_name_1.md]`**: [Brief justification for why this prompt is first and what it accomplishes.]
2.  **`[prompt_name_2.md]`**: [Brief justification for why this prompt is second, and how it uses the output of the previous step.]
3.  ...

**Identified Gaps (if any):**

*   [Description of a missing step or prompt needed to complete the workflow.]


--- prompts/preflight/docfetch-check.md ---
---
phase: "P0 Preflight Docs"
gate: "DocFetchReport"
status: "DocFetchReport.status is OK with sources captured before planning or coding."
previous:
  - "Preflight discovery (AGENTS baseline)"
next:
  - "/instruction-file"
  - "/planning-process"
---

# DocFetch Preflight Check

Trigger: /docfetch-check

Purpose: Enforce the documentation freshness gate before planning work begins. Run this guardrail to pull the latest references, update the DocFetchReport, and block further tasks until the report is OK.

## Why this matters

- Keeps lifecycle prompts aligned with the newest official docs, SDK notes, and workflow rule-packs.
- Prevents stale guidance from sneaking into planning or implementation tasks.
- Records doc coverage in `DocFetchReport` so reviewers can audit what sources were considered.

## Steps

1. **Prepare the workspace**
   - Ensure you are at the repo root (`/home/user/.codex/prompts`).
   - Review `AGENTS.md` for any newly added rule-packs that might need fresh docs.
2. **Identify the doc set**
   - Note the tech stack elements you will touch (frameworks, SDKs, infra). For each, pick the primary doc provider (contex7-mcp first, gitmcp as fallback).
3. **Fetch docs via MCP**
   - For each library/tool:

     ```bash
     # Example using contex7-mcp via CLI helper (adjust topic per dependency)
     docfetch contex7-mcp "<library-id>" --topic "<focus-topic>"
     ```

   - When contex7-mcp fails, retry with gitmcp:

     ```bash
     docfetch gitmcp "owner/repo" --path docs --topic "<focus-topic>"
     ```

   - Capture timestamps, tool calls, and URLs; you will paste them into the report.
4. **Run local metadata checks**
   - Keep prompt metadata synchronized before recording the report:

     ```bash
     npm run validate:metadata
     npm run build:catalog
     ```

   - Fix any validation errors before proceeding.
5. **Update `DocFetchReport`**
   - Open (or create) `.docfetch/DocFetchReport.json`.
   - Record for each fetch:
     - `tool` (e.g., `contex7-mcp`)
     - `query` or URL
     - `time_utc` when fetched
     - Key guidance or insights
   - Set `status` to `"OK"` only when every required area has at least one up-to-date source. Use `"Docs Missing"` or `"Stale"` if coverage is incomplete.
6. **Capture gaps and follow-ups**
   - If any source could not be retrieved, list it under `DocFetchReport.gaps`. Include remediation notes (e.g., "Retry gitmcp once service resumes").
7. **Validate the gate**
   - Confirm `DocFetchReport.status == "OK"`.
   - Paste a short summary in your working notes or PR description detailing what changed.

## Outcome checklist

- [ ] `DocFetchReport.status` is `"OK"`.
- [ ] All new tool/library areas touched by upcoming work have documented sources.
- [ ] `DocFetchReport.tools_called[]` reflects every MCP lookup.
- [ ] `DocFetchReport.key_guidance[]` links each source to how it informs planned changes.
- [ ] Any unresolved gaps are logged under `DocFetchReport.gaps` with an action plan.

## Remediation if the gate fails

- **Missing sources:** Re-run docfetch with fallback providers. If still missing, escalate in the report and block downstream tasks until resolved.
- **Stale report:** Repeat the fetch sequence; update timestamps and guidance. Do not proceed with planning on a stale report.
- **Validation errors:** Address metadata/catalog issues (`npm run validate:metadata`, `npm run build:catalog`) before re-attempting DocFetch.
- **Infrastructure outages:** Mark `DocFetchReport.status` as `"Docs Missing"`, include outage details, and schedule a re-check within 24 hours.

Only move forward with planning (`/instruction-file`, `/planning-process`) after this checklist is satisfied and `DocFetchReport` shows a clean `"OK"` state.


--- prototype-feature.md ---
---
phase:
  - "P1 Plan & Scope"
  - "P2 App Scaffold & Contracts"
gate: "Prototype review"
status: "Validate spike outcomes before committing to scope."
previous:
  - "/planning-process"
next:
  - "/scaffold-fullstack"
  - "/api-contract"
---

# Prototype Feature

Trigger: /prototype-feature

Purpose: Spin up a standalone prototype in a clean repo before merging into main.

## Steps

1. Create a scratch directory name suggestion and scaffolding commands.
2. Generate minimal app with only the feature and hardcoded data.
3. Add E2E test covering the prototype flow.
4. When validated, list the minimal patches to port back.

## Output format

- Scaffold plan and migration notes.



--- query-set.md ---
# High-Yield Query Generator

Trigger: /query-set

Purpose: Generate 4–8 targeted web search queries with operators, entity variants, and recency filters for a given objective.

Steps:

1. Restate the goal with entities and time window.
2. Produce queries using operators: site:, filetype:, inurl:, quotes, OR, date filters.
3. Include synonyms and common misspellings.
4. Mix intents: define, compare, integrate, configure, limitations, pricing, API, case study.

Output format:

```
### Goal
{1 sentence}

### Query Set
- {Q1}
- {Q2}
- … up to 8
```

Examples:

- Input: `/query-set "OpenAI Responses API streaming server-sent events" past year`
- Output: Goal + 6–8 queries with operators.

Notes:

- No evidence logging here. Use /research-item to execute.


--- rank-root-prompts.md ---
<!--
$1 = command name/identifier
$2 = example user question
$3 = project CWD path to scan for context (defaults to current directory)
$4 = prompt directory path (defaults to "~/.codex/prompts")
$5 = minimum relevance threshold (0–1)
-->

# {Context-Aware Prompt Ranking Command}

```md
# Command: $1

# Usage: $1 "$2" "$3" "$4" "$5"

# Args:

# - {{query}}: $2
# - {{project_path}}: $3
# - {{prompt_path}}: $4
# - {{threshold}}: $5

prompt = """
Task:
Given a user inquiry ({{query}}) and the context of a software project located at {{project_path}}, your goal is to identify the most relevant prompt-definition file from the directory {{prompt_path}}.

Defaults:
* If {{project_path}} is missing or blank, use the current working directory.
* If {{prompt_path}} is missing or blank, use "~/.codex/prompts".

Do the following:
1) **Analyze Project Context**: Recursively scan {{project_path}} to understand its structure, languages, and purpose. Create a concise summary of the project context.
2) **Scan Prompts**: List all candidate prompt files in {{prompt_path}} (non-recursively).
3) **Evaluate Prompts**: For each candidate prompt file:
    a) Read its content.
    b) Create a one-sentence summary of its purpose and domain.
    c) Compute a relevance score from 0 to 1. This score must measure how well the prompt's purpose aligns with the user's {{query}}, considering the project context summary. A higher score means the prompt is a better fit for solving the query within the given project.
4) **Rank and Filter**: Order the prompts by their relevance score in descending order.
5) **Generate Output**: Emit a compact markdown table with the columns: `filename | description | match_score` (rounded to 2 decimals).

Rules:
* The description must be 1–2 sentences capturing the prompt's purpose and domain.
* Only include prompts in the table where `match_score` is greater than or equal to {{threshold}}.
* If no prompts meet the threshold, output a single line: "No prompt exceeds threshold {{threshold}} — recommend creating a new prompt."

Acceptance:
* If one or more matches meet the {{threshold}}, a markdown table sorted by descending `match_score` is produced.
* Otherwise, the single-line fallback message is produced.

!{echo "Scanning project: ${PROJECT_PATH_ARG:-.}"}
!{echo "Searching for prompts in: ${PROMPT_PATH_ARG:-~/.codex/prompts}"}
"""
```

## Output format

* **Preferred**: a markdown table with columns `filename | description | match_score` sorted by `match_score` (desc) and filtered by `{{threshold}}`.
* **Fallback**: the exact one-line message when no entries meet `{{threshold}}`.


--- refactor-code.md ---
<refactoring_goal>
$1
</refactoring_goal>

Task: <refactoring_goal>

- Keep the commit isolated to this feature.
- Document but do not fix unrelated problems you find.
- Never add fallbacks/backward-compability/feature flags, we are always build the full new refactored solution.

--- refactor-file.md ---
You are a CLI assistant focused on helping contributors with the task: Suggest targeted refactors for a single file.

1. Gather context by running `sed -n '1,400p' {{args}}` for the first 400 lines of the file.
2. Suggest refactors that reduce complexity and improve readability without changing behavior. Provide before/after snippets.
3. Synthesize the insights into the requested format with clear priorities and next steps.

Output:

- Begin with a concise summary that restates the goal: Suggest targeted refactors for a single file.
- Include before/after snippets or diffs with commentary.
- Document the evidence you used so maintainers can trust the conclusion.

Example Input:
src/components/Button.tsx

Expected Output:

- Refactor proposal extracting shared styling hook with before/after snippet.

